# -*- coding: utf-8 -*-
"""SSD+AE (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMvkOm_N74axauMr94SrGXWGnHgExOsz
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import torch
torch.backends.cudnn.benchmark = False

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/SSD"

"""## ライブラリ読み込み"""

# =========================
# ライブラリのインポート（リネーム処理なし）
# =========================
import os, sys, glob, time
import os.path as osp
import random as py_random

import cv2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.optim as optim
import torch.utils.data as data
import torch.nn.functional as F
from torch.utils.data import DataLoader

from importlib import reload

# プロジェクトルートをパスに追加
PROJ_ROOT = '/content/drive/MyDrive/SSD'
if PROJ_ROOT not in sys.path:
    sys.path.insert(0, PROJ_ROOT)

UTILS_DIR = os.path.join(PROJ_ROOT, 'utils')
os.makedirs(UTILS_DIR, exist_ok=True)

# __init__.py が無ければ作成（パッケージ化）
init_file = os.path.join(UTILS_DIR, '__init__.py')
if not os.path.exists(init_file):
    open(init_file, 'a').close()

# 以前の "utils" をキャッシュから外して衝突回避
for k in list(sys.modules.keys()):
    if k == 'utils' or k.startswith('utils.'):
        sys.modules.pop(k, None)

print('[DEBUG] utils dir exists:', os.path.isdir(UTILS_DIR))
print('[DEBUG] augmentation module candidates:', glob.glob(os.path.join(UTILS_DIR, 'data_aug*')))

# モジュールをリロードして反映
import utils.ssd_model as sm; sm = reload(sm)
import utils.ssd_predict_show as sps; sps = reload(sps)

# リロード後に必要クラスを改めて import（古い定義を残さない）
from utils.ssd_model import (
    VOCDataset, DataTransform, Anno_xml2list, od_collate_fn, SSD, MultiBoxLoss
)
from utils.ssd_predict_show import SSDPredictShow

# ★ utils の再ロード（utils/*.py を編集したらこのセルだけ再実行）
from importlib import reload
import utils.ssd_model as sm; reload(sm)
import utils.ssd_predict_show as sps; reload(sps)
from utils.ssd_model import MultiBoxLoss

# リロード後に改めて import（古い定義が残らないようにする）
from utils.ssd_model import VOCDataset, DataTransform, Anno_xml2list, od_collate_fn, SSD, MultiBoxLoss
from utils.ssd_predict_show import (
    _make_anomaly_map_from_feats,
    _anomaly_score_for_boxes,
    partial_missing_from_roi_px,
)

# === SSDPredictShow を“厳密モード(Strict)”に置き換えるパッチ =================
import os, json, torch
import numpy as np

if 'USE_L2' not in globals():
    USE_L2 = None

from utils import ssd_predict_show as sps
# 既存関数を使うためにモジュールから参照（元のコードと同じ呼び方）
partial_missing_from_roi_px = sps.partial_missing_from_roi_px
_make_anomaly_map_from_feats = sps._make_anomaly_map_from_feats
anomaly_score_for_boxes = sps._anomaly_score_for_boxes

INPUT_SIZE = 300

"""#統計のロードと推論準備"""

def _load_abn_stats_raw(self, stats_path="ae_stats.json"):
    """
    目的：
      - 異常検知(ABN)の校正値（しきい値 tau_px/tau_reg/r_min と、AEの μ/σ、誤差指標L1/L2）を
        JSONファイルから“倍率なし（そのまま）”で読み込んで内部状態にセットする。

    想定JSONの例（キー名）：
      {
        "tau_px": 1.0,          # ピクセル単位のしきい値（高い画素の密度を見るための目安）
        "tau_reg": 1.5,         # 領域平均のしきい値（面として強いかを見る目安）
        "r_min": 0.05,          # 欠損と見なす最小面積率（極小ノイズ除外）
        "metric": "L2",         # 誤差指標（"L1" または "L2"）。無ければ use_l2 から推定
        "use_l2": true,         # 互換キー（metricが無いときに参照する）
        "layers": [
          {"mu": 0.30, "sigma": 0.12},   # 層1の平均/標準偏差
          {"mu": 0.25, "sigma": 0.10},   # 層2 ...
          ...
        ]
      }
    """

    # --- 1) ファイル存在チェック：無ければ「既定値のまま」早期リターン -----------------
    if not os.path.exists(stats_path):
        print(f"[strict][warn] {stats_path} not found. keep defaults.")
        return

    # --- 2) JSON読み込み：辞書として st に展開 -------------------------------------------
    with open(stats_path, "r") as f:
        st = json.load(f)

    # --- 3) しきい値（倍率ナシでそのまま適用）--------------------------------------------
    #     - get(key, 既定) で欠損時の既定値を補う
    #     - float() で数値化（"1.0" のような文字列でも受け付ける）
    self._tau_px  = float(st.get("tau_px", 1.0))   # ピクセル密度のしきい値（既定 1.0）
    self._tau_reg = float(st.get("tau_reg", 1.5))  # 領域平均のしきい値（既定 1.5）
    self._r_min   = float(st.get("r_min", 0.05))   # 最小面積率（既定 0.05=5%）

    # --- 4) 誤差指標 L1/L2 の決定 --------------------------------------------------------
    #     - metric があればそれを優先（"l1"/"L1" など表記揺れは upper() で吸収）
    #     - 無ければ互換キー use_l2（bool）から推定（True→"L2", False→"L1"）
    #       ※ この実装では use_l2 のデフォルトは True（= "L2" を採用）にしている
    metric = st.get("metric", None)
    if metric is None:
        metric = "L2" if bool(st.get("use_l2", False)) else "L1"
    self._ae_metric = str(metric).upper()  # "L1"/"L2" いずれかの大文字表記

    # --- 5) 層ごとの μ/σ の取り込み ------------------------------------------------------
    #     - "layers" がリストなら、各要素(dict)から "mu"/"sigma" を取り出して数値化
    #     - self.net.calib_mu / self.net.calib_std に「Pythonリスト」のまま格納
    #     - 層数は _ae_num_layers に記録（後続の整合チェックで利用）
    layers = st.get("layers", None)
    if layers and isinstance(layers, list):
        # 欠損時に mu→0.0, sigma→1.0 を既定とする（安全側：少なくとも正規化が発散しにくい）
        mu_list  = [float(x.get("mu", 0.0))    for x in layers]
        std_list = [float(x.get("sigma", 1.0)) for x in layers]

        # 注意：ここではテンソル化やデバイス移動は行わず“生の値”のまま持つ。
        #       実際に z 正規化等で使う直前にテンソル化/デバイス整合を取る設計を想定。
        self.net.calib_mu  = mu_list
        self.net.calib_std = std_list

        # 取り込んだ層数を記録（後で feat_hat と一致しているかチェックするため）
        self._ae_num_layers = len(layers)
    else:
        # layers 無し or 不正：未設定を示す None。以降の整合チェック側で停止判断をする前提。
        self._ae_num_layers = None

"""# 概要

この関数は「異常を判定するための基準値セット」を、外部のJSONファイルから**読み込んで**モデル内部に**そのまま**入れ込む初期化処理です。
ざっくり言うと：

* しきい値（どこから異常とみなすか）と、各層での誤差の平均・ばらつき（μ・σ）を**読み取る**
* 誤差の測り方（L1かL2か）を**決める**
* それらをモデルの内部変数に**保存する**（以降の推論で使う）

戻り値はありません。内部状態を更新するだけの関数です。

# 目的

* 学習後に集計した**キャリブレーション結果（μ、σ、しきい値）**を、推論時に**正しく再現**するため。
* 設定ミスを防ぎながら、\*\*「JSONに書いた通り」\*\*の値を倍率なしで忠実に使うため。

# 受け渡しているデータとその内容

**入力（読み込むJSONファイルの想定）**

* `tau_px`：画素レベルのしきい値（画素密度などの小さな単位で異常を見る用）
* `tau_reg`：領域レベルのしきい値（ある程度の広がりがある異常を見る用）
* `r_min`：最小面積率（ごく小さいノイズを異常扱いしないための下限）
* `metric`：誤差の測り方（"L1" か "L2"）
* `use_l2`：`metric`が無いときの互換キー（trueならL2、falseならL1）
* `layers`：各層の `mu`（平均）と `sigma`（ばらつき）のリスト
  例）`[ {"mu":0.30,"sigma":0.12}, {"mu":0.25,"sigma":0.10}, ... ]`

**関数が設定する内部状態（副作用）**

* `self._tau_px`：画素しきい値
* `self._tau_reg`：領域しきい値
* `self._r_min`：最小面積率
* `self._ae_metric`：誤差指標（"L1" または "L2"）
* `self.net.calib_mu`：各層の平均（Pythonリストのまま）
* `self.net.calib_std`：各層のばらつき（Pythonリストのまま）
* `self._ae_num_layers`：読み込んだ層数（整合性チェック用）

# 処理の流れ

1. **ファイルの有無を確認**
   見つからなければ警告を出し、**既定値のまま**終了（内部のデフォルトを使う設計）。

2. **JSONを読み込み**
   `json.load`で辞書に展開。

3. **しきい値の反映（倍率なし）**
   `tau_px` / `tau_reg` / `r_min` を取り出し、**文字列でもfloat化**して内部に格納。
   なければ既定値（1.0 / 1.5 / 0.05）を採用。

4. **誤差指標（L1/L2）の決定**

   * `metric`があればそれを採用（大文字化して"L1"/"L2"に統一）。
   * なければ`use_l2`（真偽値）で判断（未指定ならL2を採用）。

5. **各層の μ・σ の取り込み**

   * `layers`がリストなら、各要素から`mu`と`sigma`を取り出して**float化**。
   * 欠けている場合は安全側の既定値（`mu=0.0`、`sigma=1.0`）。
   * **テンソル化やGPU転送はここではしない**（後段で使う直前に行う想定）。
   * `self._ae_num_layers`に層数を記録。ない／不正なら`None`を設定。

# 出力（関数の効果）

* **戻り値なし**。
* モデル内部の各種フィールド（上記「内部状態」）が**更新される**。
* 以後の推論で、読み込んだしきい値とμ・σが**そのまま**使われる。

# 典型的な落とし穴（上位3つ）と対策

1. **JSONが見つからないのに気づかない**

   * 症状：既定値のまま推論してしまい、**期待より厳し過ぎる／甘過ぎる**判定に。
   * 対策：読み込み直後のログを確認し、**ファイルパスと存在**を必ずチェック。ユニットテストで「存在時／非存在時」の挙動を固定。

2. **層数の不一致（mu・sigmaの数と実際のAE出力層がズレる）**

   * 症状：あとで標準化や集計で**形が合わない**、または**誤った層対応**になる。
   * 対策：`self._ae_num_layers`とモデル側の実層数を**直後にアサート**。学習時に記録した層順と**同じ順序**で保存・読み込みするルールを徹底。

3. **指標の不一致（学習時はL1、推論時はL2など）**

   * 症状：μ・σと異常スコアの定義がずれて、**しきい値が意味を失う**。
   * 対策：学習・統計作成・推論の**全工程で同一の指標**を使う。JSONの`metric`を**必須化**するか、読み込み後に**警告／アサート**を入れる。

# 用語ミニ解説

* **しきい値（tau\_px, tau\_reg）**：ここを超えたら「異常っぽい」と判断する境界値。画素単位（細かい点の異常）と領域単位（面の異常）を分けて持つ。
* **最小面積率（r\_min）**：ごく小さな点ノイズを「異常」とみなさないための下限。
* **μ（mu）・σ（sigma）**：誤差（復元できなさ）の**平均値**と**ばらつき**。これで「その層での普通の範囲」を表し、外れ具合を測る。
* **L1 / L2**：誤差の数え方の違い（差の絶対値を使うか、差の二乗を使うか）。どちらを採用するかは**学習から推論まで統一**が重要。

---

必要なら、**JSONサンプル**や**読み込み後のアサート例**も用意します。

"""

# ==============================================================================
# 「SSD + AE（オートエンコーダ）」推論に“異常検知(ABN)”を安全に組み込むための
#  厳格モードパッチ（倍率ナシ版）に関する詳細コメント付き解説コード。
#
# 目的：
#  - L1/L2 指標の不一致や、層数（レイヤー数）不一致など“よくある事故”を推論時に自動検出
#    し、問題があれば ABN を明示的に無効化して安全側に倒す（誤検出や例外を防ぐ）。
#  - しきい値（tau_px, tau_reg, r_min）や μ/σ（平均・標準偏差）を JSON から
#    「倍率を一切かけずに」そのまま再読み込み・適用する。
#  - 推論関数 ssd_predict_with_anomaly を“倍率ナシ版”に差し替えて、
#    2段階しきい値（ピクセル密度 + 領域平均）で「部分欠損(PM)」を判定。
#
# 用語メモ（簡潔）：
#  - L1/L2：AEの誤差の測り方。L2は大きい誤差をより強く扱う傾向。
#  - レイヤー数：特徴量や再構成（feat_hat）を出す層の個数。μ/σも層ごとに持つため一致必須。
#  - zマップ：画素ごとの「異常の強さ」を表す2次元マップ。
#  - リング：検出枠の外側に作る細い帯。内側と比較して“欠け”を目立たせる。
#  - 2段階しきい値：①高い画素がどれだけあるか（密度）→②その平均も高いか（面として強いか）で最終判定。
#  - モンキーパッチ：既存クラスのメソッドを実行時に差し替える手法。
# ==============================================================================


# 2) L1/L2・層数の簡易チェック
def _ae_preflight_ok(self, use_l2: bool) -> bool:
    """
    【目的】
      - 異常検知(ABN)の前提条件チェック（事前チェック）。
      - JSONや内部設定から決まる誤差指標（L1/L2）が、今回の推論で使うものと一致しているか、
        かつ μ/σ（校正統計）がロード済みかを確認する。

    【戻り値】
      - True  : 条件を満たすので ABN 続行可
      - False : 条件NGなので ABN を無効化（安全側停止）
    """
    requested = "L2" if use_l2 else "L1"  # 呼び出し側が今回使いたい誤差指標を文字列化
    metric = getattr(self, "_ae_metric", None)  # JSONや既定から決定済みの指標（"L1"/"L2"）
    if metric is not None and metric.upper() != requested:
        # ここに来たら、校正値（μ/σやしきい値）が想定する指標と今回の推論指標が不一致
        print(f"[strict][warn] metric mismatch: stats={metric} vs requested={requested}. ABN disabled.")
        return False

    # μ/σ（calib_mu/std）が未ロードなら、正しい正規化や比較ができないため ABN は止める
    if getattr(self.net, "calib_mu", None) is None or getattr(self.net, "calib_std", None) is None:
        print("[strict][warn] calib_mu/std not loaded. ABN disabled.")
        return False

    return True  # 事前条件OK


def _ae_postforward_layers_ok(self) -> bool:
    """
    【目的】
      - AE の forward 後（= SSD 推論内で AE も動いている前提）にレイヤー数の整合をチェック。
      - 「実際に得られた再構成feat（feat_hat）の層数」と「calib_mu/std の層数」が完全一致しているかを確認。

    【戻り値】
      - True  : レイヤー数が揃っているので続行可
      - False : 不一致または欠落があるので ABN を無効化（安全側停止）
    """
    rec = getattr(self.net, "last_rec", None)  # 直近のforward結果（AEの再構成などが入る想定）
    if rec is None:
        print("[strict][warn] last_rec is None. ABN disabled.")
        return False

    # AEが出した層ごとの再構成（feat_hat）の個数
    L_rec = len(rec.get("feat_hat", []))
    # 校正統計（μ/σ）の層数
    L_mu  = len(getattr(self.net, "calib_mu", []) or [])
    L_sd  = len(getattr(self.net, "calib_std", []) or [])

    # いずれかが0ならそもそも比較不能。ABNは止める
    if (L_rec == 0) or (L_mu == 0) or (L_sd == 0):
        print(f"[strict][warn] AE stats/features unavailable (rec={L_rec}, mu={L_mu}, sd={L_sd}). ABN disabled.")
        return False

    # 3者が完全一致していないなら整合NG。ABNは止める
    if not (L_rec == L_mu == L_sd):
        print(f"[strict][warn] layer count mismatch: rec={L_rec}, mu={L_mu}, sd={L_sd}. ABN disabled.")
        return False

    return True  # レイヤー数OK


# 3) __init__ を包んで「倍率ナシでしきい値を再セット」＋ mode="default" を強制
_OLD_INIT = sps.SSDPredictShow.__init__  # 既存クラス（sps.SSDPredictShow）の元 __init__ を退避

def _STRICT_INIT(self, eval_categories, net, *args, **kwargs):
    """
    【目的】
      - 既存の初期化の“直後”に、AEの校正値（しきい値・μ/σ）を JSON からそのまま再読込し、
        PM(部分欠損)のモードもプロジェクト既定に「統一」して、運用のブレをなくす。
      - ここで“倍率ナシ”を徹底することで、しきい値の意味が学習/キャリブ時と一致する。

    【引数の意味（代表）】
      - eval_categories：クラス名一覧（例：['crab1','crab2']）
      - net            ：SSD + AE を内包するネットワーク
      - *args, **kwargs：元の __init__ が受け取っていた残りのパラメータ
    """
    _OLD_INIT(self, eval_categories, net, *args, **kwargs)  # まず元の初期化をそのまま行う

    # “倍率ナシ”で校正値を再読込（ae_stats.json の metric/tau/μ/σ をそのまま反映）
    _load_abn_stats_raw(self, stats_path="ae_stats.json")

    # PM(部分欠損)判定のモードを1箇所で定義された既定値に統一（存在しなければ "default"）
    self._pm_mode = getattr(sps, "DEFAULT_PM_MODE", "default")

    # しきい値を見える化（デバッグ/再現性のためにログ出力）
    print(f"[strict] thresholds: tau_px={self._tau_px}, tau_reg={self._tau_reg}, r_min={self._r_min}")


# ここで実行時にクラスへ“差し替え（モンキーパッチ）”を適用
sps.SSDPredictShow.__init__ = _STRICT_INIT
sps.SSDPredictShow._ae_preflight_ok = _ae_preflight_ok
sps.SSDPredictShow._ae_postforward_layers_ok = _ae_postforward_layers_ok
sps.SSDPredictShow._load_abn_stats_raw = _load_abn_stats_raw


# 4) ssd_predict_with_anomaly を“倍率ナシ版”に置き換え
def _STRICT_PREDICT_WITH_ANOMALY(self, image_file_path, data_confidence_level=0.4,
                                 fuse="wmean", use_l2=False,
                                 inner_shrink=1.0, ring=8,
                                 layer_weights=None, drop_first=1, gamma=0.8):
    """
    【目的】
      - 物体検出（SSD）→ zマップ生成（AE由来の異常度マップ）→ 箱ごとの異常スコア → 2段階しきい値でPM判定
        までを一括で行う“厳格版”推論関数。
      - しきい値は JSON の値を“そのまま”使う（倍率を一切かけない）。

    【主な引数】
      - image_file_path        : 入力画像のファイルパス
      - data_confidence_level  : SSDの検出スコアしきい値（低い候補を捨てるライン）
      - fuse                   : 層ごとの異常度をどう混ぜるか（例："wmean"=重み付き平均）
      - use_l2                 : AE誤差の指標（True=L2, False=L1）。None の場合は JSON/既定から自動決定
                                 ※ここで参照している USE_L2 は外部で定義されている想定
      - inner_shrink           : 枠の内側を少し縮めて評価（縁の混入を避ける）
      - ring                   : 枠の外側に作る“比較リング”の幅（ピクセル数）
      - layer_weights          : 各層の重み（未指定なら内部既定）
      - drop_first             : 浅い層を何枚スキップするか（模様ノイズ対策など）
      - gamma                  : 重み付けの傾き調整（浅層/深層どちらを効かせるかの度合い）

    【戻り値（タプル）】
      (rgb_img, predict_bbox, pre_dict_label_index, scores,
       anomaly_scores, pm_flags, pm_scores, pm_area, pm_mean, hm_vis_np)
      - rgb_img                : 入力画像（RGB）
      - predict_bbox           : 検出ボックス座標（通常0〜1正規化）
      - pre_dict_label_index   : クラスID（検出結果）
      - scores                 : SSDの確信度
      - anomaly_scores         : リング差分ベースの箱ごとの異常スコア
      - pm_flags               : 2段階しきい値でのPM最終フラグ（True=部分欠損あり）
      - pm_scores              : PMスコア（“強さ×広さ”などの合成指標）
      - pm_area                : 欠損の面積率
      - pm_mean                : “熱い画素（高z）”の平均強度
      - hm_vis_np              : 可視化用 zマップ（NumPy配列）
    """

    # 0) use_l2 が None のときは、JSONから読んだ self._ae_metric を参照して自動決定
    if use_l2 is None:
        # _ae_metric は "L1"/"L2" の想定。大文字化して L2 なら True, それ以外は False
        use_l2 = (str(getattr(self, "_ae_metric", "L1")).upper() == "L2")

    # 1) 物体検出（この forward の中で AE の last_rec も更新される設計を想定）
    rgb_img, predict_bbox, pre_dict_label_index, scores = self.ssd_predict(
        image_file_path, data_confidence_level)
    H, W = rgb_img.shape[:2]  # 後続で正規化座標→画素座標へ変換するため、画像サイズを保持

    # 2) zマップ（異常度マップ）生成
    #    - eval(): 推論モードへ（ドロップアウトやBNの挙動をテスト時仕様に固定）
    #    - torch.no_grad(): 勾配計算無効化（高速化・省メモリ）
    self.net.eval()
    with torch.no_grad():
        # _make_anomaly_map_from_feats:
        #  各層の特徴から異常度を作り、fuse（融合戦略）で1枚のマップにまとめる。
        #  use_l2: L1/L2どちらの誤差で異常度を測るか
        #  make_vis=True: 可視化用ヒートマップも返す
        hm_std_b, hm_vis_b = _make_anomaly_map_from_feats(
            self.net, size_hw=(H, W), fuse=fuse, use_l2=use_l2, make_vis=True,
            layer_weights=layer_weights, drop_first=drop_first, gamma=gamma
        )
    # バッチ次元を外して1枚取り出す
    hm_std = hm_std_b[0]
    hm_vis = hm_vis_b[0]
    hm_vis_np = hm_vis.cpu().numpy()  # 可視化用は NumPy に変換（後段でプロットなどに使いやすい）

    # 3) ABN 実行の互換性チェック：
    #    - 事前条件（L1/L2一致・μ/σロード有無）と、
    #    - 事後条件（feat_hat と μ/σ のレイヤー数一致）の両方を満たすか
    abn_ok = self._ae_preflight_ok(use_l2) and self._ae_postforward_layers_ok()

    # 4) “リング差分”ベースの異常度は常に算出（ABN停止でも参考情報として出す）
    #    - 内側と外側リングを比較して「内側がより異常に見えるか」を点数化するイメージ
    anomaly_scores = _anomaly_score_for_boxes(hm_std, predict_bbox, ring=ring)

    # 5) PM（二段閾）判定：倍率ナシで JSON の値をそのまま適用
    pm_flags, pm_scores, pm_area, pm_mean = [], [], [], []
    if abn_ok:
        # SSDのボックスは通常0〜1正規化なので、画素座標に変換（境界チェック込み）
        boxes_px = []
        for i in range(predict_bbox.shape[0]):
            x1 = int(max(0, min(W - 1, predict_bbox[i, 0] * W)))
            y1 = int(max(0, min(H - 1, predict_bbox[i, 1] * H)))
            x2 = int(max(0, min(W,     predict_bbox[i, 2] * W)))
            y2 = int(max(0, min(H,     predict_bbox[i, 3] * H)))
            boxes_px.append((x1, y1, x2, y2))

        # 各ボックスで“部分欠損”を判定
        for (x1, y1, x2, y2) in boxes_px:
            # partial_missing_from_roi_px:
            #   ROI内の「高い画素の密度（tau_px）」と、その平均の高さ（tau_reg）、
            #   最小面積率（r_min）を使って PM を真偽＋スコアで返す。
            ok, sc, r, m = partial_missing_from_roi_px(
                hm_std, (x1, y1, x2, y2),
                tau_px = getattr(self, "_tau_px", 1.0),     # JSON由来：倍率ナシ
                tau_reg = getattr(self, "_tau_reg", 1.5),   # JSON由来：倍率ナシ
                r_min = max(0.02, getattr(self, "_r_min", 0.05)),  # 極小ノイズ対策で下限を0.02に
                shrink = inner_shrink,  # 枠を少し縮めて縁の混入を避ける
                mode = getattr(self, "_pm_mode", getattr(sps, "DEFAULT_PM_MODE", "default")),  # モード統一
                morph = 0,              # 形態学的処理（膨張/収縮）を使わない
                min_pixels = 8          # 面積が小さすぎる場合の足切り
            )
            pm_flags.append(bool(ok))     # True=部分欠損あり
            pm_scores.append(float(sc))   # 強さ×広さ等の合成スコア
            pm_area.append(float(r))      # 欠損の面積率
            pm_mean.append(float(m))      # “熱い画素”の平均強度
    else:
        # 互換性NGの場合は ABN を停止（安全側）。PM系はゼロで返す。
        n = predict_bbox.shape[0]
        pm_flags  = [False] * n
        pm_scores = [0.0] * n
        pm_area   = [0.0] * n
        pm_mean   = [0.0] * n

    # まとめて返す。下流は abn_ok の可否をログで確認しつつ、この戻り値を解釈する。
    return (rgb_img, predict_bbox, pre_dict_label_index, scores,
            anomaly_scores, pm_flags, pm_scores, pm_area, pm_mean, hm_vis_np)


# 既存クラスの推論関数を“厳格版”に差し替える（モンキーパッチ）
sps.SSDPredictShow.ssd_predict_with_anomaly = _STRICT_PREDICT_WITH_ANOMALY
print("[strict] SSDPredictShow patched: use ae_stats.json as-is (no extra multipliers).")
# ==============================================================================

"""# 概要

このコードは、物体検出モデル（SSD）に「欠け・破損などの異常」を見つける処理（AEベースの異常検知）を**安全に**組み込むための“厳格モード”パッチです。
やっていることをざっくり言うと：

* 外部JSONから読み込んだ**基準値**（しきい値、平均・ばらつき、誤差の種類）を**そのまま**使うように初期化を差し替える
* **指標の不一致（L1/L2）**や**層数不一致（AE出力と統計の数が違う）**など、よくある事故を**推論時に自動検出**し、問題があれば異常検知を**自動停止**（安全側）
* 画像を読んで物体を見つけ、\*\*異常度の地図（zマップ）**を作り、**2段階の基準**（高い画素の密度＋領域平均）で**部分欠損（PM）\*\*を判定する

# 目的

* 学習時に作った**キャリブレーション結果**（しきい値・平均・ばらつき・指標）を、推論時に**倍率をかけずに**再現し、**ブレなく**運用する
* よくある設定ミス（L1/L2違い・層数ズレ・統計未読込）を**自動検知**し、誤検出やクラッシュを**未然に防止**する
* 既存クラスの初期化・推論関数を**最小限の差し替え**（モンキーパッチ）で安全化する

# 入力となる変数・データ

* **外部JSON（`ae_stats.json`）**

  * `tau_px`：画素レベルの基準値
  * `tau_reg`：領域レベルの基準値
  * `r_min`：最小面積率（極小ノイズを無視するための下限）
  * `metric`：誤差の種類（"L1" または "L2"）。無ければ `use_l2` で代用
  * `layers`: 各層の `mu`（平均）と `sigma`（ばらつき）のリスト
* **画像パス（`image_file_path`）**：推論対象の画像ファイル
* **推論パラメータ**

  * `data_confidence_level`：検出スコアの下限（低すぎる候補を捨てる）
  * `fuse`：層ごとの異常度の混ぜ方（例："wmean"）
  * `use_l2`：誤差の種類（`None`ならJSONの設定に従う）
  * `inner_shrink`：枠の内側を少し縮める量（縁の混入を避ける）
  * `ring`：外側の比較リングの幅（ピクセル）
  * `layer_weights` / `drop_first` / `gamma`：層の重み付けや使う層の調整

# 処理の流れ

1. **初期化を差し替え（`_STRICT_INIT`）**

   * 元の初期化を実行 → 直後に `ae_stats.json` を読み込み
   * **倍率なし**で `tau_px`/`tau_reg`/`r_min` と `mu`/`sigma`/`metric` を内部に反映
   * PM判定モードをプロジェクト既定で**統一**（運用ブレ防止）

2. **事前チェック（`_ae_preflight_ok`）**

   * 使う誤差の種類（L1/L2）が**JSONの設定と一致**しているか
   * **μ・σが読み込まれているか**（無ければ比較不能なので停止）

3. **推論本体（`_STRICT_PREDICT_WITH_ANOMALY`）**

   * 画像から**物体検出**（SSD）。同時にAEの中間結果も更新される前提
   * 画像サイズに合わせて\*\*異常度マップ（zマップ）\*\*を作成（層を混ぜて1枚に）
   * **事後チェック（`_ae_postforward_layers_ok`）**：
     AEが出した層数と、μ・σの層数が**完全一致**かを確認
   * **リング差分の異常スコア**を各ボックスで計算（内側が外側より“熱い”か）
   * **2段階しきい値**でPM最終判定：

     * ①高い画素がどれぐらいあるか（`tau_px`）
     * ②その平均も高いか（`tau_reg`）
     * ③面積率が最小下限（`r_min`）を超えるか
     * 内側を `inner_shrink` で少し縮めて縁の混入を避ける
   * 互換性に問題があれば、**PM系の値はすべて0/False**にして安全停止

4. **戻り値**

   * `rgb_img`：元画像（RGB）
   * `predict_bbox`：検出ボックス（通常0〜1）
   * `pre_dict_label_index`：検出クラスID
   * `scores`：検出スコア
   * `anomaly_scores`：リング差分ベースの異常スコア
   * `pm_flags`：PM最終フラグ（True=部分欠損あり）
   * `pm_scores`：PMスコア（強さ×広さ等の合成指標）
   * `pm_area`：欠損の面積率
   * `pm_mean`：熱い画素の平均強度
   * `hm_vis_np`：可視化用ヒートマップ（NumPy配列）

# 出力

* **戻り値のタプル**（上記の一式）
* 互換性エラーが起きた場合でも、**検出結果や参考用スコアは返す**。PM系は安全のためゼロクリア

# 典型的な落とし穴（上位3つ）と対策

1. **JSONの指標（L1/L2）と実行時の指標が違う**

   * 症状：μ・σと異常度の定義がズレて、しきい値が意味を失う
   * 対策：このパッチは**自動検出してABNを停止**。運用面は、学習→統計作成→推論まで**同じ指標**を使うルールを徹底

2. **層数の不一致（AEの出力層数とμ・σの数がズレる）**

   * 症状：正規化や融合で**形が合わない**／誤った層対応
   * 対策：このパッチは**推論時に自動検出してABNを停止**。統計作成時の層順・層数と、推論モデルを**必ず一致**させる

3. **JSONが存在しない／値が不適切**

   * 症状：既定値のまま動いて**厳しすぎる／甘すぎる**判定になったり、極端な閾値で誤検出
   * 対策：初期化時のログとファイルパスを必ず確認。値の範囲・単位（倍率なし）を見直し、**学習時に使った設定をそのまま保存**して使う

# 用語ミニ解説

* **L1 / L2**：誤差の数え方（差の絶対値か二乗か）。どちらを使うかは**全工程で統一**が重要
* **μ（mu）・σ（sigma）**：各層の“普通さ”を表す平均とばらつき。これで異常度を**標準化**しやすくなる
* **zマップ**：画像の各場所がどれだけ“普通から外れているか”を表す地図
* **リング差分**：枠の**内側**と、その外側の細い帯（リング）を比べて、内側がより“熱い”かを見る方法
* **2段階しきい値**：
  ①**高い画素がどのくらい集まっているか**（密度）→ ②**平均も高いか**（面として強いか）で判断する手順
* **モンキーパッチ**：既存クラスのメソッドを**実行時に差し替える**手法。コード全体を大改修せずに振る舞いを変えられる

---

必要なら、**JSONの例**や**統計作成→保存→推論**までの**完全な手順**もあわせて整理します。

# 重みの読み込み
"""

WEIGHTS_DIR = '/content/drive/MyDrive/SSD/ssd_runs/crabs_ae'  # 好きなフォルダでOK
os.makedirs(WEIGHTS_DIR, exist_ok=True)
print("Weights will be saved to:", WEIGHTS_DIR)

"""# *データ準備*"""

# =============================================================================
# 目的：
#   物体検出（例：SSD）学習用の「画像リスト」と「アノテーション（VOC XML）リスト」を
#   安全に作る前処理。正例（crab1/crab2）はクラス比を保って train/val に分割し、
#   背景（OOD）は独立にペア化＆分割して最後に結合する。
#
# 前提：
#   - PROJ_ROOT が先に定義されていること（プロジェクトのルート絶対パス）
#   - クラス別ディレクトリ構成：
#       <PROJ_ROOT>/od_crabs/
#         ├─ crab1/  a.jpg, a.xml, ...
#         └─ crab2/  b.jpg, b.xml, ...
#   - 背景ディレクトリ：
#       <PROJ_ROOT>/od_crabs_ood/（任意の下位構造でOK）
#   - VOC XMLの <name> は 'crab1' または 'crab2' に統一されていること
#   - OpenCV(cv2) がインストール済み
# =============================================================================

# クラス名（正例）。XMLの <name> も同じ表記に揃えること
voc_classes = ['crab1', 'crab2']  # <- XMLの <name> も 'crab1' / 'crab2' に統一

# === クラス別サブフォルダを走査して .jpg/.xml ペアを列挙 ===
import os, glob
from sklearn.model_selection import train_test_split  # データをtrain/valに分けるユーティリティ

# === データリスト（画像/アノテ/ラベル）を作成：train_test_split の前に必須 ===
import os, glob, xml.etree.ElementTree as ET  # ETは今回は未使用だが、XML操作の標準ライブラリ
from collections import Counter  # クラス枚数の集計に使用

# -----------------------------------------------------------------------------
# 1) 依存関数を先に定義（重複は1つに統一）
#    クラス別ディレクトリを回って、"XMLを起点"に同名画像ファイルを探し、
#    画像パス / アノテXMLパス / クラスラベル の3つのリストを返す。
#    ※ XML基準にするのは、アノテの無い画像を誤って混ぜないため（安全側）。
# -----------------------------------------------------------------------------
def build_lists_from_class_dirs(root_dir, classes, img_exts=('.jpg', '.jpeg', '.png')):
    img_list, anno_list, labels = [], [], []
    for cls in classes:
        # 例：<root_dir>/crab1/*.xml を列挙（ソートして再現性を担保）
        xmls = sorted(glob.glob(os.path.join(root_dir, cls, '*.xml')))
        for xml_path in xmls:
            # XMLと同名の画像を探す（拡張子は候補を順に試す）
            base = os.path.splitext(os.path.basename(xml_path))[0]
            img_path = None
            for ext in img_exts:
                cand = os.path.join(root_dir, cls, base + ext)
                if os.path.exists(cand):
                    img_path = cand; break  # 最初に見つかったものを採用
            if img_path is None:  # 対応画像が無いXMLはスキップ（不整合データの混入防止）
                continue
            # 3つの並行リストに追記（インデックスで対応関係がわかる）
            img_list.append(img_path); anno_list.append(xml_path); labels.append(cls)
    return img_list, anno_list, labels

# -----------------------------------------------------------------------------
# 背景画像に対して「空のVOC XML」を作るための補助
#   - 背景は“物体なし”が正しいため、最小限のVOC XML（サイズのみ）を生成してペア化する。
#   - こうすることで、下流のローダが「画像とXMLは常に対」である前提を保てる。
# -----------------------------------------------------------------------------
import cv2  # 画像サイズ取得に使用（imreadで読み、高さh/幅wを得る）
def _write_empty_voc_xml(xml_path, img_path):
    os.makedirs(os.path.dirname(xml_path), exist_ok=True)  # 出力先ディレクトリが無ければ作る
    im = cv2.imread(img_path)  # 画像を読み込む（壊れた画像だとNoneになる点に注意）
    h, w = im.shape[:2]        # 画像の高さ・幅（チャンネルは使わない）
    # 物体要素の無い最小構成のVOC XMLを作る（背景なのでobjectタグは書かない）
    xml = f"""<annotation>
  <folder>.</folder>
  <filename>{os.path.basename(img_path)}</filename>
  <size><width>{w}</width><height>{h}</height><depth>3</depth></size>
</annotation>"""
    with open(xml_path, 'w', encoding='utf-8') as f:
        f.write(xml)

# -----------------------------------------------------------------------------
# 背景(OOD)の画像群から、画像パスとXMLパスのペアを作る
#   - recursive=True ならサブフォルダも含めて再帰的に走査
#   - create_empty_xml=True なら、画像に対応するXMLが無いとき空XMLを生成
# -----------------------------------------------------------------------------
def build_bg_pairs(bg_root, img_exts=('.jpg','.jpeg','.png'), recursive=True, create_empty_xml=True):
    imgs, annos = [], []
    pattern = "**/*" if recursive else "*"  # 再帰 or 直下のみ
    for p in sorted(glob.glob(os.path.join(bg_root, pattern), recursive=recursive)):
        ext = os.path.splitext(p)[1].lower()
        if ext not in img_exts:
            continue  # 対象拡張子以外はスキップ
        img = p
        xml = os.path.splitext(p)[0] + ".xml"  # 同名.xml を期待
        if not os.path.exists(xml):
            if create_empty_xml:
                _write_empty_voc_xml(xml, img)  # 見つからなければ空XMLを作る（推奨）
            else:
                continue  # 生成しない設定なら、この画像は採用しない
        imgs.append(img); annos.append(xml)  # ペアとして登録
    return imgs, annos

# -----------------------------------------------------------------------------
# ルート（クラス別サブフォルダ: <PROJ_ROOT>/od_crabs/crab1/*.xml, crab2/*.xml）
#   - 相対パスではなく PROJ_ROOT を基準に絶対パスで扱うと、環境差で壊れにくい
# -----------------------------------------------------------------------------
data_root = os.path.join(PROJ_ROOT, 'od_crabs')  # ★ 相対→絶対に（PROJ_ROOT は事前定義が必要）
all_img_list, all_anno_list, all_labels = build_lists_from_class_dirs(data_root, voc_classes)
print("[LIST] imgs:", len(all_img_list), "annos:", len(all_anno_list), "labels:", len(all_labels))
cnt = Counter(all_labels); print("[LIST] per-class:", dict(cnt))  # クラスごとの枚数を確認（偏りチェック）

# -----------------------------------------------------------------------------
# 3) 正例だけ stratify 分割
#   - クラス比を保った状態で train/val に分ける（例：9:1）
#   - 背景はあとで独立に分割するため、ここでは正例のみを分割する
# -----------------------------------------------------------------------------
p_tr_img, p_va_img, p_tr_an, p_va_an, p_tr_lab, p_va_lab = \
    train_test_split(all_img_list, all_anno_list, all_labels,
                     test_size=0.1,      # 検証10%
                     random_state=42,     # 乱数シード固定（再現性のため）
                     stratify=all_labels) # クラス比維持（層化抽出）

# -----------------------------------------------------------------------------
# 4) 背景の作成（ここで初めてbg_imgを作る）
#   - 背景(OOD)は正例とは独立に集める
#   - create_empty_xml=True なので、XMLが無い画像は空XMLを生成してペア化される
# -----------------------------------------------------------------------------
BG_ROOT = os.path.join(PROJ_ROOT, 'od_crabs_ood')  # ★ 相対→絶対に
bg_img, bg_anno = build_bg_pairs(BG_ROOT, recursive=True, create_empty_xml=True)
print(f"[BG] imgs={len(bg_img)} annos={len(bg_anno)} (from {BG_ROOT})")

# -----------------------------------------------------------------------------
# 5) 背景 split（ガード付き）← ここ以外に split は置かない！
#   - 背景が0枚の場合は分割をスキップ（空リストをセット）
#   - 背景はクラス概念が無いので通常の分割（stratifyなし）
# -----------------------------------------------------------------------------
if len(bg_img) == 0:
    print("[BG] no OOD images found; skipping background split.")
    bg_tr_img, bg_va_img, bg_tr_an, bg_va_an = [], [], [], []
else:
    bg_tr_img, bg_va_img, bg_tr_an, bg_va_an = \
        train_test_split(bg_img, bg_anno, test_size=0.1, random_state=42)

# -----------------------------------------------------------------------------
# 6) 最終結合
#   - 正例(train/val) と 背景(train/val) をそれぞれ結合して、最終的な学習・検証リストにする
#   - 画像リストとアノテーションリストは必ず同じ長さ・順序対応にする（並行リスト）
# -----------------------------------------------------------------------------
train_img_list = p_tr_img + bg_tr_img
train_anno_list = p_tr_an + bg_tr_an
val_img_list   = p_va_img + bg_va_img
val_anno_list  = p_va_an + bg_va_an
print("[counts] train:", len(train_img_list), "val:", len(val_img_list),
      "bg_in_train:", len(bg_tr_img), "bg_in_val:", len(bg_va_img))

# =============================================================================
# 使い方メモ：
#   - 以後のデータローダには、(train_img_list, train_anno_list) / (val_img_list, val_anno_list)
#     を渡す。各インデックスで画像とXMLが1対1で対応している。
#   - 背景枚数が多すぎる／少なすぎる場合は、bg_tr_img/bg_va_img をサンプリングして比率調整する。
#   - cv2.imread が None を返す壊れ画像は _write_empty_voc_xml で例外を招くため、
#     運用上は事前に壊れ画像検出スクリプトで除去しておくと安定する。
# =============================================================================

"""# 概要

このコードは、**物体検出の学習に使う「画像リスト」と「注釈（VOC形式XML）リスト」を、安全に作るための前処理スクリプト**です。
やっていることをひとことで言うと、**クラスつきの画像はクラス比を崩さず train/val に分け、背景画像は「物体なしXML」を自動で作って別枠で分け、最後に合体**します。
こうすることで、後段のデータローダが\*\*「画像とXMLが必ず1対1で揃っている」\*\*前提で、安心して学習を回せます。

# 目的

* **欠損や不整合（画像はあるのにXMLがない等）を自動回避**し、壊れにくい学習用リストをつくる
* **クラス比（例：crab1/crab2）を保ったまま** train/val を分割する（偏り防止）
* 背景（OOD）画像を**独立にペア化**（必要なら空XMLを自動生成）→ **別で分割** → **最後に正例と結合**
* すべてのパスを\*\*絶対パス基準（PROJ\_ROOT）\*\*で扱い、環境差で壊れにくくする

# 入力となる変数・前提

* `PROJ_ROOT`：プロジェクトの**ルート絶対パス**（事前に用意が必要）
* ディレクトリ構成（例）

  * `PROJ_ROOT/od_crabs/crab1/*.jpg, *.xml`
  * `PROJ_ROOT/od_crabs/crab2/*.jpg, *.xml`
  * `PROJ_ROOT/od_crabs_ood/**`（背景画像の置き場。下位フォルダOK）
* `voc_classes = ['crab1', 'crab2']`
  XML内の `<name>` も**この表記に統一**（大小文字・スペル違い禁止）
* ライブラリ：`opencv-python(cv2)`, `scikit-learn`, `glob`, `os` など

# 受け渡しているデータとその内容

* **正例（クラスつき）**

  * `all_img_list`：画像パスの並行リスト
  * `all_anno_list`：対応するXMLパスの並行リスト
  * `all_labels`：各画像のクラス名（`voc_classes` のいずれか）
* **train/val 分割（正例のみ）**

  * `p_tr_img / p_tr_an / p_tr_lab`：train
  * `p_va_img / p_va_an / p_va_lab`：val
    ※ `train_test_split(..., stratify=all_labels)` で**クラス比維持**
* **背景（OOD）**

  * `bg_img / bg_anno`：画像とXMLのペア
    XMLがなければ `_write_empty_voc_xml` で**空XMLを自動生成**
  * 背景の train/val：`bg_tr_img / bg_tr_an / bg_va_img / bg_va_an`（**正例とは独立に分割**）
* **最終結合（学習で使う）**

  * `train_img_list = p_tr_img + bg_tr_img`
  * `train_anno_list = p_tr_an + bg_tr_an`
  * `val_img_list   = p_va_img + bg_va_img`
  * `val_anno_list  = p_va_an + bg_va_an`
    → **画像とXMLは常に同じ長さ・同じ順序**（インデックスで1対1）

# 処理の流れ

1. **クラス一覧を定義**：`voc_classes = ['crab1', 'crab2']`
2. **正例の収集**：`build_lists_from_class_dirs(data_root, voc_classes)`

   * クラス別フォルダを走査し、**XML基準**で同名画像を探す
   * 画像・XML・ラベルの**並行リスト**を作る
   * XMLがあるのに画像がない等の**不整合を自然に除外**できる（安全）
3. **クラス枚数の確認**（`Counter` で枚数を出力）→ **偏りチェック**
4. **正例の分割**：`train_test_split(..., stratify=all_labels, test_size=0.1)`

   * **クラス比を維持したまま** 9:1（例）で train/val に分ける
5. **背景（OOD）のペア化**：`build_bg_pairs(BG_ROOT, recursive=True, create_empty_xml=True)`

   * フォルダを走査、**同名XMLが無い画像には空XMLを自動生成**
   * 背景にはクラスがないので、**画像とXMLのペアのみ**作る
6. **背景の分割**：`train_test_split(..., test_size=0.1)`

   * **正例とは独立**に分割（`stratify` は使わない）
   * 背景が0枚なら**安全にスキップ**
7. **正例と背景の結合**（train/valごと）

   * 画像リストとXMLリストをそれぞれ結合
   * **長さと順序を必ず一致**させる（1対1対応を保つ）
8. **カウント出力**（確認用）

   * `train`/`val`の枚数、背景の枚数など

# 出力

* 学習用：`train_img_list`, `train_anno_list`
* 検証用：`val_img_list`, `val_anno_list`
  これらを**そのままデータローダ**に渡せば、**画像とXMLが常に1対1**で読み込めます。

# 典型的な落とし穴（上位3つ）と対策

1. **XMLの `<name>` が `voc_classes` と不一致（表記ゆれ・別名）**

   * 症状：想定外クラスとして扱われたり、後段でクラス数不一致が起きる
   * 対策：**事前にXMLを一括置換**して表記統一（大小文字・スペース含め完全一致）

2. **画像 or XML が欠けている／破損している**

   * 症状：読み込み失敗、長さ不一致、学習で例外
   * 対策：

     * 正例は**XML基準で収集**（画像欠けを自然排除）
     * 背景は**空XMLを自動生成**し、常にペア化
     * 壊れ画像検出スクリプトで**事前に除去**

3. **背景の割合が極端（多すぎ／少なすぎ）**

   * 症状：学習が背景に偏る、検出が鈍る
   * 対策：`bg_tr_img/bg_va_img` を**サンプリング**して比率調整（例：上限枚数を決めてランダム間引き）

# 用語ミニ解説

* **VOC XML**：物体の位置とクラス名を記録したXML形式の注釈ファイル
* **背景（OOD）**：学習クラスに含まれない画像。ここでは\*\*“物体なし”\*\*として扱う
* **空XML**：**物体要素を持たない**最小限のVOC XML。背景画像を**常にXMLとペア**にするために使う
* **並行リスト**：同じ長さで、**同じインデックス同士が対応**する2本のリスト（画像／XML）
* **層化抽出（stratify）**：分割しても**クラス比を保つ**方法
* **絶対パス**：`PROJ_ROOT` を起点にした完全なパス。環境が変わっても壊れにくい

---

必要なら、**背景比の調整例**や、**壊れ画像の検出スクリプト**、**クラス名の一括正規化スクリプト**も用意できます。

"""

# =============================================================================
# サニティチェック & OOD(背景)用 空VOC-XML 自動生成スクリプト
#
# 目的（ざっくり）：
#   1) 学習に使うアノテXML群（train_anno_list）が「未知ラベルを含まない」こと、
#      かつ「背景XMLは object なし（=前景0）」になっていることを点検する
#   2) 背景画像フォルダ(./od_crabs_ood)を走査し、まだXMLが無い画像に対して
#      「object タグを一切含まない」空のVOC-XMLを自動生成する
#
# ねらい：
#   - データの整合性を早めに検出（= サニティチェック）
#   - 画像とXMLが常に1対1で揃うようにし、下流のデータローダを安定させる
#   - 背景(OOD)画像を「前景0の負例」として扱えるようにする（誤検出の抑制に効く）
# =============================================================================


# === サニティ：背景XMLが空であること・未知ラベルが混入していないこと（新規セル） ===
import xml.etree.ElementTree as ET  # 標準ライブラリのXMLパーサ。軽量・依存なし。

unknown, empty_bg = 0, 0  # カウンタ初期化：未知ラベル数 / 空XML（背景）数
for xml in train_anno_list:  # 事前に作成済みの「学習用アノテXMLのリスト」を順にチェック
    root = ET.parse(xml).getroot()  # XMLを読み込み、ルート要素（<annotation>）を取得
    objs = list(root.iter('object'))  # すべての<object>タグを列挙（物体があれば1つ以上ある）
    if len(objs) == 0:
        # object が1つも無い → このXMLは「背景（前景0）」として数える
        empty_bg += 1
    for obj in objs:
        # 各 object の <name>（クラス名）を取り出して許容リストに入っているか確認
        name = obj.find('name').text.strip()
        if name not in voc_classes:  # voc_classes は ['crab1','crab2'] 等、許可クラス一覧
            # クラス外の名前が混入している → 学習時に崩れるので発見次第修正が必要
            unknown += 1

# 結果を見やすく出力：未知ラベル数 / 背景XML数
print(f"[sanity] unknown labels in train: {unknown}  empty-background xmls: {empty_bg}")

# フォルダ ./od_crabs_ood を走査し、画像ごとに <object> を含まない XML を作成。
# → これで OOD 画像が「前景0の負例」として VOC 互換データセットに混ぜられる。


# === OOD画像に「object 0」の空VOC-XMLを自動生成（新規セル） ===
import os, glob, cv2, xml.etree.ElementTree as ET
# os      : パス操作/ディレクトリ作成
# glob    : パターンに合うファイルを一括列挙
# cv2     : OpenCV。画像サイズを取得するために使用（imread→shape）
# ET      : 上と同じ（再インポートは無害）

def write_blank_voc_xml(img_path, xml_path):
    """
    指定画像と同名のVOC-XMLを新規作成する。
    - 「object タグは一切付けない」＝前景なしの空アノテーション
    - 画像サイズなどの基本情報だけを書き込む
    """
    img = cv2.imread(img_path)               # 画像を読み込む（Noneなら壊れている可能性）
    assert img is not None, img_path         # 壊れ画像はここで検知して止める（ファイル名を表示）
    h, w = img.shape[:2]                     # 画像の高さ・幅（チャンネル数は不要）

    # <annotation> ルート要素を作り、最小限の子要素を追加していく
    ann = ET.Element("annotation")
    ET.SubElement(ann, "folder").text = os.path.basename(os.path.dirname(img_path))  # 画像の親フォルダ名
    ET.SubElement(ann, "filename").text = os.path.basename(img_path)                 # 画像ファイル名

    # <size> ブロック：学習パイプラインが画像サイズを参照する前提が多いので必須
    size = ET.SubElement(ann, "size")
    ET.SubElement(size, "width").text  = str(w)
    ET.SubElement(size, "height").text = str(h)
    ET.SubElement(size, "depth").text  = "3"  # RGB想定（グレースケールなら1にするなど環境に合わせて）

    ET.SubElement(ann, "segmented").text = "0"  # VOC準拠の定番フィールド（セグメンテーション未対応）

    # ※ object タグは付けない（=前景なし）。これが「負例（検出対象が無い画像）」のサインになる。
    ET.ElementTree(ann).write(xml_path, encoding="utf-8", xml_declaration=True)  # XMLとして保存

# 背景画像を置くフォルダ（なければ作る）。ここにある画像が「負例候補」になる。
OOD_DIR = "./od_crabs_ood"
os.makedirs(OOD_DIR, exist_ok=True)  # 既にあるなら何もしない

made = 0  # 生成したXMLのカウント
for img_path in glob.glob(os.path.join(OOD_DIR, "*.*")):
    # OOD_DIR直下のファイルをざっと走査（必要なら再帰に拡張可能）
    base, ext = os.path.splitext(img_path)
    if ext.lower() not in (".jpg",".jpeg",".png"):
        continue  # 画像拡張子以外は対象外（.txt, .xml などはスキップ）

    xml_path = base + ".xml"  # 画像と同名の .xml を期待する（例：foo.jpg → foo.xml）
    if not os.path.exists(xml_path):
        # まだXMLが無ければ空のVOC-XMLを新規作成
        write_blank_voc_xml(img_path, xml_path)
        made += 1

print(f"[ood] blank xml created: {made}")  # 何件作られたかをログ表示（0なら既に揃っている）

"""# 概要（要点）

* **何のコード？**
  学習用データの健全性を確認（サニティチェック）し、背景画像（OOD）に**空の Pascal VOC XML**を自動生成するユーティリティ。
* **目的**

  1. `train_anno_list` の XML を走査し、**未知ラベルの混入**と**空（=前景なし）XMLの個数**を集計して可視化。
  2. `./od_crabs_ood` 内の画像ごとに、未作成なら **`<object>` を含まない空XML** を生成して、背景を「前景0の負例」として学習データに混ぜられるようにする。

---

# 受け渡しデータの整理

## 入力（前提）

* `train_anno_list`：学習に使う**アノテーション（VOC-XML）ファイルパスのリスト**
* `voc_classes`：許可するクラス名の一覧（例：`['crab1', 'crab2']`）
* `./od_crabs_ood`：背景（OOD）画像フォルダ（`jpg/jpeg/png`）

## 出力・副作用

* **サニティ結果のログ**：

  ```
  [sanity] unknown labels in train: <未知ラベル数>  empty-background xmls: <空XML数>
  ```
* **空XMLの自動生成**（必要な場合のみ）：

  * `./od_crabs_ood/foo.jpg` → `./od_crabs_ood/foo.xml` を新規作成（`<object>` なし）
  * 生成数のログ：

  ```
  [ood] blank xml created: <作成数>
  ```

## 処理の流れ（ざっくり）

1. **サニティチェック**

   * 各XMLを読み、`<object>` が0なら背景としてカウント。
   * `<object><name>` が `voc_classes` にない場合を未知ラベルとしてカウント。
2. **空VOC-XMLの生成**

   * OODフォルダ内の画像を列挙。
   * 同名の `.xml` がなければ、画像サイズなど基本情報だけを持つ**空XML**を作成（`<object>` なし）。

---

# 典型的な落とし穴（上位3つ）と対策

1. **未知ラベルの混入**

   * **症状**：`<name>` が `voc_classes` にない → 学習でクラス崩れやエラーの原因。
   * **対策**：サニティの `unknown` が **常に0** になるようにアノテ修正（バルク置換/ツールのラベルプリセット）。

2. **壊れた画像で空XML生成が失敗**

   * **症状**：`cv2.imread(img_path)` が `None` → サイズ取得で失敗。
   * **対策**：関数内の `assert img is not None` は良いガード。事前に**壊れ画像検出**や再保存で除去。

3. **拡張子や大文字小文字の揺れで未処理画像が出る**

   * **症状**：`.JPG` などがフィルタに引っかからず、XML未生成でローダが落ちる。
   * **対策**：`ext.lower()` で判定しているのはOK。必要に応じて対象拡張子のリストを拡張（`tif` 等は別対応）。

---

# 用語ミニ解説

* **サニティチェック**：学習前にデータの整合性を確認する最低限の検査。
* **Pascal VOC（VOC-XML）**：物体検出で一般的なXML形式。画像サイズや物体領域を記録する。
* **OOD（Out-of-Distribution）/背景**：学習対象に含まれない画像群。**前景0の負例**として混ぜると誤検出の抑制に有効。
* **前景0の負例**：画像内に検出対象が**存在しない**ことを示すサンプル（`<object>` を付けないXML）。

---

> 💡このスクリプトを回した後は、**画像リストとXMLリストが常に1対1**になるため、下流のデータローダが安定します。

"""

# =============================================================================
# 全アノテ(XML)を走査して、
#   1) クラス別の登場回数（件数）
#   2) 各物体の「画像に占める大きさ（面積率）」の平均
# を表示するサンプル。
#
# ねらい：
#   - データの偏り（例：crab1 が多くて crab2 が少ない）を把握する
#   - ラベルの綴り揺れ・漏れ（unknown名の混入）を早期に検出する
#     → 検出したら [WARN] を出して気づけるようにする
#
# 使う前提：
#   - all_anno_list に Pascal VOC 形式の XML パスが入っている
#   - 対象クラスは 'crab1' と 'crab2'
#   - 各 XML の <size> に画像幅/高さが入っている（width/height）
#   - 各 <object> に <bndbox>（xmin, ymin, xmax, ymax）が入っている
# =============================================================================

from collections import Counter
import xml.etree.ElementTree as ET, os

# 件数カウンタ（クラス別の出現数を足し込む）
cnt = Counter()

# 面積率を溜めるバッファ（クラスごとにリストで保持 → あとで平均を出す）
areas = {"crab1": [], "crab2": []}

for xml in all_anno_list:
    # XML を読み込んで、ルート（<annotation>）を取得
    root = ET.parse(xml).getroot()

    # 画像の幅と高さを XML から取得（文字列 → int 変換）
    # 例）<size><width>1280</width><height>720</height>...</size>
    W = int(root.find('size/width').text)
    H = int(root.find('size/height').text)

    # すべての <object> タグ（＝物体）を順に処理
    for obj in root.iter('object'):
        # <name> からクラス名を取得し、前後空白を除去して小文字化
        #  小文字化することで、"Crab1" / "crab1" のような綴り揺れを吸収しやすくする
        name = obj.find('name').text.strip().lower()

        # 想定外のラベル名が来たら警告を出してスキップ
        #  → 学習前にラベルの混入・綴りミスに気づける
        if name not in {'crab1', 'crab2'}:
            print("[WARN] unknown name:", name, "in", xml)
            continue

        # バウンディングボックス（物体の外接矩形）の座標を取得
        # VOC は 1始まりで保存されていることがあるため、ここでは -1 して 0始まりに寄せる
        #  （※ データに合わせて必要なければ -1 を外しても良い）
        bnd = obj.find('bndbox')
        xmin = int(bnd.find('xmin').text) - 1
        ymin = int(bnd.find('ymin').text) - 1
        xmax = int(bnd.find('xmax').text) - 1
        ymax = int(bnd.find('ymax').text) - 1

        # 幅・高さを計算（万一、座標の並びが逆/はみ出し等があっても負にならないように 0 で下駄を履かせる）
        bw = max(0, (xmax - xmin))
        bh = max(0, (ymax - ymin))

        # 面積率（＝ 物体の占める面積 / 画像全体の面積）
        #  画像面積が 0 の異常値でもゼロ割りしないために、わずかな値(1e-6)を足しておく
        a = (bw * bh) / (W * H + 1e-6)

        # 件数を 1 つカウントアップし、面積率をリストに追加
        cnt[name] += 1
        areas[name].append(a)

# クラス別の件数を表示（例：Counter({'crab1': 120, 'crab2': 80})）
print("[count]", cnt)

# 平均面積率を出す。データが 0 件でもゼロ割りしないように max(1, 件数) で割る
#  → 件数が 0 のクラスは 0.0 として出力される
mean_crab1 = sum(areas['crab1']) / max(1, len(areas['crab1']))
mean_crab2 = sum(areas['crab2']) / max(1, len(areas['crab2']))

print("[area mean] crab1=", mean_crab1, " crab2=", mean_crab2)

"""# 概要・目的（要点）

このコードは、Pascal VOC 形式のアノテーション一覧を走査して、

1. クラス別の登場回数を数え、
2. 各物体が画像内でどのくらいの面積を占めるか（面積率）の平均を求め、
3. ラベルの漏れや綴り揺れを警告する、
   ための**サニティチェック用スクリプト**です。対象クラスは `crab1` と `crab2` です。

# 入力となる変数

* `all_anno_list`：Pascal VOC 形式のアノテーション XML ファイルのパス一覧。
* 対象クラス集合：`{'crab1', 'crab2'}`（コード内で固定）。

# 処理の流れ（ざっくり）

1. 画像サイズを XML の size/width, size/height から取得。
2. すべての object を走査し、

   * name を小文字化して対象クラスか確認（対象外は WARN を出してスキップ）。
   * bndbox の座標から幅・高さを計算し、画像面積に対する割合（面積率）を求めて記録。
   * クラス別の件数カウンタを加算。
3. 走査終了後、

   * クラス別の出現数（Counter）を表示。
   * クラス別の面積率の平均を表示。

# 出力（画面ログ）

* `[WARN] unknown name: ... in ...`
  対象クラス以外のラベル名が見つかったときに表示。

* `[count] Counter({'crab1': N1, 'crab2': N2})`
  クラス別の登場回数。

* `[area mean] crab1= A1  crab2= A2`
  クラス別の平均面積率（0〜1の範囲を想定）。

# 受け渡しているデータとその内容

* **入力（XML から取得する主なフィールド）**

  * 画像サイズ：size/width, size/height
  * 物体：object 配下の name（クラス名）、bndbox（xmin, ymin, xmax, ymax）

* **内部で保持する構造**

  * `cnt`：`collections.Counter` によるクラス別の件数。
  * `areas`：`{"crab1": [...], "crab2": [...]}` という辞書で、各クラスの面積率をリストで蓄積。

* **出力の意味**

  * `cnt`：学習データにおけるクラス分布の粗い把握。
  * 面積率平均：クラスごとの物体サイズ傾向を把握（大きめばかり/小さめばかり等）。

# 典型的な落とし穴（上位3つ）と対策

1. **ラベルの綴り揺れ・未知ラベル混入**

   * 症状：`Crab1` や `crab_1` などで学習時のクラス解釈が崩れる。
   * 対策：コードのように name を**小文字化**し、対象集合に含まれなければ **WARN を出して修正**。可能ならアノテツール側でラベル名を固定。

2. **座標の不正（はみ出し・逆順・1 始まり/0 始まりの混在）**

   * 症状：負の幅・高さが出る、不正な面積率になる。
   * 対策：コードは幅・高さ計算で **0 未満を切り捨て**てガード。必要に応じて座標の正規化（境界クリップ、1 始まり→0 始まりの統一）を事前整備。

3. **画像サイズの欠損・不正**

   * 症状：面積率の計算が崩れる、ゼロ割り相当が起きる。
   * 対策：サイズは XML 生成時に必ず記録。コード側は分母にごく小さな値を足して**ゼロ割り回避**しているが、根本はデータ整備で防ぐ。

# 用語ミニ解説

* **Pascal VOC 形式**：物体検出で広く使われるアノテーション仕様。画像サイズや物体の矩形座標が XML に記述される。
* **サニティチェック**：学習前に異常や不整合を早期発見するための軽量な検査。
* **面積率**：物体の枠が画像全体に対して占める割合。大きさの偏りを把握する目安になる。
* **bndbox（バウンディングボックス）**：物体を囲む長方形。左上と右下の座標で表す。

# *アンカー設計*
"""

# =============================================================================
# GT（教師データ）のサイズ分布から、SSD300 用の“デフォルトの箱サイズ”を
# データ駆動（自動調整）で提案するスクリプト（解説付き）。
#
# 目的（直感的に）：
#   - 学習データに出てくる「物の大きさ」をざっくり数え上げる
#   - その分布に合うように、検出で最初に置く箱サイズを6段階で提案する
#   - データに合った箱を置くことで、見逃しを減らし、重なり・ダブりを改善する
#
# 事前に用意されている想定の変数：
#   - all_anno_list : Pascal VOC 形式のアノテXMLのパス一覧
#   - voc_classes   : 対象とするクラス名（例：{'crab1','crab2'}）
#   - INPUT_SIZE    : 学習時の入力画像サイズ（SSD300 なら 300）
#
# 出力（ログ）：
#   - サンプル数／最小・中央値・最大
#   - 提案する min_sizes（6段）・max_sizes（6段）・aspect_ratios（各段の縦横比候補）
#
# 重要な考え方（数式なしの直感）：
#   - 画像ごとに物体の四角形（幅×高さ）がある
#   - 画像サイズの違いをそろえるために、学習時の入力サイズに合わせて換算する
#   - 幅と高さを“バランスよく”1つのスケール指標（相当辺長）にまとめる
#   - その分布の節目（分位点）を箱サイズの候補にする
# =============================================================================

import xml.etree.ElementTree as ET, numpy as np, math

def collect_box_side_px(anno_list, input_size=INPUT_SIZE):
    """
    役割：
      - すべてのアノテXMLを読み、対象クラスの箱（四角形）の“サイズ感”を集める
      - サイズ感は、入力サイズに換算したときの「相当する一辺の長さ」を使う
        （幅だけ・高さだけに引っ張られないよう、バランス良くまとめるイメージ）

    引数：
      - anno_list : アノテXMLのパス一覧
      - input_size: 学習時の入力画像の一辺（SSD300 なら 300）

    戻り値：
      - sides : 相当辺長の配列（float32）
    """
    sides = []
    for xml in anno_list:
        # XML を読み込み、画像サイズを取得（サイズは面積換算に必要）
        root = ET.parse(xml).getroot()
        W = int(root.find('size/width').text)
        H = int(root.find('size/height').text)

        # すべての物体（object）を走査
        for obj in root.iter('object'):
            name = obj.find('name').text.strip()
            # 対象外クラスはスキップ（未知ラベルが混じると分布が歪むため）
            if name not in set(voc_classes):
                continue

            # 物体の四角形（bndbox）座標を取得
            b = obj.find('bndbox')
            # 1始まりの可能性を考慮し、-1 している（データ仕様に合わせて調整可能）
            x0 = max(0, int(b.find('xmin').text) - 1)
            y0 = max(0, int(b.find('ymin').text) - 1)
            x1 = int(b.find('xmax').text) - 1
            y1 = int(b.find('ymax').text) - 1

            # 幅・高さ（最低でも1ピクセルにして、ゼロや負を防ぐ）
            w = max(1, x1 - x0)
            h = max(1, y1 - y0)

            # 入力サイズ相当に換算した“相当辺長”を計算
            #  - 画像が大きくても小さくても、学習時と同じスケールで比較できるようにする
            #  - 幅だけ/高さだけではなく、両方のバランスをとった1値にまとめる
            side = math.sqrt((w * input_size / W) * (h * input_size / H))

            # 集計リストに追加
            sides.append(side)

    # NumPy配列にまとめて返す（統計処理や分位点計算がやりやすい）
    return np.array(sides, np.float32)

# 収集：相当辺長の配列を作る
sides = collect_box_side_px(all_anno_list, input_size=INPUT_SIZE)

# ざっくりの分布（サンプル数・最小・中央値・最大）を出力し、データ感覚を掴む
print("[anchor] samples:", len(sides), "min/med/max:",
      float(sides.min()), float(np.median(sides)), float(sides.max()))

# ---------------------------------------------------------------
# 分位点から6段の min/max を提案
#   - データの節目（例：10/25/40/55/70/85%）を min_sizes に採用
#   - max_sizes は min + 40 を基本に、
#       * 小さすぎるのを防ぐため min + 5 以上
#       * 大きすぎるのを防ぐため 315 以下（SSD300 の最大想定）
#   - aspect_ratios は一般的な候補を提示（細長物体が少なければ [2] を中心に）
# ---------------------------------------------------------------
percentiles = [10, 25, 40, 55, 70, 85]               # 必要に応じて調整可能（外れ値が多いなら見直す）
min_sizes = np.percentile(sides, percentiles).astype(int)
max_sizes = np.clip(min_sizes + 40, min_sizes + 5, 315)  # “幅”を固定値40にして、上下限で安全側にクリップ
aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]  # データに合わせて増減させても良い

# 提案値を出力（この値を DBox/DefaultBox 生成部に反映する）
print("[anchor] suggested min_sizes:", list(map(int, min_sizes)))
print("[anchor] suggested max_sizes:", list(map(int, max_sizes)))
print("[anchor] aspect_ratios:", aspect_ratios)

"""# 概要・目的（要点）

* **どのようなコード？**
  画像の中の「物の大きさ」をざっくり数え上げて、**検出用の箱サイズのおすすめセット**を自動で提案するスクリプトです。

* **目的**
  学習データに合わせて箱サイズを現実的な値に調整し、**見逃しを減らし、重なりやダブりを改善**することを狙います。

# 入力となる変数

* `all_anno_list`：アノテーションファイル（Pascal VOC 形式）のパス一覧。
* `voc_classes`：対象とするクラス名の集合。
* `INPUT_SIZE`：学習時の入力画像サイズ（ここでは 300 前提の処理）。

# 処理の流れ

1. **箱の「だいたいの一辺の長さ」を集める**

   * すべてのアノテーションを読み、各物体の箱（四角形）を取り出します。
   * 画像サイズの違いをそろえるために、**入力サイズ相当に換算**した「だいたいの一辺の長さ」を計算し、リストに貯めます。

     * ※ 実装では幅と高さから“相当する一辺”を計算していますが、ここでは直感的に「サイズ感」と理解してOK。

2. **全体の分布をざっくり要約**

   * 集めた「サイズ感」について、サンプル数・最小値・真ん中あたり・最大値を表示します。

3. **6段の箱サイズを提案**

   * 「サイズ感」の分布をもとに、**下から 10% / 25% / 40% / 55% / 70% / 85%** の位置を使って、6つの **min\_sizes** を提案します。
   * **max\_sizes** は基本的に **min + 40** としつつ、**小さすぎない・大きすぎない**ように上下限を調整します。
   * **aspect\_ratios**（縦横の細長さの候補）は、一般的な設定を提示し、細長い物が少なければ `[2]` 寄りにする案を示します。

4. **結果を表示**

   * 提案された **min\_sizes / max\_sizes / aspect\_ratios** をログに出力します。

# 出力

* 例）

  * `[anchor] samples: ... min/med/max: ...`
  * `[anchor] suggested min_sizes: [...]`
  * `[anchor] suggested max_sizes: [...]`
  * `[anchor] aspect_ratios: [[2], [2, 3], ...]`

これらを、モデルの「デフォルトの箱」を作る部分（アンカー生成）に反映します。

# 典型的な落とし穴（上位3つ）と対策

1. **外れ値に引っ張られる**

   * 症状：極端に大きい／小さい箱が少数あると、提案サイズが不自然になる。
   * 対策：分位点のパーセンタイルを**適度に見直す**（例：5〜90% にする、上側の比率を少し下げる等）。または極端なサイズを**事前に除外**。

2. **画像サイズの記録ミスや壊れたアノテ**

   * 症状：画像幅・高さが不正、箱が画像外や負の幅／高さで計算される。
   * 対策：座標は**0未満を切り捨て**、最小1ピクセルにするなどのガードを維持。サニティチェック（サイズ欠損・座標の逆転）を**別途実施**。

3. **クラス混入や対象外の箱が混ざる**

   * 症状：想定外のクラスの箱が計算に入って分布が歪む。
   * 対策：`voc_classes` に含まれないラベルを**スキップ**する現在の実装を維持し、**未知ラベルの検出ログ**を別途仕込むと安心。

# 用語ミニ解説

* **箱サイズ（アンカー）**：検出モデルが最初に置く「基準の四角形」。実際の物体に合わせて調整すると、当たりやすくなる。
* **分位点（パーセンタイル）**：データを小さい順に並べたときの位置の目安。たとえば 70% は「下から 70% の位置」。分布の形に引っ張られにくく、設定の基準に使いやすい。
* **aspect ratios**：箱の縦横比の候補。細長い物が多いデータなら比率の選択肢を増やす、少ないならシンプルにする。
* **入力サイズ相当に換算**：元画像の大きさがバラバラでも、学習時の一辺（ここでは 300）に合わせたスケールで比較できるようにすること。

# データローダ
"""

# =============================================================================
# データローダ設定（詳細コメント付き）
#
# 目的（直感的）：
#   - 「統計・キャリブ用」は“画像の見た目を一切いじらず”（=Augなし）に読み込む
#   - 「学習・検証」も、必要に応じて“推論と同じ見た目”で読み込めるように揃える
#   - 読み込みの挙動をコードで強制し、うっかりAugが入る事故を防ぐ
#
# 前提：
#   - VOCDataset, DataTransform, Anno_xml2list, od_collate_fn, DataLoader が既に利用可能
#   - train_img_list / train_anno_list / val_img_list / val_anno_list が準備済み
#   - voc_classes が定義済み（例：['crab1', 'crab2']）
# =============================================================================
from utils.config import build_cfg, SSD_DEFAULTS
# 画像前処理で使う“色の基準値”（BGR順）：
#   画像の色をある程度そろえるための引き算基準（典型的にCaffe系で使う並び）
over = {
    "num_classes": len(voc_classes) + 1,
    "input_size": INPUT_SIZE,                 # 既存変数を活かす場合
    # 必要なら anchors を解析結果で上書き
    # "min_sizes": list(min_sizes),
    # "max_sizes": list(max_sizes),
    # "aspect_ratios": aspect_ratios,
    "detect": {
        "conf_thresh": 0.3,                   # ここで一元管理しても良い
        "nms_thresh" : 0.45,
        "top_k"      : 400
    },
    # 学習損失やAugも一括調整可
    # "loss": {...}, "preprocess": {...}
}
ssd_cfg = build_cfg(over)
color_mean = tuple(ssd_cfg["preprocess"]["color_mean"])
input_size = int(ssd_cfg["input_size"])
# -----------------------------------------------------------------------------
# ★ 重要：統計（キャリブ）用データセットは「trainの画像を使う」が「phase='val'」
#   - phase='val' は「Augなし」を意味する運用にする
#   - 統計は推論と同じ見た目で取らないと、現場での挙動とズレる
# -----------------------------------------------------------------------------
calib_dataset = VOCDataset(
    img_list=train_img_list,            # 学習用の画像をそのまま使う
    anno_list=train_anno_list,          # 対応するアノテーション
    phase="val",                        # ← Augなし・推論と同一の見た目で読み込む
    transform=DataTransform(input_size, color_mean),           # 画像の基本整形（リサイズ/色基準合わせ等）
    transform_anno=Anno_xml2list(voc_classes)                  # アノテ(XML)を数値配列に
                   if 'Anno_xml2list' in globals() else None   # 万一未定義なら安全にNone
)

# 統計を取るためのデータローダ：
#   - shuffle=False：順番はどうでも良いが、再現性を保つために固定
#   - drop_last=False：端数バッチも捨てずに使う（統計なので全件を反映したい）
calib_dataloader = DataLoader(
    calib_dataset,
    batch_size=32,            # GPUメモリに合わせて調整。統計なら大きめでもOK
    shuffle=False,            # 再現性のため固定
    collate_fn=od_collate_fn, # 可変長のアノテを束ねるための関数（いつものやつ）
    drop_last=False
)

# ログで状態を明示し、誤設定を早期に検出
print(f"[calib] dataset phase: {calib_dataset.phase}  (expect 'val')")
print(f"[calib] images: {len(calib_dataset)}  batch_size={32}")
assert calib_dataset.phase == "val", "calib_dataset は必ず phase='val'（No-Aug）にしてください。"

# -----------------------------------------------------------------------------
# 1) いつもの前処理（DataTransform）を準備しておく
# -----------------------------------------------------------------------------
base_tf = DataTransform(input_size, color_mean)

# -----------------------------------------------------------------------------
# 2) No-Aug ラッパ：どんな引数で呼ばれても 'phase' を 'val' に差し替える
#   - 既存のTransform実装が (img, phase, boxes, labels) の順を想定しているケースに対応
#   - kwargs形式（phase='train' など）で渡されても上書きする
#   - これにより、呼び出し元がうっかり 'train' を渡しても常に「Augなし」へ強制できる
# -----------------------------------------------------------------------------
class NoAugTransform:
    def __init__(self, base_tf):
        self.base_tf = base_tf  # 実仕事は既存のDataTransformに任せる
    def __call__(self, *args, **kwargs):
        # 典型的な呼び出し：(img, phase, boxes, labels)
        if len(args) >= 2:
            args = list(args)   # タプルはイミュータブルなのでリストへ
            args[1] = 'val'     # ← ここがキモ：どんな状況でも“評価モード”に固定
            return self.base_tf(*args, **kwargs)
        # kwargsで渡された場合でも 'phase' を上書き
        if 'phase' in kwargs:
            kwargs['phase'] = 'val'
        return self.base_tf(*args, **kwargs)

# -----------------------------------------------------------------------------
# 学習データセット：
#   - 見かけは phase="train" だが、Transform側で 'val' に強制するので実質Augなし
#   - 「推論と同じ見た目で学習・検証したい」場合の安定運用に向く
#   - ※ もちろん、積極的にAugを使いたい実験では NoAugTransform を外せば良い
# -----------------------------------------------------------------------------
train_dataset = VOCDataset(
    train_img_list, train_anno_list, phase="train",
    transform=NoAugTransform(base_tf),        # ← 内部で常に 'val' に上書き（Augなし）
    transform_anno=Anno_xml2list(voc_classes) # アノテ整形は学習・検証・統計で揃える
)

# -----------------------------------------------------------------------------
# 検証データセット：
#   - もともと phase="val" なので、そのままDataTransformでOK（Augなし）
#   - 学習・検証・統計の“見た目の一貫性”を保つことがポイント
# -----------------------------------------------------------------------------
val_dataset = VOCDataset(
    val_img_list, val_anno_list, phase="val",
    transform=DataTransform(input_size, color_mean),  # ← No-Aug固定
    transform_anno=Anno_xml2list(voc_classes)
)

# =============================================================================
# 実務メモ（運用のコツ）
#   - キャリブ/統計は必ず「Augなし」で実施：推論と同じ見た目で基準値を作ること
#   - 事故防止：NoAugTransform と assert で二重にガード（レビューでも意図が伝わる）
#   - color_mean はBGR順前提：前処理チェーン全体のチャネル順（BGR/ RGB）を一貫させる
#   - バッチサイズはメモリと相談：統計なら大きめ→高速、学習ならOOMに注意
# =============================================================================

"""# 概要・目的（要点）

* **どのようなコード？**
  学習用画像の読み込みルールを整えて、「統計を取るときは一切のランダム加工をしない」「評価と同じ見た目で読む」を徹底するための設定コードです。

* **目的**

  1. 後段で使う基準値・統計を、**加工なしの素の画像**から正しく計算する。
  2. 学習・評価・統計の読み込み手順を**明確に分けて安定化**させる。

# 入力となる変数

* `train_img_list`, `train_anno_list`：学習用の画像パスと注釈パスのリスト
* `val_img_list`, `val_anno_list`：検証用の画像パスと注釈パスのリスト
* `voc_classes`：対象クラス名の一覧
* `DataTransform`：画像の前処理（サイズ合わせや色の基準合わせなど）
* `Anno_xml2list`：注釈（XML）を数値リストへ変換する関数
* `VOCDataset`：画像と注釈をセットで読むためのデータ集
* `DataLoader`：データ集からミニバッチを作って取り出す仕組み
* `od_collate_fn`：可変長の注釈をうまく束ねるための関数
* `color_mean=(104,117,123)`, `input_size=300`：色の基準値と入力画像の大きさ

# 処理の流れ

1. **統計用データ集（加工なし）**

   * `calib_dataset` を作るとき、**学習用の画像を使うが、モードは必ず "val"** にする。
   * これは「**一切のランダム加工をかけない**で読む」合図。推論時と同じ見た目で統計を取るため。
   * そのための読み出し器 `calib_dataloader` を用意（順番固定・シャッフルなし）。

2. **通常の学習・検証データ集**

   * いつもの前処理 `DataTransform` を `base_tf` として準備。
   * **NoAugTransform** というラッパーを定義し、**受け取ったモードを強制的に "val" に上書き**してから `base_tf` を呼ぶ。

     * これにより、`phase="train"` と書いてあっても、**実際には常に加工なし**で通る。
   * `train_dataset` にはこの `NoAugTransform` を適用（＝学習でも加工なしで整合を保つ用途に使える）。
   * `val_dataset` はもともと "val" なので、そのまま加工なしの `DataTransform` を適用。

> まとめると
>
> * 統計用：学習画像を使うがモードは "val"（加工なし）
> * 学習用：ラッパーで強制的に "val"（加工なし）
> * 検証用：もともと "val"（加工なし）
>   → **どの経路も推論と同じ見た目**で読み込める。

# 出力

* 画面表示：

  * `[calib] dataset phase: val (expect 'val')`
  * `[calib] images: ...  batch_size=32`
* 使えるオブジェクト：

  * `calib_dataset`, `calib_dataloader`（統計計算用）
  * `train_dataset`, `val_dataset`（学習・検証用。ここでは両方とも加工なし運用に揃えている）

# 典型的な落とし穴（上位3つ）と対策

1. **うっかり加工が入る**

   * 症状：統計や評価の結果がブレる（学習時と推論時の見た目が違う）。
   * 対策：**モードを必ず "val" に固定**する。`assert calib_dataset.phase == "val"` のようなガードを維持。`NoAugTransform` で二重に予防。

2. **シャッフルしてしまう**

   * 症状：統計の再現性が落ちる、同じ条件で値がズレる。
   * 対策：**統計用のローダは `shuffle=False`** を徹底。順序はどうでもいいが、安定性のため固定。

3. **注釈変換の不一致・未設定**

   * 症状：学習・検証・統計で注釈の形式がズレ、後段の処理が落ちる/精度が下がる。
   * 対策：`transform_anno=Anno_xml2list(voc_classes)` を**統一**。存在チェック（`if 'Anno_xml2list' in globals()`）も残しておくと安全。

# 用語ミニ解説

* **前処理（Transform）**：画像を決まったサイズや色の基準に合わせること。
* **加工（Augmentation）**：ランダムな回転や色変えなど、学習を強くするための見た目の変化。ここでは**統計や評価では禁止**。
* **モード（phase）**：読み込みの動作モード。"train" は加工あり、"val" は加工なし、のように切り替えるスイッチ。
* **データ集（Dataset）**：画像と注釈のセットを、1枚ずつ取り出せる形にしたもの。
* **読み出し器（DataLoader）**：データ集からミニバッチを作って供給する仕組み。
* **束ね関数（collate\_fn）**：画像ごとに注釈の数が違っても、ひとまとめのバッチにできるよう整形する関数。
* **統計（Calibration用の統計）**：後工程で使う基準値づくり。**見た目をいじらない**ことが重要。

"""

# =============================================================================
# DataLoader：クラス/背景の「出現比率」を狙って調整するためのサンプラ設定（詳解）
#
# ねらい（直感的な説明）：
#   - 学習で見る画像の順番・割合を「こちらの狙い」に寄せる。
#   - 具体的には「少ないクラスは多めに」「背景は目標の割合（例：30%）」で出す。
#   - これにより、クラスの偏りや背景だらけの学習になってしまう事態を避ける。
#
# 重要ポイント：
#   - WeightedRandomSampler は「各サンプルの重み」に基づいて抽選する。
#   - 今回は
#       * crab1, crab2：枚数が少ないほど重く（当たりやすく）
#       * 背景：最終的に r_bg（例：0.30）に近い割合になるように調整
#     という方針。
#   - sampler を使うときは shuffle=False（併用NG）。
# =============================================================================

from torch.utils.data import WeightedRandomSampler
import re

# 各サンプル（画像）の「タグ」を決める関数。
#   - フォルダ名やパスから 'crab1' / 'crab2' / 'bg' を判定するルールにしている。
#   - Windowsの「\」とUnix系の「/」の違いを吸収するため、置換してから判定。
def tag_of(img_path):
    # 例:
    #   ./od_crabs/crab1/xxx.jpg  → 'crab1'
    #   ./od_crabs/crab2/yyy.jpg  → 'crab2'
    #   ./od_crabs_ood/zzz.jpg    → 'bg'（背景）
    if "od_crabs_ood" in img_path.replace("\\","/"):
        return "bg"
    # 正規表現で /crab1/ または /crab2/ を検出
    m = re.search(r"/(crab1|crab2)/", img_path.replace("\\","/"))
    return m.group(1) if m else "bg"  # どちらも見つからなければ背景扱い

# 学習用画像リストに対してタグ付けを一括実行
tags = [tag_of(p) for p in train_img_list]

# クラスごとの枚数を数える（N1: crab1, N2: crab2, Nb: 背景）
N1 = sum(t=="crab1" for t in tags)
N2 = sum(t=="crab2" for t in tags)
Nb = sum(t=="bg"    for t in tags)

# -------------------------------------------------------------------------
# 逆頻度ベースの重み（crab1/crab2）：
#   - 枚数が少ないクラスほど重みが大きくなるように、ざっくり「1 / 枚数」にする。
#   - これで「少ないクラスが当たりやすく」なり、1:1 に近づく。
#   - max(1, N) でゼロ割り等の事故を防止。
# -------------------------------------------------------------------------
w1 = 1.0 / max(1, N1)
w2 = 1.0 / max(1, N2)

# -------------------------------------------------------------------------
# 背景割合 r_bg の目標（例：0.30）：
#   - 学習全体として「背景はこのくらいの比率で出てきてほしい」という目安値。
#   - w_bg はこの目標に合わせて調整するための重み。
#   - 分かりやすく言うと：
#       「正例2クラス（crab1, crab2）と背景の“引かれ方”のバランスを
#        r_bg に近づけるように、背景の重みを設定する」
#   - 2.0 は「正例クラス数（=2）」を表す。クラスが増えたらここも合わせて変える。
# -------------------------------------------------------------------------
r_bg = 0.30
w_bg = (2.0 * r_bg) / (max(1, Nb) * (1.0 - r_bg))  # 直感：背景が“このくらいの頻度で当たる”重み

# 各サンプル（画像）に対応する「重み」を並べたリストを作る
#   - crab1 → w1,  crab2 → w2,  背景 → w_bg
sample_weights = []
for t in tags:
    if t == "crab1": sample_weights.append(w1)
    elif t == "crab2": sample_weights.append(w2)
    else: sample_weights.append(w_bg)

# -------------------------------------------------------------------------
# WeightedRandomSampler の作成
#   - weights     : 先ほど作った「サンプルごとの重み」
#   - num_samples : 1エポックで「何回」引くか。ここでは“データ数ぶん”引く＝1巡相当。
#   - replacement : 置換あり（同じサンプルを何度も引いてOK）
#                   → 少数クラスでも、重み次第でエポック内に複数回登場できる
# -------------------------------------------------------------------------
sampler = WeightedRandomSampler(
    weights=torch.tensor(sample_weights, dtype=torch.double),
    num_samples=len(train_img_list),
    replacement=True
)

# 注意：sampler を指定する場合、shuffle=True は使わない（挙動が競合するため）
train_dataloader = data.DataLoader(
    train_dataset,
    batch_size=32,
    sampler=sampler,    # ← ここが肝。サンプル数によって選ばれる確率が変わる
    shuffle=False,      # ← sampler 併用時は必ず False
    collate_fn=od_collate_fn,
    drop_last=False
)

# 検証側は通常どおり（順不同にしないため shuffle=False のまま）
val_dataloader = data.DataLoader(
    val_dataset,
    batch_size=3,
    shuffle=False,
    collate_fn=od_collate_fn
)

# 目視で確認：クラス枚数と、設定された重み・背景目標
print(f"[sampler] N1={N1} N2={N2} Nb={Nb}  w1={w1:.4g} w2={w2:.4g} wbg={w_bg:.4g}  r_bg={r_bg}")

"""# 概要・目的（要点）

* **どのようなコード？**
  学習に使う画像の取り出し順をコントロールして、**クラス間の枚数差や背景の多さに引っ張られないようにする**ための設定です。

* **目的**

  * 少ないクラスも**ほどよい頻度**で学習されるようにする。
  * **背景の混ぜる割合**（例：30%）を狙って保つ。
  * その結果、学習の**偏りや過学習**を抑え、安定したモデルづくりを狙う。

# 入力となる変数

* `train_img_list`：学習用の画像パス一覧
* `train_dataset` / `val_dataset`：作成済みのデータセット
* `od_collate_fn`：バッチ化のための関数
* `torch.utils.data.WeightedRandomSampler`：重みに応じてサンプルを引く仕組み（本コードの要）
* クラス判定に使う規則：

  * パスに `od_crabs_ood` を含む → 背景（`bg`）
  * パスの中に `/crab1/` または `/crab2/` が含まれる → それぞれのクラス

# 処理の流れ

1. **各画像にタグ付け**

   * 画像パスから `crab1` / `crab2` / `bg` のどれかを判定（正規表現とパス判定）。

2. **枚数カウント**

   * `N1`（crab1 の枚数）・`N2`（crab2 の枚数）・`Nb`（背景の枚数）を数える。

3. **重みを計算**

   * クラスは「枚数が少ないほど重く」なるような逆数ベースの重みをセット（おおまかに 1/枚数）。
   * 背景は、**最終的に背景の出現割合が `r_bg`（例：0.30）に近づく**ように重みを調整。

4. **重み一覧を作る**

   * 画像ごとに対応する重みを配列にして、`WeightedRandomSampler` に渡す。

5. **データローダを作る**

   * `train_dataloader` は **sampler を使用**（`shuffle=False` にすること）。
   * `val_dataloader` は通常の並び（`shuffle=False`）。

6. **ログ出力**

   * 各クラスの枚数と、設定した重み・背景比率目標を表示して確認。

# 出力

* 例：

  ```
  [sampler] N1=120 N2=80 Nb=300  w1=0.008333 w2=0.0125 wbg=0.002571  r_bg=0.3
  ```

  * `N1/N2/Nb`：枚数
  * `w1/w2/wbg`：サンプル抽選に使う重みの目安
  * `r_bg`：目標の背景割合（ここでは 30%）

# 典型的な落とし穴（上位3つ）と対策

1. **`sampler` と `shuffle` を併用してしまう**

   * 症状：意図しない順序や比率になる。
   * 対策：`sampler=...` を使うときは **必ず `shuffle=False`**。本コードはその形にしている。

2. **タグ付けルールの取りこぼし**

   * 症状：クラスが正しく判定できず、背景扱いになる（比率が崩れる）。
   * 対策：パス規則を**実データに合わせて確認**。`tag_of` の正規表現やパス判定を、実フォルダ構成に沿って調整。

3. **背景比率の目標と重みのバランスが合わない**

   * 症状：実際のミニバッチ内の背景割合が狙いから外れる。
   * 対策：`r_bg` を**小刻みに調整**。また、`num_samples`（1エポック当たりの引き数）や `replacement=True`（置換あり）も挙動に影響するので確認する。

# 用語ミニ解説

* **重み付きサンプリング**：各データに重要度（重み）をつけ、重いものが当たりやすくなる取り出し方。
* **置換ありサンプリング**：同じデータを複数回引いてよいルール。少ないクラスでも十分回ってくる。
* **背景割合**：学習時に背景画像をどのくらいの頻度で見せるかの目標。多すぎると前景を覚えにくく、少なすぎると誤検出が増えることがある。
* **逆頻度重み**：枚数が少ないクラスほど重みを大きくして、当たりやすくする考え方のこと。

---

> ヒント：学習ログで、実際のバッチ内のクラス比・背景比を定期的に表示すると、狙いどおりの混ざり方になっているか素早く確認できます。

# モデル設定と初期化（冒頭のみdataloader)
"""

# =============================================================================
# 目的（ざっくり）
# -----------------------------------------------------------------------------
# - 学習/検証の DataLoader をひとまとめにし、あとから参照しやすくする
# - SSD300 の各種設定（入力サイズ・アンカーサイズ・しきい値など）を用意
# - データ分布から計算した「提案アンカー」を設定に上書きして反映
# - モデルの作成、事前学習重みのロード、チェックポイントからの再開
# - 未初期化レイヤの初期化、GPU/CPU の自動選択、最終的な準備完了ログ
# =============================================================================


# ------------------------------------------------------------------
# 1) DataLoader を辞書にまとめる（後段で参照しやすくするため）
# ------------------------------------------------------------------
dataloaders_dict = {"train": train_dataloader, "val": val_dataloader}
# 例：学習ループ内で dataloaders_dict['train'] / ['val'] と取り出せる


# ------------------------------------------------------------------
# 3) アンカーのスケール・縦横比を「提案値」に上書き
#    - 直前の解析で得た min_sizes / max_sizes / aspect_ratios を反映
#    - データの実態に合わせることで当たりやすさを高める
# ------------------------------------------------------------------
ssd_cfg['min_sizes'] = [int(x) for x in list(min_sizes)]
ssd_cfg['max_sizes'] = [int(x) for x in list(max_sizes)]
ssd_cfg['aspect_ratios'] = aspect_ratios
print("[cfg/train] anchors updated:", ssd_cfg['min_sizes'], ssd_cfg['max_sizes'])


# ------------------------------------------------------------------
# 4) SSD ネットワークの作成
#    - phase="train"：学習用として作る（内部で挙動が少し変わる実装が多い）
# ------------------------------------------------------------------
net = SSD(phase="train", cfg=ssd_cfg)


# ------------------------------------------------------------------
# 5) 事前学習済みのバックボーン重みをロード
#    - まずファイルパスの存在チェック（typoや未配置の早期検出）
#    - CPU で一度読み込んでから、あとでモデルごとデバイスに乗せる
# ------------------------------------------------------------------
PROJ_ROOT = "/content/drive/MyDrive/SSD"
WEIGHT_PATH = os.path.join(PROJ_ROOT, "weights", "vgg16_reducedfc.pth")
assert os.path.exists(WEIGHT_PATH), f"not found: {WEIGHT_PATH}"

vgg_weights = torch.load(WEIGHT_PATH, map_location="cpu")
net.vgg.load_state_dict(vgg_weights)  # VGG 部分へ流し込む


# ------------------------------------------------------------------
# 6) チェックポイントからの再開（あれば）
#    - last.pth があれば直近状態を復元、なければ最初から
#    - strict=False：レイヤ構成の差異（例：ABN追加）に強く、読み込めるところだけ読む
#    - missing/unexpected を出力して、どのレイヤが合わなかったか可視化
# ------------------------------------------------------------------
RESUME_PATH = os.path.join(WEIGHTS_DIR, 'last.pth')
ckpt = None
start_epoch = 0

if os.path.exists(RESUME_PATH):
    ckpt = torch.load(RESUME_PATH, map_location='cpu')
    ret = net.load_state_dict(ckpt['model'], strict=False)  # ← 安全ロード
    print("[resume] loaded:", RESUME_PATH)
    print("[resume] missing keys:", ret.missing_keys)       # 期待したが見つからない
    print("[resume] unexpected keys:", ret.unexpected_keys) # 予期しない余分なキー
    start_epoch = ckpt.get('epoch', -1) + 1                 # 再開エポックを算出
else:
    print("[resume] no checkpoint found. start from scratch.")
    start_epoch = 0


# ------------------------------------------------------------------
# 7) 未初期化レイヤ（extras/loc/conf）に He(Kaiming) 初期化
#    - 事前重みを入れたのは VGG 主体。追加ヘッドは自分で良い初期値を与える
#    - Conv2d の重みだけを対象にし、bias は 0 に初期化
# ------------------------------------------------------------------
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        init.kaiming_normal_(m.weight.data)    # He 初期化
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)

if ckpt is None:
    # “最初から”の場合のみ初期化（再開時はチェックポイントの値をそのまま使う）
    net.extras.apply(weights_init)
    net.loc.apply(weights_init)
    net.conf.apply(weights_init)


# ------------------------------------------------------------------
# 8) デバイス（GPU/CPU）を決め、モデルを移動
#    - 利用可能なら GPU（cuda:0）、なければ CPU を自動選択
#    - cuDNN のベンチマークを ON にして、畳み込みの高速設定を有効化
# ------------------------------------------------------------------
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("使用デバイス：", device)
net.to(device)
torch.backends.cudnn.benchmark = True

print('ネットワーク設定完了：学習済みの重みをロードしました')

"""# 概要・目的（要点）

このコードは、**学習で使うデータの入口（データローダ）と、物体検出モデルの設定・初期化・再開**をひとまとめにした準備スクリプトです。
目的は、**学習・評価で使うデータの流れを束ね**、**モデル設定（箱サイズなど）をデータ由来の提案値に合わせて上書き**し、**重みの読み込みや途中再開**まで一気に整えることです。

# 入力となる変数

* `train_dataloader`, `val_dataloader`：前段で作った学習・検証用の取り出し口
* `voc_classes`：対象クラス名の一覧（背景を含まないクラス数）
* `INPUT_SIZE`：モデルに入れる画像の一辺
* `min_sizes`, `max_sizes`, `aspect_ratios`：前段の統計から提案された箱サイズ・縦横比
* `SSD`, `nn`, `init`, `torch`, `data`：モデル定義や初期化に使うモジュール一式
* `WEIGHTS_DIR`, `PROJ_ROOT`：重みや作業ディレクトリのパス
* 事前に用意されたファイル

  * `vgg16_reducedfc.pth`：バックボーンの初期重み
  * `last.pth`：前回学習のチェックポイント（あれば再開）

# 処理の流れ

1. **データローダをひとまとめに**

   * `{"train": train_dataloader, "val": val_dataloader}` という辞書にまとめ、後段で参照しやすくします。

2. **モデル設定オブジェクトを用意**

   * 入力サイズ、各段の特徴マップの大きさ、箱サイズ候補、縦横比、しきい値、上限件数などを一括で定義します。
   * さらに、**前段で計算した提案値**で `min_sizes` / `max_sizes` / `aspect_ratios` を**上書き**して、データに合わせます。

3. **モデルを作成し、重みを読み込む**

   * `SSD(phase="train", cfg=ssd_cfg)` でモデルを組み立てます。
   * バックボーンの初期重み（`vgg16_reducedfc.pth`）を読み込みます。
   * 直近のチェックポイント（`last.pth`）があれば**途中再開**（`strict=False` で安全にロード）。

4. **未学習の層は初期化**

   * 事前重みがない追加層（extras/loc/conf）には**He 初期化**を適用し、学習を安定させます。

5. **デバイスへ移動して最終準備**

   * CUDA があれば GPU、なければ CPU を自動選択。
   * 高速化のための設定（`cudnn.benchmark=True`）を有効化。
   * ここまでで「ネットワーク設定完了」ログ。

# 出力

* 代表的な画面ログ

  * `[cfg/train] anchors updated: ...`：箱サイズの上書き内容
  * `[resume] loaded: ...` / `no checkpoint found...`：再開の有無
  * `使用デバイス： cuda:0`（または cpu）
  * `ネットワーク設定完了：学習済みの重みをロードしました`

# 受け渡しているデータとその内容

* **入→モデル設定**

  * クラス数は `len(voc_classes) + 1`（背景ぶんを足す想定）。
  * 箱サイズと縦横比は、**前段の統計から得た提案値**で置き換え（学習・推論の整合性を高める）。
  * 推論時の動作（信頼度しきい値、重なり抑制の基準、最大保持数）もここで固定。

* **入→重みロード**

  * 事前学習済みのバックボーン重みを読み込み。
  * 途中再開がある場合は、**不足/過剰なキーの一覧**もログに出し、何がロードされなかったかを可視化。

* **出→学習ループ**

  * 初期化済みの `net`、`dataloaders_dict`、`start_epoch` を持って、以降の学習ループへ。

# 典型的な落とし穴（上位3つ）と対策

1. **箱サイズ・縦横比の不整合**

   * 症状：当たりにくい箱が多く、検出が不安定。
   * 対策：**前段の提案値で必ず上書き**する（本コードどおり）。上書き後のログを確認して、意図した値になっているかチェック。

2. **重みファイルの取り違え・欠損**

   * 症状：ロード失敗、`missing_keys` が大量に出る、学習が極端に不安定。
   * 対策：`assert os.path.exists(WEIGHT_PATH)` のような**存在チェックを維持**。`strict=False` ロード時は**missing/unexpected の一覧**を必ず目視。

3. **デバイス設定の不一致**

   * 症状：CPU で動いてしまい極端に遅い、マルチ GPU 環境で想定外のデバイスに載る。
   * 対策：`torch.cuda.is_available()` の判定結果を**ログで確認**し、`net.to(device)` が呼ばれていることを確認。必要なら環境変数や引数で明示的に指定。

# 用語ミニ解説

* **箱サイズ（アンカー）**：モデルが最初に置く基準の四角形。データのサイズ分布に合わせると当たりやすい。
* **縦横比（aspect ratios）**：箱の横長・縦長の候補。細長い物が多いなら候補を増やす。
* **しきい値（conf\_thresh / nms\_thresh / top\_k）**：確からしさの最低ライン、重なりの整理基準、保持する最大件数。
* **バックボーン**：特徴を抜き出す土台のネットワーク。ここでは VGG の軽量版重みを使用。
* **He 初期化（Kaiming）**：畳み込み層の重みを安定させる代表的な初期化方法。
* **チェックポイント**：学習途中の保存データ。再開するときに使う。

---

> ヒント：\*\*「箱サイズの上書きログ」・「missing/unexpected keys」・「使用デバイス」\*\*の3つを毎回確認すると、初期トラブルの大半を素早くつぶせます。
レイヤーの初期化を行った場合にログが出るように変更すべきでは？

# 最適化の準備
"""

# =============================================================================
# 追学習（ファインチューニング）向けの設定：
#   - 検出パートは “小さい学習率” でそっと微調整
#   - AE（オートエンコーダ）パートは “中くらいの学習率” で少し積極的に更新
#   - Optimizer の状態はリセットして、過去の癖を持ち込まない
# =============================================================================

# -----------------------------
# 1) パラメータの仕分け（検出パート）
# -----------------------------
det_params = []
# net の中で検出に関わる主要ブロック（VGG本体、追加ブロック、位置回帰ヘッド、分類ヘッド）
# それぞれの .parameters() を列挙して det_params にまとめる。
for mod in (net.vgg, net.extras, net.loc, net.conf):
    det_params += list(mod.parameters())

# -----------------------------
# 2) パラメータの仕分け（AEパート）
# -----------------------------
ae_params = []
# AE 関連の層は、モデルに無い構成もあり得るため hasattr で存在チェックしながら集める。
# ここではプロジェクタ（ae_proj*）とデコーダ（ae_dec*）を対象にしている想定。
for name in ('ae_proj2','ae_proj3','ae_proj4','ae_dec2','ae_dec3','ae_dec4'):
    if hasattr(net, name):
        ae_params += list(getattr(net, name).parameters())

# -----------------------------
# 3) 学習率の割り当て（パラメータグループ）
# -----------------------------
param_groups = []
if det_params:
    # 検出パートは “すでに良い性能が出ている可能性が高い” ため、小さく微調整
    param_groups.append({'params': det_params, 'lr': 1e-4})
if ae_params:
    # AE パートは “新規/変更あり” の想定があるため、やや積極的に動かす
    param_groups.append({'params': ae_params, 'lr': 5e-4})

# -----------------------------
# 4) Optimizer の作成（SGD）
# -----------------------------
# momentum: 勢い（局所的なばらつきに流されにくくする）
# weight_decay: 重み減衰（過学習の抑制）
optimizer = optim.SGD(param_groups, momentum=0.9, weight_decay=5e-4)

# ★ 追学習では “以前の optimizer state（過去の学習履歴）” を読まずにリセットした方が安定しやすい。
#   過去の学習率スケジュールやモーメントが、今回の新しい学習方針と合わないことがあるため。
print("[resume] optimizer state is RESET for fine-tuning.")

"""# 概要

このコードは、**学習済みの検出モデルを“そっと微調整”しながら、同じネットの中にある自動修復パート（AE）を“少し積極的に学習”させるための準備**をしています。
やっていることはシンプルで、①モデルの中から学習させたい部品を集め、②それぞれに**別々の学習率**を割り当て、③**新しい最適化器（SGD）**を作って**状態をリセット**する、の3ステップです。

---

# 目的

* \*\*検出パート（VGGやSSDヘッド）\*\*は既にそれなりに良いので、**小さい学習率**で形を崩さず微調整。
* **AEパート**は新しく追加・変更した可能性があるので、**中くらいの学習率**でしっかり更新。
* **最適化器の過去の履歴を引きずらない**よう、**状態をリセット**して再スタート。

---

# 入力となる変数

* `net`
  SSD系のモデル本体。想定されている主な属性：

  * `net.vgg`, `net.extras`, `net.loc`, `net.conf`（検出パート）
  * `net.ae_proj2/3/4`, `net.ae_dec2/3/4`（AEパート。存在しない構成もある）
* （暗黙の前提）`import torch.optim as optim` が済んでいること

---

# 処理の流れ

1. **検出パートのパラメータを集める**

   * `net.vgg, net.extras, net.loc, net.conf` から `.parameters()` を列挙し、`det_params` にまとめます。
     → ここに集めた重みは**小さめの学習率**を後で設定します。

2. **AEパートのパラメータを（存在チェックしながら）集める**

   * `ae_proj2/3/4`, `ae_dec2/3/4` が**モデルに存在するかを `hasattr` で確認**し、あれば `.parameters()` を `ae_params` に追加。
     → ここに集めた重みは**中くらいの学習率**を設定します。

3. **パラメータグループを作る**

   * `det_params` が空でなければ `{'params': det_params, 'lr': 1e-4}` を追加。
   * `ae_params` が空でなければ `{'params': ae_params, 'lr': 5e-4}` を追加。
     こうして**グループごとに学習率を変える**準備が整います。

4. **SGD最適化器を新規作成**

   * `optimizer = optim.SGD(param_groups, momentum=0.9, weight_decay=5e-4)`
     **新しいインスタンス**を作ることで、**過去の学習履歴（モーメント等）を持ち込まない**状態で開始できます。

5. **リセットした旨をログ出力**

   * `print("[resume] optimizer state is RESET for fine-tuning.")`

---

# 出力

* 変数 `optimizer`

  * **学習率が部品ごとに設定された**、新規の `torch.optim.SGD` インスタンス。
  * ここから先は、この `optimizer` をいつも通り学習ループで使っていきます。

---

# 典型的な落とし穴（上位3つ）と対策

1. **`net` の構成が想定と違い、属性が存在しない**

   * 例：`net.loc` が無いモデルで `list(net.loc.parameters())` を呼ぶとエラー。
   * **対策**：検出パートも **`hasattr(net, 'loc')` のように存在チェック**してから集める。
     （AE側と同じやり方に揃えると安全）

2. **同じパラメータが複数のグループに入ってしまう**

   * 設計次第で**重複参照**が起こると、学習率や更新が**意図しない混在**になる。
   * **対策**：収集後に **IDベースで重複除去**する（`seen = set(); uniq = []; for p in det_params: ...`）
     もしくは**名前で厳密に振り分け**、集合演算で重複を排除。

3. **学習率の桁や比率のミス**

   * 例：AEに 5e-4 を想定しているのに 5e-2 と書いてしまう、逆に小さすぎて**全然学習が進まない**等。
   * **対策**：

     * **初回は短いステップで損失の減り方を確認**（急落や停滞がないか）。
     * **学習率をログに出す**、あるいは**学習率スケジューラ**で段階的に調整する。

---

# 用語ミニ解説

* **学習率（Learning Rate）**
  重みをどれだけ動かすかの幅。大きすぎると不安定、小さすぎると進まない。

* **最適化器（Optimizer）**
  重みを更新する仕組み。SGD は最も基本的な方法のひとつ。

* **モーメント（momentum）**
  直前の更新の“勢い”を加味してブレを抑えるための仕組み。

* **重み減衰（weight decay）**
  重みをほんの少しずつ小さく保つことで、過学習を抑えやすくする工夫。

* **パラメータグループ**
  モデルの部品ごとに**別の学習率や設定を与える仕組み**。微調整の自由度が上がる。

---

必要なら、**検出パート側にも `hasattr` を入れた堅牢版スニペット**や、**重複除去を入れた安全版**もすぐ用意できます。

# *AEヘルパ*
"""

# === AE前景損失ヘルパ（train_model 定義セルの直前に新規セルとして貼り付け） ===
# 目的：
# ・学習用のアノテーション（物体の箱とラベル）を「安全な形」に整える
# ・その箱の位置から「前景だけ」を示すマスクを作る
# ・AE（オートエンコーダ）の再構成誤差を、前景だけに限定して平均する
#   → 背景ノイズの影響を下げ、対象物の見た目を正しく復元できているかを評価しやすくする

import math, torch
import torch.nn.functional as F


def filter_known_targets(targets, num_known):
    """
    既知クラスだけ残し、ラベル表記を 1..num_known に統一して返すユーティリティ。
    - inputs
      targets   : list[Tensor[N,5]]  各テンソルは「x0,y0,x1,y1,label」を縦に並べた形
                  （座標は 0..1 でもピクセルでも良い。ここではラベルの正規化だけ行う）
      num_known : 既知クラス数（背景は含めない）
    - outputs
      out       : 上記と同じ構造の list[Tensor[N,5]]（未知・不正ラベルは除去済み）
    - ポイント
      * ラベルが 0..K-1 または 1..K のどちらで来ても、最終的に 1..K に合わせる。
      * 範囲外のラベルは捨てて、空テンソルで埋める（学習が止まらないように）。
    """
    out = []
    for t in targets:
        # t が None もしくは中身ゼロなら、空テンソルにしてスキップ
        if t is None or (torch.is_tensor(t) and t.numel() == 0):
            dev = t.device if torch.is_tensor(t) else torch.device("cpu")
            out.append(torch.zeros((0, 5), device=dev, dtype=torch.float32))
            continue

        t = t.clone()
        lbl = t[:, 4].long()  # ラベル列だけ取り出す（整数化）

        # ラベルの表記ゆれを吸収して 1..num_known に統一
        if (lbl.min() >= 1) and (lbl.max() <= num_known):
            # すでに 1..K ならそのまま
            ok = (lbl >= 1) & (lbl <= num_known)
        elif (lbl.min() >= 0) and (lbl.max() <= num_known - 1):
            # 0..K-1 なら +1 して 1..K に合わせる
            lbl = lbl + 1
            t[:, 4] = lbl.to(t.dtype)
            ok = (lbl >= 1) & (lbl <= num_known)
        else:
            # それ以外（範囲外）は -1（除外マーク）に落とす
            lbl2 = lbl.clone()
            bad = (lbl2 < 1) | (lbl2 > num_known)
            lbl2[bad] = -1
            t[:, 4] = lbl2.to(t.dtype)
            ok = (lbl2 != -1)

        # 有効な行だけ残す（無効なら空テンソルを入れる）
        t = t[ok]
        out.append(t if t.numel() else t.new_zeros((0, 5)))
    return out


def _roi_mask_from_targets(targets, feat_hw, input_size, device, inner_shrink=0.9):
    """
    特徴マップ上（H,W）に「前景マスク」を作る。
    - inputs
      targets     : list[Tensor[N,5]]  各テンソルは [x0,y0,x1,y1,label]
                    （座標がピクセル単位でも 0..1 でもOK）
      feat_hw     : 特徴マップの (H, W)（例：畳み込みの出力解像度）
      input_size  : 学習入力画像の一辺（例：300）。ピクセル座標→0..1 に直すときに使う
      device      : どのデバイスでテンソルを作るか（cuda など）
      inner_shrink: 箱を少し内側に縮める割合（縁で背景を巻き込みにくくするため）
    - output
      mask        : 形状 [B, 1, H, W] の前景マスク（1:前景, 0:それ以外）
    - ポイント
      * 座標がピクセル（値が 1 より十分大きい）なら input_size で割って 0..1 に正規化。
      * 0..1 の箱を特徴マップ座標へ写像し、該当領域を 1 に塗る。
      * inner_shrink で箱を少し縮めて、背景を拾いにくくする。
    """
    H, W = feat_hw
    N = len(targets)
    mask = torch.zeros((N, 1, H, W), device=device, dtype=torch.float32)

    for n, t in enumerate(targets):
        if t is None or (torch.is_tensor(t) and t.numel() == 0):
            # その画像に前景が無ければスキップ（マスクはゼロのまま）
            continue

        # coordinates: [x0,y0,x1,y1] のみ抜き出して作業
        t = t.to(device)
        xyxy = t[:, :4].clone()

        # ピクセル座標なら 0..1 に正規化（値が 1 より十分大きいと仮定）
        if float(xyxy.max().item()) > 1.5:
            s = float(input_size)
            xyxy /= s

        # 前景の各箱について、特徴マップ上のインデックス範囲を計算
        for b in xyxy:
            x0, y0, x1, y1 = [float(v) for v in b]

            # 万一のはみ出しを防ぐため 0..1 にクリップ
            x0 = max(0.0, min(1.0, x0)); y0 = max(0.0, min(1.0, y0))
            x1 = max(0.0, min(1.0, x1)); y1 = max(0.0, min(1.0, y1))
            if x1 <= x0 or y1 <= y0:
                # 幅や高さがゼロ以下なら無視
                continue

            # 箱の中心と幅高を求め、inner_shrink で箱を内側に縮める
            # （縁の背景混入を避けて、より純粋な前景だけを取りたい）
            cx, cy = 0.5*(x0+x1), 0.5*(y0+y1)
            bw, bh = (x1-x0)*inner_shrink, (y1-y0)*inner_shrink
            xs0 = max(0.0, cx - 0.5*bw); xs1 = min(1.0, cx + 0.5*bw)
            ys0 = max(0.0, cy - 0.5*bh); ys1 = min(1.0, cy + 0.5*bh)

            # 特徴マップ解像度にスケールして整数インデックスに
            ix0 = int(math.floor(xs0 * W)); ix1 = int(math.ceil(xs1 * W))
            iy0 = int(math.floor(ys0 * H)); iy1 = int(math.ceil(ys1 * H))

            # インデックス範囲の安全化（範囲外アクセス回避）
            ix0 = max(0, min(W-1, ix0)); ix1 = max(1, min(W, ix1))
            iy0 = max(0, min(H-1, iy0)); iy1 = max(1, min(H, iy1))
            if ix1 <= ix0 or iy1 <= iy0:
                continue

            # 対応領域を 1 にセット（ここが「前景」）
            mask[n, 0, iy0:iy1, ix0:ix1] = 1.0

    return mask


def recon_loss_from_feats_fg(feat_hat_list, feat_ref_list, targets, ssd_cfg,
                             kind="l1", inner_shrink=0.9):
    """
    前景だけを使って AE の再構成誤差を平均し、層（複数段）でも平均した 1つの値を返す。

    - inputs
      feat_hat_list : list[Tensor[B,C,H,W]]  各層の「再構成後の特徴マップ」
      feat_ref_list : list[Tensor[B,C,H,W]]  各層の「元の特徴マップ」
      targets       : list[Tensor[N,5]]      各画像のアノテーション（x0,y0,x1,y1,label）
      ssd_cfg       : dict                   入力サイズなどの設定（input_size を参照）
      kind          : "l1" または "l2"       誤差の種類（絶対値 or 二乗）
      inner_shrink  : float                  前景マスクを内側に縮める割合（背景混入の軽減用）

    - output
      損失のスカラー値（前景領域だけで平均した、各層の誤差の平均）

    - ポイント
      * 背景を無視して、対象物の領域だけで AE の誤差を測る。
      * 前景がゼロの時でも割り算が壊れないように、分母を下限でクリップ。
    """
    assert len(feat_hat_list) == len(feat_ref_list), "参照と再構成の層数は一致している必要があります。"
    device = feat_hat_list[0].device
    input_size = int(ssd_cfg.get('input_size', 300))

    losses = []
    for fh, fr in zip(feat_hat_list, feat_ref_list):
        # 再構成誤差マップ（チャネル方向の平均で 1 チャネル化）
        # kind="l1" → 絶対値、"l2" → 二乗を想定。初学者は「差の大きさ」と捉えてOK。
        e = (fh - fr)
        e = e.pow(2) if (kind == "l2") else e.abs()
        e = e.mean(dim=1, keepdim=True)  # [B,1,H,W] に集約
        B, _, H, W = e.shape

        # 前景マスク（特徴マップ解像度に合わせる点が重要）
        m = _roi_mask_from_targets(targets, (H, W), input_size, device, inner_shrink=inner_shrink)

        # 前景が無い時に 0 割りにならないよう、分母を最小 1 にクリップ
        denom = m.sum().clamp(min=1.0)

        # 前景部分だけの平均誤差（要素ごとの積 → 総和 → 前景画素数で割る）
        losses.append((e * m).sum() / denom)

    # 複数層の結果をさらに平均して、ひとつのスカラー損失として返す
    return sum(losses) / max(1, len(losses))

"""# 概要

このコードは「物体が写っている場所だけ」に注目して、**自動エンコード系（AE）の再構成誤差**を集計するためのヘルパーです。
ざっくり言うと：

* 学習用アノテーション（箱とラベル）を**安全な形**にそろえる
* その箱の位置から、**画像内の前景エリアだけのマスク**を作る
* AE の誤差マップをそのマスクで切り出し、**前景部分だけの平均誤差**を作る

こうすることで、背景のノイズに引っ張られず、「対象物の見た目を正しく再現できているか」を測れます。

# 目的

* **データの安全化**：ラベルのずれや範囲外の値を吸収して、学習が止まるのを防ぐ
* **前景の抽出**：物体のいる範囲をピンポイントで切り出す
* **前景だけでの誤差集計**：AE の良し悪しを、対象物に集中して評価する

# 入力となる変数

* `targets`：各画像のアノテーションをまとめたリスト。各要素は「箱の位置4つ＋ラベル1つ」を持つテンソル。
* `num_known`：既知クラス数（背景は含まない）
* `feat_hat_list` / `feat_ref_list`：AE の**再構成特徴**と**元の特徴**（複数層ぶんのリスト）
* `ssd_cfg`：入力サイズなどの設定（ここでは前景マスク作成に使う）
* `inner_shrink`：前景マスクを少し内側に縮める比率（背景の巻き込みを防ぐ目的）
* `kind`：誤差の種類（L1 または L2 を想定）

# 処理の流れ

## 1) `filter_known_targets(targets, num_known)`

* **やっていること**

  * 各バッチのアノテーションから、「既知クラスだけ」を残し、ラベル表記を **1..K**（K は既知クラス数）に統一します。
  * 0 始まり（0..K-1）で来ても、1 始まり（1..K）に直します。
  * 範囲外のラベルは除外して、空テンソルで埋めます（学習がコケないように）。

* **ポイント**

  * データの**ばらつく表記**（0 始まり／1 始まり）を吸収。
  * 不正ラベルを早めに取り除くことで、後段の計算を安定化。

---

## 2) `_roi_mask_from_targets(targets, feat_hw, input_size, device, inner_shrink=0.9)`

* **やっていること**

  * 特徴マップ（畳み込みの出力）のサイズに合わせて、**前景だけが 1 のマスク画像**を作ります。
  * アノテーション座標がピクセルで来た場合は、入力サイズで割って**0～1の範囲**に直してから、特徴マップの解像度にスケール。
  * 箱の範囲を `inner_shrink` で少し縮め、背景を巻き込みにくくします。

* **ポイント**

  * 画像 → 0～1 正規化 → 特徴マップ座標へ、という**座標合わせ**が肝心。
  * マスクは `[バッチ, 1, 高さ, 幅]` で作成。どのピクセルが前景かを示します。

---

## 3) `recon_loss_from_feats_fg(feat_hat_list, feat_ref_list, targets, ssd_cfg, kind="l1", inner_shrink=0.9)`

* **やっていること**

  * 各層の**再構成誤差マップ**（再構成 − 参照）を作り、チャネル方向の平均で**1枚の誤差マップ**にまとめます。
  * `_roi_mask_from_targets` で作った前景マスクをかけて、**前景領域だけの平均誤差**を計算。
  * 複数層ぶんの結果をさらに平均して、**ひとつのスカラー損失**として返します。

* **ポイント**

  * 背景の影響を避け、**対象物の誤差**に集中できます。
  * 前景がまったくないバッチでも、割り算が壊れないように**ゼロ割回避**を入れています。

# 出力

* `filter_known_targets`：クリーンアップ済みの `targets` リスト
* `_roi_mask_from_targets`：前景マスク（1 は前景、0 はそれ以外）
* `recon_loss_from_feats_fg`：前景だけを使った AE 誤差の平均（学習で使うスカラー）

# 典型的な落とし穴（上位3つ）と対策

1. **座標系のズレ（ピクセルか 0～1 かが混ざる）**

   * *症状*：マスクがずれて前景がうまく切り出せない、誤差が変。
   * *対策*：`_roi_mask_from_targets` 内の「1.5 より大きければピクセル座標」の判定で正規化済み。入力サイズの指定を必ず確認。

2. **ラベルの表記ゆれ（0 始まりと 1 始まり）**

   * *症状*：未知扱いされて箱ごと落ちる、もしくは学習で例外。
   * *対策*：`filter_known_targets` で 1..K にそろえる。学習前にこの関数を必ず通す。

3. **背景の巻き込み（箱の縁で背景が混ざる）**

   * *症状*：前景誤差が背景に引っ張られる。
   * *対策*：`inner_shrink` を少し小さめ（例：0.9）にして、箱の内側だけをマスク。縁を避けられます。

# 用語ミニ解説

* **AE（オートエンコーダ）**：入力をいったん圧縮し、再び元に近い形に復元するモデル。復元できない部分が「おかしさ」のヒントになる。
* **再構成誤差**：元の特徴と再構成した特徴の差。値が大きいほど「うまく再現できていない」。
* **前景マスク**：画像（または特徴マップ）上で、対象物の位置だけを 1 にしたもの。
* **特徴マップ**：畳み込み処理の出力。入力画像より小さな解像度で、見た目の情報が詰まった多チャネル画像。
* **チャネル平均**：特徴の各チャネル方向の値を平均すること。1 枚のマップにまとめるための一般的な手法。
* **正規化（0～1）**：値の範囲を 0 から 1 にそろえること。座標合わせや安定化に使う。
* **縮小係数（inner\_shrink）**：箱の内側を少し縮める調整。縁の混ざり物（背景）を拾いにくくするための工夫。

疑問：
特徴マップ（畳み込みの出力）のサイズに合わせて、前景だけが 1 のマスク画像を作る
→無条件で前景なら１となる場合、部位欠損が正常に認識されなくなるのでは？

# 学習モデル
"""

def train_model(net, dataloaders_dict, criterion, optimizer,
                num_epochs, start_epoch=0, device='cpu'):
    """
    学習・検証ループ。
    - 出力を (loc, conf, dbox_list) に正規化して MultiBoxLoss へ渡す。
    - priors 数は predictions に合わせて動的生成（学習を止めないための暫定手当）。
    - K==5 (4+1) は 2クラスロジット [bg, fg] に変換。
    - 重要: targets のラベルを num_known (=len(voc_classes)) に合わせて安全化。
    """
    # ↑ 高レベルの目的説明。検出（loc/conf）に加えて、必要に応じてAEの損失も後で合算する設計。

    import os, sys, time, hashlib, math
    import torch
    from torch import nn
    from tqdm import tqdm
    from torch.cuda.amp import autocast, GradScaler
    try:
        from utils.ssd_model import DBox
    except ModuleNotFoundError:
        from ssd_model import DBox  # utils 配下でない構成用のフォールバック
    # ↑ 依存ライブラリとユーティリティの読み込み。
    #   DBox はSSDの基準箱を生成するクラス。パス差異に対応するため try/except で2通りの場所から読み込む。

    # AMPの有効可否とスケーラ
    _device_obj = torch.device(device) if not isinstance(device, torch.device) else device
    amp_enabled = (_device_obj.type == 'cuda')
    scaler = GradScaler(enabled=amp_enabled)
    # ↑ 自動混合精度(AMP)をGPU時のみ有効化。勾配スケーラは小数計算の安定化と高速化に使う。

    # 念押しで device へ
    net = net.to(_device_obj)
    if isinstance(criterion, nn.Module):
        criterion = criterion.to(_device_obj)
    # ↑ モデルと損失関数を指定デバイスへ移動。ここで移し忘れがあると実行時にデバイス不一致でエラーになる。

    # モデル実デバイス（以後これを使う：device を上書きしない）
    dev = next(net.parameters()).device
    # ↑ モデルの実際のデバイスを取得（DataParallel 等でも確実に追従）。以後はこれを基準にテンソルを載せる。

    def _split_ssd_loss_output(out, like_tensor):
        """
        MultiBoxLoss の出力を (loss_l, loss_c) に正規化する。
        - out が (loss_l, loss_c) タプル/リスト → そのまま
        - out が 単一テンソル → (out, 0)
        - out が None → (0, 0)
        """
        import torch
        z = torch.zeros((), device=like_tensor.device, dtype=like_tensor.dtype)
        if out is None:
            return z, z
        if isinstance(out, (tuple, list)):
            if len(out) >= 2:
                return out[0], out[1]
            elif len(out) == 1:
                return out[0], z
        if torch.is_tensor(out):
            return out, z
        raise TypeError(f"Unexpected loss output type: {type(out)}")
    # ↑ 損失関数の戻り仕様の違いを吸収するヘルパ。学習ループ側の分岐を減らして見通しを良くする。

    # --- デバッグ時に行番号を正確化したい場合は、学習前に一度だけ有効化 ---
    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    # ↑ CUDAの非同期を止め、エラー行番が正確に出るようにするオプション（通常はコメントアウトのまま）。

    # 念押しで device へ
    net = net.to(device)
    if isinstance(criterion, nn.Module):
        criterion = criterion.to(device)
    # ↑ 念のため再度 device へ。上で設定済みだが、外部から device を変えた場合にも対応。

    num_known = len(voc_classes)                # 既知クラス数（背景を含まない）
    num_classes = int(ssd_cfg.get('num_classes', num_known + 1))  # 背景含む
    assert num_classes == (num_known + 1), \
        f"ssd_cfg['num_classes']は背景込み。len(voc_classes)+1={num_known+1} と一致させてください（現在 {num_classes}）。"
    # ↑ 設定の安全チェック。クラス数の不一致は学習途中で形状不整合を起こしやすいので開始前に落とす。

    # 1チャネル前景スコア→2クラスロジット [bg, fg]
    def _to_two_class_logits_from_single(s, *, is_prob=None):
        # is_prob が None の場合のみフォールバック判定（後方互換）
        if is_prob is None:
            with torch.no_grad():
                is_prob = bool(torch.all((s >= 0) & (s <= 1)))
        if is_prob:
            eps = 1e-6
            s = torch.clamp(s, eps, 1 - eps)
            logit = torch.log(s) - torch.log1p(-s)  # logit(p)
        else:
            logit = s
        zeros = torch.zeros_like(logit)
        return torch.cat([zeros, logit], dim=-1)  # [..., 2]
    # ↑ 出力が「前景1スコアのみ」のモデルに対し、背景と前景の2値表現（ロジット）へ変換する小道具。

    # モデル/学習cfgの DBox と出力 priors 数を厳密に合わせる（本数＋中身）
    def _ensure_priors_or_fail(P: int):
      """
      学習cfg/モデルのpriorsと本数だけでなく**中身の整合**も厳密チェックする。
      不一致なら即停止して原因特定を促す。
      """
      # cfg から再生成（基準）
      dbox_cfg = DBox(ssd_cfg).make_dbox_list().to(dev)
      if dbox_cfg.shape[0] != P:
          raise RuntimeError(
              f"[FATAL] priors count mismatch: cfg has {int(dbox_cfg.shape[0])} but predictions imply {P}. "
              "cfg（min_sizes/max_sizes/feature_maps/steps/aspect_ratios）とヘッド出力が不整合です。"
          )

      # モデルが保持していれば優先し、中身比較も行う
      dbox = getattr(net, "dbox_list", None)
      if torch.is_tensor(dbox) and dbox.dim() == 2 and dbox.shape[0] == P:
          # md5 比較（中身の完全一致を要求）
          def _md5_tensor(t: torch.Tensor) -> str:
              arr = t.detach().float().cpu().numpy().view(np.uint8)
              return hashlib.md5(arr.tobytes()).hexdigest()
          md5_model = _md5_tensor(dbox)
          md5_cfg   = _md5_tensor(dbox_cfg)
          if md5_model != md5_cfg:
              raise RuntimeError(
                  f"[FATAL] priors content mismatch (count matches but content differs): model={md5_model} cfg={md5_cfg}. "
                  "cfg と学習時の DBox の“並び/サイズ”が異なります。feature_maps の順序やヘッドの並びを確認してください。"
              )
          return dbox.to(dev)

      return dbox_cfg
     # ↑ 予測の本数と基準箱の本数を必ず一致させる。ズレるとロス計算が成り立たないため早期に止める。


    # predictions を (loc, conf, dbox_list) に正規化
    def _normalize_ssd_predictions(raw):

        k_expect = 4 + num_classes
        # ↑ 1予測あたり [位置4 + クラス数] の合計次元を算出。後の形状推定に使う。

        # dict
        if isinstance(raw, dict):
            for k3 in [('loc','conf','dbox_list'), ('loc','conf','priors'), ('loc','conf','anchors')]:
                if all(k in raw for k in k3):
                    loc, conf = raw[k3[0]], raw[k3[1]]
                    P = loc.shape[1]
                    # --- conf 厳密化（1ch→2クラス変換も含む） ---
                    if torch.is_tensor(conf) and conf.dim() == 3:
                        if conf.size(-1) == 1 and num_classes == 2:
                            N = loc.size(0)
                            s = conf.contiguous().view(N, P, 1)
                            is_prob_fg1 = bool(ssd_cfg.get('single_fg_is_prob', True))
                            conf = _to_two_class_logits_from_single(s, is_prob=is_prob_fg1)
                        elif conf.size(-1) != num_classes:
                            raise TypeError(
                                f"dict conf shape mismatch: lastdim={conf.size(-1)} expected={num_classes} "
                                "(use single_fg_is_prob for 1ch-foreground models)"
                            )
                    # --- priors（本数＋中身）を厳密化。外部同梱は検証のみ・採用しない ---
                    dbox_expected = _ensure_priors_or_fail(P)
                    dbox_supplied = raw[k3[2]]
                    if torch.is_tensor(dbox_supplied):
                        import numpy as np, hashlib
                        def _md5_tensor_canon(t: torch.Tensor) -> str:
                            a = t.detach().to(dtype=torch.float32, memory_format=torch.contiguous_format).cpu().numpy()
                            a = np.ascontiguousarray(a)
                            return hashlib.md5(a.tobytes()).hexdigest()
                        same_shape = (tuple(dbox_supplied.shape) == tuple(dbox_expected.shape))
                        same_md5   = (_md5_tensor_canon(dbox_supplied) == _md5_tensor_canon(dbox_expected))
                        if (not same_shape) or (not same_md5):
                            raise RuntimeError(
                                "[FATAL] priors(content) mismatch in dict output: "
                                f"supplied.shape={tuple(dbox_supplied.shape)} expected.shape={tuple(dbox_expected.shape)} "
                                f"supplied.md5={_md5_tensor_canon(dbox_supplied)} expected.md5={_md5_tensor_canon(dbox_expected)}. "
                                "cfg（input_size/feature_maps/steps/min_sizes/max_sizes/aspect_ratios）やヘッド順序の不整合を確認してください。"
                            )
                    return loc, conf, dbox_expected
        # ↑ 出力が辞書の場合に想定キー名の組を順に試し、見つかれば取り出す。
        #   基準箱の本数がズレていれば補助関数で作り直す（または後で厳密一致チェック）。

        # list/tuple
        if isinstance(raw, (list, tuple)):
            if len(raw) == 3 and not (isinstance(raw[0], (list, tuple)) and len(raw[0]) == 3):
                loc, conf, dbox_sup = raw[0], raw[1], raw[2]
            else:
                first = raw[0]
                if isinstance(first, (list, tuple)) and len(first) == 3:
                    loc, conf, dbox_sup = first[0], first[1], first[2]
                else:
                    flat = []
                    def _flatten(x):
                        if isinstance(x, (list, tuple)):
                            for y in x:
                                _flatten(y)
                        else:
                            flat.append(x)
                    _flatten(raw)
                    if len(flat) < 3:
                        raise ValueError(f"SSD outputs から loc/conf/dbox を特定できません（flat_len={len(flat)}）")
                    loc, conf, dbox_sup = first[0], first[1], first[2]
            P = loc.shape[1]
            dbox = _ensure_priors_or_fail(P)
            return loc, conf, dbox

        # ↑ 出力がタプル/リストの場合、入れ子の形でもloc/conf/dboxに再構成して返す。
            dbox_expected = _ensure_priors_or_fail(P)
            # 外部同梱 dbox は採用しない。中身のみ厳密照合してズレを即検知。
            if torch.is_tensor(dbox_sup):
                import numpy as np, hashlib
                def _md5_tensor_canon(t: torch.Tensor) -> str:
                    a = t.detach().to(dtype=torch.float32, memory_format=torch.contiguous_format).cpu().numpy()
                    a = np.ascontiguousarray(a)
                    return hashlib.md5(a.tobytes()).hexdigest()
                same_shape = (tuple(dbox_sup.shape) == tuple(dbox_expected.shape))
                same_md5   = (_md5_tensor_canon(dbox_sup) == _md5_tensor_canon(dbox_expected))
                if (not same_shape) or (not same_md5):
                    raise RuntimeError(
                        "[FATAL] priors(content) mismatch in list/tuple output: "
                        f"supplied.shape={tuple(dbox_sup.shape)} expected.shape={tuple(dbox_expected.shape)} "
                        f"supplied.md5={_md5_tensor_canon(dbox_sup)} expected.md5={_md5_tensor_canon(dbox_expected)}. "
                        "cfg やヘッド順序の不整合を確認してください。"
                    )
            return loc, conf, dbox_expected

        # tensor
        if torch.is_tensor(raw):
            x = raw

            # [N, H, P, K]
            if x.dim() == 4:
                N, H, P, K = x.shape
                loc = x[..., :4].contiguous().view(N, H * P, 4)
                tail = x[..., 4:]
                if K - 4 == num_classes:
                    conf = tail.contiguous().view(N, H * P, num_classes)
                elif K - 4 == 1 and num_classes == 2:
                    s = tail.contiguous().view(N, H * P, 1)
                    # 明示フラグで確率/ロジットを指定（既定: True=確率）
                    is_prob_fg1 = bool(ssd_cfg.get('single_fg_is_prob', True))
                    conf = _to_two_class_logits_from_single(s, is_prob=is_prob_fg1)
                else:
                    raise TypeError(f"4次元テンソルのクラス部が不整合: K-4={K-4}, 期待: 1 or {num_classes}")
                dbox = _ensure_priors_or_fail(H * P)
                return loc, conf, dbox

            # [N, P, 4+num_classes] / 4+1
            if x.dim() == 3 and x.size(-1) >= 4:
                N, P, C = x.shape
                if C == k_expect:
                    loc  = x[..., :4].contiguous().view(N, P, 4)
                    conf = x[..., 4:4+num_classes].contiguous().view(N, P, num_classes)
                elif C == 5 and num_classes == 2:
                    # 前景1クラス用ネット専用（bg+fg）。あなたの設定（bg+2）では許容しない。
                    loc  = x[..., :4].contiguous().view(N, P, 4)
                    s    = x[..., 4:].contiguous().view(N, P, 1)
                    is_prob_fg1 = bool(ssd_cfg.get('single_fg_is_prob', True))
                    conf = _to_two_class_logits_from_single(s, is_prob=is_prob_fg1)
                else:
                    raise TypeError(
                        f"3次元テンソル末次元が不整合: C={C}, 期待: {k_expect}。"
                        " 4+1 形式は『前景1クラス（bg+fg）』学習専用です。"
                    )
                dbox = _ensure_priors_or_fail(P)
                return loc, conf, dbox

            # [N, (4+num_classes)*P]
            if x.dim() == 2 and x.size(1) % k_expect == 0:
                N, C = x.shape
                P = C // k_expect
                x = x.view(N, P, k_expect).contiguous()
                loc  = x[..., :4].contiguous().view(N, P, 4)
                conf = x[..., 4:4+num_classes].contiguous().view(N, P, num_classes)
                dbox = _ensure_priors_or_fail(P)
                return loc, conf, dbox

            raise TypeError(f"単一 Tensor 出力の形状が想定外です: shape={tuple(x.shape)}")
        # ↑ 出力がテンソルのときは形状を手掛かりにloc/confへ切り出す。
        #   3パターン（4D, 3D, 2D）を想定し、それ以外は想定外としてエラーにする。

        # それ以外
        raise TypeError(f"SSD outputs の型が想定外です: {type(raw)}")
    # ↑ どんな形式のモデル出力でも、学習コードが受け取れる共通形式に統一する関数。

    # === 追加: SSD用に targets を安全化（範囲外ラベルは除去） ===
    def _sanitize_targets_for_ssd(targets, num_known, input_size=None):
      """
      targets: list[Tensor] 各Tensorは [M,5] = (x0,y0,x1,y1,label)
      - ラベルは 0..num_known-1 でも 1..num_known でも受け付け、最終的に 1..num_known に正規化する
      - 座標は [0,1] に正規化/クランプし、x1>x0, y1>y0 を満たさない箱は除外
      戻り値: (targets_sanitized, dropped_total)
      """
      new, dropped = [], 0
      for t in targets:
          if t is None or (torch.is_tensor(t) and t.numel() == 0):
              # 正例なし（負例画像）
              device = t.device if torch.is_tensor(t) else torch.device("cpu")
              new.append(torch.zeros((0, 5), device=device, dtype=torch.float32))
              continue

          assert torch.is_tensor(t) and t.shape[1] >= 5, "targetは [N,5] (xyxy+label) を想定"
          device = t.device
          t = t.clone()

          # --- 座標の正規化 ---
          # もし絶対座標（>1）が入っていれば input_size で割って [0,1] にする
          if input_size is not None and (t[:, :4].max() > 1.5):
              s = float(input_size)
              t[:, :4] = t[:, :4] / s

          # [0,1] にクランプ
          t[:, 0] = t[:, 0].clamp(0.0, 1.0)
          t[:, 1] = t[:, 1].clamp(0.0, 1.0)
          t[:, 2] = t[:, 2].clamp(0.0, 1.0)
          t[:, 3] = t[:, 3].clamp(0.0, 1.0)

          # 幾何異常の除外（x1>x0 かつ y1>y0）
          w = t[:, 2] - t[:, 0]
          h = t[:, 3] - t[:, 1]
          keep_geo = (w > 1e-6) & (h > 1e-6)

          # --- ラベルを 1..num_known に正規化 ---
          lbl = t[:, 4].long()
          # すでに 1..num_known ならそのまま
          if (lbl.min() >= 1) and (lbl.max() <= num_known):
              lbl_mapped = lbl
          # 0..num_known-1 なら +1 シフト
          elif (lbl.min() >= 0) and (lbl.max() <= num_known - 1):
              lbl_mapped = lbl + 1
          else:
              # それ以外は不正 → 除外対象にマーク
              lbl_mapped = lbl.clone()
              bad = (lbl_mapped < 1) | (lbl_mapped > num_known)
              lbl_mapped[bad] = -1

          keep_lbl = (lbl_mapped >= 1) & (lbl_mapped <= num_known)
          keep = keep_geo & keep_lbl
          dropped += int((~keep).sum().item())

          if keep.any():
              t = t[keep]
              t[:, 4] = lbl_mapped[keep].to(t.dtype)
              new.append(t)
          else:
              new.append(t.new_zeros((0, t.size(1))))
      return new, dropped
    # ↑ 学習前にターゲット注釈をきれいに整える。座標の範囲外・幅や高さがゼロに近い箱・範囲外ラベルを除去。

    # === 追加: その場で検知するための一時バリデータ ===
    def _assert_batch_safe(loc, conf, dbox, targets, epoch, phase, bidx):
        # priors 数一致
        assert loc.shape[1] == conf.shape[1] == dbox.shape[0], \
            f"Priors不一致: locP={loc.shape[1]} confP={conf.shape[1]} dboxP={dbox.shape[0]}"
        # クラス次元一致
        assert conf.shape[2] == num_classes, f"confのクラス次元 {conf.shape[2]} が num_classes {num_classes} と不一致"
        # ラベル範囲確認
        for i, t in enumerate(targets):
            if t is None or (torch.is_tensor(t) and t.numel()==0):
                continue
            lbl = t[:,4].long()
            if (lbl < 1).any() or (lbl > num_known).any():
                bad = lbl[(lbl < 1) | (lbl > num_known)]
                raise ValueError(f"[{phase} ep{epoch} b{bidx}] 範囲外ラベル検出: {bad.tolist()} / 許容: 1..{num_known}")
    # ↑ バッチ単位の安全チェック。形状不一致や不正ラベルを早期に検知して原因調査を容易にする。

    best_val_loss = float('inf')
    # ↑ これまでの最良検証損失を記録し、モデル保存の更新判定に使う。

    for epoch in range(start_epoch, start_epoch + num_epochs):
        t0 = time.time()

        phase_loss_sum = {'train': 0.0, 'val': 0.0}
        phase_batches  = {'train': 0,    'val': 0}
        # --- 損失の内訳（loc / conf / AE*λ）の観測用集計 ---
        breakdown_sum = {
            'train': {'l': 0.0, 'c': 0.0, 'ae': 0.0},
            'val'  : {'l': 0.0, 'c': 0.0, 'ae': 0.0},
        }
        breakdown_mb = {'train': 0, 'val': 0}  # マイクロバッチ数
        # AE ロスの観測用（足されているかを可視化）
        ae_loss_sum    = {'train': 0.0, 'val': 0.0}
        ae_mb_counts   = {'train': 0,    'val': 0}
        # ↑ 各フェーズの合計損失とバッチ数を記録し、後で平均を出す。

        for phase in ['train', 'val']:
            if phase not in dataloaders_dict or dataloaders_dict[phase] is None:
                continue
            # ↑ データローダが無いフェーズはスキップできる柔軟な設計。

            net.train(mode=(phase == 'train'))
            # ↑ train フェーズでは学習モード、val では推論モードに切り替える（Dropout/BNの挙動が変わる）。

            with tqdm(dataloaders_dict[phase], desc=f"{phase} {epoch}", file=sys.stdout) as iterator:
                for bidx, (images, targets) in enumerate(iterator):
                    # 入力を device へ
                    images = images.to(device, non_blocking=True)
                    # ↑ 画像テンソルをデバイスに転送。non_blocking=Trueは転送の非同期化で若干の高速化狙い。

                    # --- SSD用に targets を安全化 ---
                    if isinstance(targets, (list, tuple)):
                        targets = list(targets)
                    elif torch.is_tensor(targets):
                        targets = [targets]
                    elif isinstance(targets, dict):
                        targets = [targets['boxes_labels']] if 'boxes_labels' in targets else []
                    targets = [t.to(device) if torch.is_tensor(t) else t for t in targets]
                    targets_ssd, dropped = _sanitize_targets_for_ssd(
    targets, num_known=num_known, input_size=int(ssd_cfg.get('input_size', 300))
)
                    if dropped > 0 and phase == 'train' and bidx == 0:
                        print(f"[warn] 範囲外ラベルを {dropped} 個除外（ep{epoch} {phase}）")
                    # ↑ ターゲットの型を揃え、デバイスに載せ、バリデーション済みに変換。
                    #   最初のバッチで除外発生を知らせ、ラベル不備の気づきを早める。

                    # 勾配初期化（蓄積を使うので最初に1回）
                    optimizer.zero_grad(set_to_none=True)
                    # ↑ 勾配をリセット。set_to_none=True はメモリ節約と速度向上のための設定。

                    B = images.size(0)
                    mb = min(MICRO_BS, B)
                    accum_steps = (B + mb - 1) // mb  # 端数切り上げ（= マイクロバッチ数）
                    # ↑ バッチを複数の小さなかたまりに分けて順次 backward し、最後に一度だけ更新する「勾配蓄積」を行う。

                    # targets をバッチ方向に安全にスライス
                    def _slice_targets(tlist, s, e):
                        return [tlist[i] for i in range(s, e)]  # list[Tensor] 前提
                    # ↑ マイクロバッチに合わせてターゲットも同じ範囲で取り出すヘルパ。

                    # マイクロバッチで分割して順次 forward/backward
                    for s in range(0, B, mb):
                        e = min(B, s + mb)
                        images_mb = images[s:e]
                        targets_mb = _slice_targets(targets_ssd, s, e)

                        with torch.set_grad_enabled(phase == 'train'):
                            with autocast(enabled=amp_enabled):  # ★ AMP
                                # モデルと同じデバイスへ（device を上書きしない）
                                images_mb = images_mb.to(dev, non_blocking=True)

                                raw_outputs = net(images_mb)
                                loc, conf, dbox = _normalize_ssd_predictions(raw_outputs)
                                # ↑ ここまでで予測を共通形式に統一。以後の損失関数は常に同じインターフェースで呼べる。

                                # SSD 損失
                                _loss_raw = criterion((loc, conf, dbox), targets_mb)
                                loss_l, loss_c = _split_ssd_loss_output(_loss_raw, like_tensor=loc)
                                loss = loss_l + loss_c
                                # ↑ 検出の回帰損失＋分類損失を合算。MultiBoxLoss の返り形が何でも扱えるよう統一。

                                # AE 損失（あれば）
                                rec = getattr(net, 'last_rec', None)
                                if rec is not None:
                                    known_targets = targets_mb  # sanitize済み
                                    loss_rec = recon_loss_from_feats_fg(
                                        rec['feat_hat'], rec['feat_ref'],
                                        targets=known_targets, ssd_cfg=ssd_cfg, kind="l1",
                                    )
                                    loss = loss + lambda_rec * loss_rec
                                    # --- 監視用：AE が本当に足されているかをカウント＆集計 ---
                                    ae_loss_sum[phase]  += float(loss_rec.detach().item())
                                    ae_mb_counts[phase] += 1
                                else:
                                    # AEが無い場合は 0 として扱う（ログ整形用）
                                    loss_rec = None
                                    # VALでAEが落ちている場合の気づき用ワンタイム警告
                                    if phase == 'val' and bidx == 0 and s == 0:
                                        print("[val] notice: net.last_rec is None -> AE loss not added on this batch")
                                # ↑ AE（自己再現）ブランチが用意されている場合、前景領域だけの再現誤差を加点。
                                #   重み lambda_rec で影響度を調整できる（大きすぎると検出性能に悪影響もありうる）。

                            if phase == 'train':
                                # 勾配蓄積：マイクロバッチ数で割ってから backward
                                scaler.scale(loss / accum_steps).backward()
                        # ↑ AMPスケーラで損失を拡大しつつ逆伝播。accum_stepsで割るのは更新量を一定に保つため。

                        # AEの一時参照を解除（滞留防止・任意）
                        if hasattr(net, 'last_rec'):
                            net.last_rec = None
                        # ↑ 参照を切って不要なメモリ滞留を避ける（GCを促す）。

                    # まとめて更新（train のときだけ）
                    if phase == 'train':
                        # AMP流儀：unscale → clip → step
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5.0)
                        scaler.step(optimizer)
                        scaler.update()
                    # ↑ 全マイクロバッチの逆伝播が終わってから一回だけ重み更新。
                    #   unscale_で実勾配に戻し、爆発回避のため勾配クリップ後、step→update。

                    breakdown_sum[phase]['l'] += float(loss_l.detach().item())
                    breakdown_sum[phase]['c'] += float(loss_c.detach().item())
                    if loss_rec is not None:
                      breakdown_sum[phase]['ae'] += float((lambda_rec * loss_rec).detach().item())
                    breakdown_mb[phase] += 1

                    phase_loss_sum[phase] += float(loss.detach().item())
                    phase_batches[phase]  += 1
                    # ↑ ログ用に損失とバッチ数を加算。

        # 平均損失で記録
        train_loss = phase_loss_sum['train'] / max(1, phase_batches['train'])
        val_loss   = phase_loss_sum['val']   / max(1, phase_batches['val'])
        t1 = time.time()

        print(f'epoch {epoch}/{start_epoch + num_epochs - 1} '
              f'{t1 - t0:.1f}sec || train_Loss:{train_loss:.4f} val_Loss:{val_loss:.4f}')
        # ↑ エポックごとの要約ログ。時間と学習/検証損失の平均を表示。
        # --- 内訳（マイクロバッチ平均）を表示 ---
        def _avg(v, n):
            return (v / max(1, n))
        trL = _avg(breakdown_sum['train']['l'], breakdown_mb['train'])
        trC = _avg(breakdown_sum['train']['c'], breakdown_mb['train'])
        trA = _avg(breakdown_sum['train']['ae'], breakdown_mb['train'])
        vaL = _avg(breakdown_sum['val']['l'],   breakdown_mb['val'])
        vaC = _avg(breakdown_sum['val']['c'],   breakdown_mb['val'])
        vaA = _avg(breakdown_sum['val']['ae'],  breakdown_mb['val'])
        print(f"[breakdown] train L:{trL:.4f} C:{trC:.4f} AE:{trA:.4f} | "
              f"val L:{vaL:.4f} C:{vaC:.4f} AE:{vaA:.4f}")
        # AE がどの程度計算されたか（ミニバッチ単位の平均）
        if (ae_mb_counts['train'] + ae_mb_counts['val']) > 0:
            avg_tr = ae_loss_sum['train'] / max(1, ae_mb_counts['train'])
            avg_va = ae_loss_sum['val']   / max(1, ae_mb_counts['val'])
            print(f"[ae] lambda_rec={lambda_rec}  "
                  f"train_ae_mb={ae_mb_counts['train']} avg={avg_tr:.4f} | "
                  f"val_ae_mb={ae_mb_counts['val']} avg={avg_va:.4f}")
        # --- checkpoint 保存 ---
        def _md5_of_first_param(m):
            t = next(m.parameters()).detach().float().cpu().numpy().tobytes()
            return hashlib.md5(t).hexdigest()
        # ↑ 最初のパラメータのMD5を取り、重みが確かに変わっているかの目安を残す小道具。

        ckpt_last = {
            'model': net.state_dict(),
            'optimizer': optimizer.state_dict(),
            'epoch': epoch,
            'best_val_loss': best_val_loss,
            'cfg': ssd_cfg,
            'classes': voc_classes,
            'lambda_rec': lambda_rec,
            'weight_md5': _md5_of_first_param(net),
        }
        # --- dbox を保存（学習時 cfg からの再生成 or net 保持を優先） ---
        try:
            _dbox = getattr(net, "dbox_list", None)
            if not (torch.is_tensor(_dbox) and _dbox.dim() == 2):
                try:
                    _dbox = DBox(ssd_cfg).make_dbox_list()
                except Exception:
                    _dbox = None
            if _dbox is not None:
                _dbox_cpu = _dbox.detach().cpu()
                _dbox_md5 = hashlib.md5(_dbox_cpu.numpy().view(np.uint8).tobytes()).hexdigest()
                ckpt_last['dbox'] = _dbox_cpu
                ckpt_last['dbox_md5'] = _dbox_md5
        except Exception as e:
            print("[warn] failed to attach dbox to checkpoint:", e)
        torch.save(ckpt_last, os.path.join(WEIGHTS_DIR, 'last.pth'))
        print(f"[save] last.pth  md5={ckpt_last['weight_md5']}")
        # ↑ 直近チェックポイントを毎エポック保存。途中で中断してもここから再開できる。

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            ckpt_best = ckpt_last.copy()
            ckpt_best['best_val_loss'] = best_val_loss
            torch.save(ckpt_best, os.path.join(WEIGHTS_DIR, 'best.pth'))
            print(f"[save] best.pth updated: val_loss={best_val_loss:.4f}  md5={ckpt_best['weight_md5']}")
        # ↑ 検証損失が最良を更新したときだけ best.pth を上書き。後で一番良かったモデルを取り出せる。

    return net
    # ↑ 学習済みモデルを返す。外側で推論や評価にすぐ使える形。

"""# 概要（直感的に）

この関数は、**画像を少しずつ読み込み→モデルに通す→間違い具合を数値化→重みを更新する**という作業を、**学習用**と**確認用**の2モードでくり返す「学習ループ」です。途中で**出力の形**や**ラベルの範囲**が正しいかを厳しく点検し、もしおかしければ早めに止めて原因を見つけやすくします。終わったら**重みを保存**します。

---

# 目的

* モデルの出力を**統一フォーマット**に整え、損失関数に渡せるようにする
* ターゲット（正解ラベル）を**安全な表記に正規化**して、学習を安定化
* 大きなバッチを小分け（**マイクロバッチ**）にして、**省メモリ**で学習
* \*\*自動混合精度（AMP）\*\*で高速化＆省メモリ
* エポックごとに**損失を表示**し、**チェックポイントを保存**（last/best）

---

# 入力となる引数

* **net**：学習するモデル（SSDベース）
* **dataloaders\_dict**：`{"train": train_dataloader, "val": val_dataloader}` の辞書
  学習用と検証用のデータを取り出す仕組み
* **criterion**：損失関数（MultiBoxLoss）
* **optimizer**：最適化アルゴリズム（SGDなど）
* **num\_epochs**：何周まわすか（エポック数）
* **start\_epoch**：再開時の開始エポック（0からでもOK）
* **device**：学習に使う計算機（"cuda" など）

※ 関数内では、`ssd_cfg`（設定）、`voc_classes`（クラス名リスト）、`MICRO_BS`（マイクロバッチサイズ）、`lambda_rec`（AE損失の重み）、`WEIGHTS_DIR`（保存先）などの**外部変数**も利用します。

---

# 受け渡しているデータとその内容

* **dataloader からの出力**

  * `images`：ミニバッチ画像（テンソル）
  * `targets`：各画像の正解ボックスとラベル（画像ごとのリスト）

* **モデルの出力（生の出力）**
  バリエーションがあるため、`_normalize_ssd_predictions`で
  **(loc, conf, dbox\_list)** の形に**正規化**します。

  * `loc`：各候補枠の位置予測
  * `conf`：各候補枠のクラススコア
  * `dbox_list`：事前定義の候補枠（数が出力と一致しているかも確認）

* **損失**

  * `loss_l`（位置のずれ）と `loss_c`（クラスの間違い）の合計
  * 必要なら `lambda_rec * AE前景再構成損失` を足す

---

# 処理の流れ

1. **環境セットアップ**

   * AMP（自動混合精度）の有効化、GradScalerの用意
   * モデルと損失関数を`device`へ移動
   * クラス数の整合チェック（`num_classes == len(voc_classes)+1`）

2. **サブルーチンの定義**

   * `_to_two_class_logits_from_single`：1チャネル前景スコアを2クラス表現に変換（bg/fg）
   * `_ensure_priors_or_fail`：候補枠（priors）の本数チェックと再生成（不整合なら即中断）
   * `_normalize_ssd_predictions`：モデル出力を\*\*(loc, conf, dbox\_list)\*\* へ統一
   * `_sanitize_targets_for_ssd`：**ターゲットを安全化**（ラベル表記統一・座標のクランプ・異常枠除外）
   * `_assert_batch_safe`：**その場チェック**（priors本数・クラス次元・ラベル範囲）

3. **エポックループ**

   * `train` と `val` をこの順で処理
   * `net.train(mode=(phase=='train'))` で学習/推論モードを切替

4. **バッチ処理**

   * `images` を `device` へ
   * `targets` を `_sanitize_targets_for_ssd` で**安全化**
   * 勾配をクリア（`optimizer.zero_grad`）
   * **マイクロバッチ分割**（`MICRO_BS`）で順次 forward/backward

     * `raw_outputs = net(images_mb)`
     * `(loc, conf, dbox)` に正規化
     * **損失を計算**：`criterion((loc, conf, dbox), targets_mb)`
     * **AE前景損失**があれば加算
     * 学習中（train）なら `scaler.scale(loss/accum_steps).backward()` で勾配をためる
   * 1バッチ分が終わったら、AMPの手順で
     `unscale → clip_grad_norm → step → scaler.update()`
   * フェーズごとの損失を加算

5. **ログと保存**

   * エポック平均の train/val 損失を表示
   * **last.pth** と、val損失が改善したら **best.pth** を保存

---

# 出力

* **戻り値**：学習後の `net`（重みが更新済み）
* **副作用**：`WEIGHTS_DIR` に `last.pth` / `best.pth` を保存、エポックごとのログ出力

---

# 典型的な落とし穴（上位3つ）と対策

1. **クラス数の不一致**

   * *症状*：損失計算で形が合わない、学習が進まない
   * *対策*：`ssd_cfg['num_classes'] == len(voc_classes)+1` を必ず満たす（関数内でも assert でチェック）

2. **priors（候補枠）の本数不一致**

   * *症状*：出力と候補枠の数が食い違い、後段でエラーや不安定化
   * *対策*：`_ensure_priors_or_fail` で**即エラーにして原因を特定**。
     `min_sizes / max_sizes / feature_maps / steps` の設定とヘッド出力を一致させる。

3. **ターゲット（正解）側の不正（座標やラベル）**

   * *症状*：学習が止まる・損失がNaN・精度が上がらない
   * *対策*：`_sanitize_targets_for_ssd` で**事前に安全化**（ラベル表記統一、座標クランプ、幅や高さがゼロの箱を除去）。
     初回バッチで除外件数を警告表示して気づけるようにしている。

---

# 用語ミニ解説

* **エポック**：データセットを一周学習すること。
* **ミニバッチ / マイクロバッチ**：一度に処理する画像の束。マイクロバッチはさらに小さく分けて省メモリで学習する工夫。
* **AMP（自動混合精度）**：自動で精度を切り替え、計算を速く・軽くする仕組み。`autocast` と `GradScaler` を使う。
* **priors（候補枠）**：事前に用意された基準の四角形群。ここに対して位置とクラスを予測する。
* **loc / conf / dbox\_list**：それぞれ「位置予測」「クラススコア」「候補枠の座標リスト」。
* **checkpoint（チェックポイント）**：学習途中の重みや状態を保存したファイル。`last.pth` と `best.pth` を保存している。
* **クリップ（勾配クリップ）**：勾配が暴れすぎるのを防ぐガード。

---

必要なら、この説明に沿って「どの部分をログで詳しく出すか」「どの例外で止めるか」をさらに調整できます。

追加解説：
AMP・GradScalarは学習の効率化のためのコード

# 学習
"""

# --- 追加: デバイス設定とモデル配置・確認 ----------------------------------------
# ここは「学習を開始する直前の最終準備」をまとめたセルです。
# ・どの計算機（GPU/CPU）で動かすかを決める
# ・モデルをその計算機に移す
# ・損失関数（採点方法）を用意する
# ・設定の整合性（クラス数など）をチェックする
# ・準備ができたら train_model(...) を呼び出す
# ---------------------------------------------------------------------------

# === 損失関数の定義（学習セルの直前に置く） ===
# from utils.ssd_model import MultiBoxLoss
# import torch
import torch
# ↑ 上の2行は参考用のコメント。実際の import は下で行っています。
#    MultiBoxLoss は SSD の訓練で使う損失関数（位置と分類の採点）です。

from torch.cuda.amp import autocast, GradScaler
# ↑ AMP（自動混合精度）機能の補助ツール。
#   - autocast: forward/backward 中の一部演算を自動的に半精度化して高速・省メモリ化
#   - GradScaler: 半精度学習時に勾配のスケールを調整して安定化する

MICRO_BS = 8  # 1マイクロバッチのサイズ（8〜4にすると更に軽くなる）
# ↑ 1回の“実際の更新”の中で、バッチを小分けにして順番に積み上げる単位。
#   大きなバッチを一気に載せられないGPUでも、分割して勾配を溜めれば同等の効果を狙えます。

lambda_rec = 0.3  # AE再構成損失の重み。無効化したいときは 0.0
# ↑ 異常検知（オートエンコーダ）の“自己再現のズレ”をどれだけ全体損失に混ぜるか。
#   0.12 だと“メインの検出タスク＋少しだけAEの学習”というバランスになります。

# 背景込みのクラス数を必ず一致させる（背景=0, 物体=1..K）
assert 'num_classes' in ssd_cfg
# ↑ ssd_cfg に 'num_classes' が必ず入っていることを確認。
assert ssd_cfg['num_classes'] == len(voc_classes) + 1, \
    f"ssd_cfg['num_classes']は背景込み。len(voc_classes)+1={len(voc_classes)+1} に合わせてください。"
# ↑ “背景を含む総クラス数” と “実際の対象クラス数+背景” が一致していないと、出力の形がズレて学習が壊れます。
#   この assert で早めに落として原因をはっきりさせます。

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ↑ 利用可能なら GPU（cuda）を、そうでなければ CPU を選びます。
#   Colab などでは「ランタイムの種類」で GPU を有効にすると cuda が使えます。

# モデルを必ずdeviceへ
net = net.to(device)
# ↑ ネットワーク本体を選んだデバイス（GPU/CPU）へ移します。
#   これを忘れると「デバイスが混在しています」というエラーになります。

# device は既に作ってある想定（例: torch.device('cuda')）
# Hard Negative Mining の比率はあなたの設定のまま
criterion = MultiBoxLoss(
    jaccard_thresh=0.5,  # IoU閾値：予測と正解の重なり度合いがこの値以上なら“当たり”寄りとして扱う基準
    neg_pos=4,           # HNMの負:正 比：難しい背景を多めに学習して分類を安定させるための比率
    device=device        # 損失計算もモデルと同じデバイスで実行
)
# （実装によっては num_classes 引数が必要な版もあります）
# criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=4,
#                          num_classes=len(voc_classes)+1, device=device)
# ↑ あなたの MultiBoxLoss 実装によっては num_classes を明示する必要があります。
#   その場合はコメントアウトを外して、こちらを使ってください。

print("criterion ready on", device)
# ↑ ステータス表示：損失関数の準備ができたことと利用デバイスを表示。

# DataParallelを使っている場合の確認（使っていなくてもOK）
base = net.module if isinstance(net, torch.nn.DataParallel) else net
# ↑ 複数GPUの DataParallel を使っていると、実体は net.module の下にぶら下がります。
#   単GPU/CPU のときはそのまま net を使います。
print("model on:", next(base.parameters()).device)
# ↑ 本当にモデルのパラメータがどのデバイスに載っているかを表示して最終確認します。

# 学習・検証を実行する
num_epochs = 200
# ↑ 何エポック（データを何周）学習するか。200 は目安。早期終了を併用すると実際には早く止まることも。
train_model(net, dataloaders_dict, criterion, optimizer,
            num_epochs=num_epochs, start_epoch=start_epoch, device=device)
# ↑ ここが“学習を開始するスイッチ”です。
#   - net: モデル本体
#   - dataloaders_dict: {"train": ..., "val": ...} のイテレータ群
#   - criterion: SSD の損失関数（位置＋分類）
#   - optimizer: パラメータ更新手法（例: SGD）
#   - num_epochs: 総エポック数
#   - start_epoch: 再開学習なら途中からスタート
#   - device: どの計算機で実行するか
# train_model 内では、各エポックで学習フェーズ・検証フェーズを回し、
# 損失の表示、保存（last/best）など一連の流れを管理します。

"""# 概要

このコードは「学習を始めるための最終チェックとスイッチ押し」です。
使う機械（GPU/CPU）を決め、モデルをその機械に載せ、損失関数（採点方法）を用意し、設定の矛盾がないかを確認してから、学習ループ `train_model(...)` を実行します。

# 目的

* 使うデバイス（GPU/CPU）の自動選択と反映
* 検出用の損失関数（MultiBoxLoss）の準備
* 学習に関わる細かなハイパラ（マイクロバッチ、AEの重み）の設定
* クラス数など致命的な設定ミスの早期検出
* 最終的に `train_model` を呼び出して、学習をスタート

# 入力となる変数

* `ssd_cfg`：SSDの各種設定（`num_classes` を必ず含む想定）
* `voc_classes`：学習対象クラスの一覧（背景を含まない）
* `net`：学習させたいモデル（SSDベース）
* `optimizer`：最適化手法（SGDなど）
* `dataloaders_dict`：`{"train": ..., "val": ...}` のデータローダ辞書
* `start_epoch`：再開学習時の開始エポック番号（なければ 0 ）
* `MultiBoxLoss`：検出用の損失関数クラス

# 処理の流れ

1. **（コメントアウトの残骸を含む）インポート**
   `torch` と AMP（自動混合精度）関連を使えるようにしています。
2. **マイクロバッチとAEの重みを設定**

   * `MICRO_BS = 8`：大きなバッチを小分けにして学習する単位
   * `lambda_rec = 0.12`：AE（自己再構成）の損失を全体にどれくらい混ぜるか
3. **クラス数チェック**

   ```python
   assert ssd_cfg['num_classes'] == len(voc_classes) + 1
   ```

   背景を含めたクラス総数が正しいかを確認。ここがズレると後で形が合わずに壊れます。
4. **デバイス決定とモデル移動**

   ```python
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   net = net.to(device)
   ```

   使えるならGPU、なければCPUを使う。モデル本体をそのデバイスに載せます。
5. **損失関数の作成**

   ```python
   criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=4, device=device)
   ```

   IoUの閾値やハードネガティブの比率など、SSDの学習ルールを設定。
6. **ログで確認**

   * `print("criterion ready on", device)`
   * DataParallel の有無に関係なく、実際にパラメータが乗っているデバイスを表示
7. **学習開始**

   ```python
   num_epochs = 200
   train_model(net, dataloaders_dict, criterion, optimizer,
               num_epochs=num_epochs, start_epoch=start_epoch, device=device)
   ```

   準備が整ったら `train_model` に渡して本番スタート。

# 出力

* 画面ログとして

  * 使用デバイス
  * 損失関数の準備完了メッセージ
  * モデルが載っている実デバイス
* 以降は `train_model` 内で各エポックの進行・損失・保存などが出力されます。

# 典型的な落とし穴（上位3つ）と対策

1. **クラス数の不一致**

   * 症状：学習中に形状エラーや謎のNaN
   * 原因：`ssd_cfg['num_classes']` と `len(voc_classes)+1` がズレている
   * 対策：このコードの `assert` を活かす。落ちたらクラス定義を再確認（背景を+1すること）。
2. **モデルや損失関数の“デバイスの乗せ忘れ”**

   * 症状：`Expected all tensors to be on the same device` エラー
   * 原因：CPUとGPUが混在
   * 対策：`net.to(device)`、`criterion.to(device)` を必ず実施（本コードは実施済み）。外から追加した層や変数も同様に。
3. **DataParallelの実デバイス誤認識**

   * 症状：`cuda:0` に載せたはずなのに動かない/遅い
   * 原因：`net.module` を経由せずに確認している／複数GPU環境での混乱
   * 対策：このコードのように `base = net.module if ... else net` で安全に確認。学習中に `next(base.parameters()).device` を見る癖をつける。

# 用語ミニ解説

* **デバイス（device）**：計算を行う実行機械（GPUやCPU）。
* **MultiBoxLoss**：SSDで使う損失関数。位置ズレとクラスの間違いを同時に採点する。
* **IoU（Jaccard）閾値**：予測と正解の重なり度合いがどれくらいなら「当たり」とみなすかの基準。
* **Hard Negative Mining（neg\_pos）**：間違えやすい背景サンプルを多めに学習させるための比率の設定。
* **マイクロバッチ（MICRO\_BS）**：メモリ節約のために大きなバッチを小分けで処理し、勾配を貯めてから一括更新する単位。
* **AE（自己再構成）と `lambda_rec`**：特徴を自分で再現させる学習を少し混ぜて、表現を安定させる工夫。その影響の強さを決める重み。
* **DataParallel**：複数GPUに同じモデルを複製して同時並行で学習するPyTorchの仕組み。実パラメータは `net.module` の下にぶら下がる。

必要なら、この直前のセルで **学習率のスケジューラ** や **早期終了** を追加するなど、学習の安定化支援も組み込めます。

"""

# === 前景（GTボックス内）だけで μ/σ を推定する関数 ===
import math, torch
from torch import no_grad

@no_grad()
def compute_calib_stats_fg(
    dataloader,
    net,
    device,
    ssd_cfg=None,
    use_l2=USE_L2,
    max_batches=None,
    verbose=True,
):
    """
    前景GTボックス内のAE残差だけを段ごとに集計して μ/σ を推定する。
    - dataloader: phase="val" 前処理（=推論と同一）
    - targets: [xmin,ymin,xmax,ymax,label]（0-1正規化 or ピクセルいずれでも可）
    - net.forward(images) 後に net.last_rec = {'feat_hat': [...], 'feat_ref': [...]} を参照
    """
    net.eval()
    input_size = (ssd_cfg or {}).get('input_size', 300)

    sums, sqs, cnts = [], [], []
    n_used_imgs, n_used_boxes = 0, 0

    for bi, (images, targets) in enumerate(dataloader):
        if (max_batches is not None) and (bi >= max_batches):
            break

        images = images.to(device, non_blocking=True)
        _ = net(images)

        rec = getattr(net, 'last_rec', None)
        if rec is None:
            raise RuntimeError("net.last_rec が None。SSD.forward で self.last_rec を設定してください。")

        feat_hat, feat_ref = rec['feat_hat'], rec['feat_ref']  # list of [N,C,H,W]
        L = len(feat_hat)
        if len(sums) < L:
            diff = L - len(sums)
            sums.extend([0.0]*diff); sqs.extend([0.0]*diff); cnts.extend([0]*diff)

        # 残差（チャネル平均）の段ごとテンソル [N,1,H,W]
        err_maps = []
        for li in range(L):
            e = (feat_hat[li] - feat_ref[li])
            e = e.pow(2) if use_l2 else e.abs()
            e = e.mean(dim=1, keepdim=True)
            err_maps.append(e)

        N = images.size(0)
        for n in range(N):
            gt = targets[n]
            if gt is None or gt.numel() == 0:
                continue
            gt = gt.to('cpu')

            # 座標スケール自動判定（>1.5 があればピクセル座標）
            coords_are_norm = bool(float(gt[:, :4].max().item()) <= 1.5)

            for li, e in enumerate(err_maps):
                _, _, H, W = e.shape
                for bb in gt:
                    x0, y0, x1, y1 = bb[:4].tolist()
                    if not coords_are_norm:
                        x0 /= input_size; x1 /= input_size
                        y0 /= input_size; y1 /= input_size

                    # 0-1にクランプ
                    x0 = max(0.0, min(1.0, x0)); x1 = max(0.0, min(1.0, x1))
                    y0 = max(0.0, min(1.0, y0)); y1 = max(0.0, min(1.0, y1))
                    if x1 <= x0 or y1 <= y0:
                        continue

                    ix0 = int(math.floor(x0 * W)); ix1 = int(math.ceil(x1 * W))
                    iy0 = int(math.floor(y0 * H)); iy1 = int(math.ceil(y1 * H))
                    ix0 = max(0, min(W-1, ix0)); ix1 = max(1, min(W, ix1))
                    iy0 = max(0, min(H-1, iy0)); iy1 = max(1, min(H, iy1))
                    if ix1 <= ix0 or iy1 <= iy0:
                        continue

                    patch = e[n, 0, iy0:iy1, ix0:ix1]
                    if patch.numel() == 0:
                        continue

                    s  = float(patch.sum().item())
                    sq = float((patch*patch).sum().item())
                    c  = int(patch.numel())

                    sums[li] += s
                    sqs[li]  += sq
                    cnts[li] += c

                n_used_imgs  += 1
                n_used_boxes += int(gt.shape[0])

        if verbose and bi == 0:
            print(f"[calib-fg] batch0 layers={L} shapes=",
                  [tuple(t.shape) for t in feat_ref])

    if len(sums) == 0 or sum(cnts) == 0:
        raise RuntimeError("GT前景から有効サンプルが集計できません。calib_dataloader/アノテーション要確認。")

    mu_list, std_list = [], []
    for s, sq, c in zip(sums, sqs, cnts):
        if c <= 0:
            mu, std = 0.0, 1e-6
        else:
            mu  = s / c
            var = max(0.0, sq / c - mu*mu)
            std = (var + 1e-12) ** 0.5
        mu_list.append(mu); std_list.append(std)

    mu  = torch.tensor(mu_list,  dtype=torch.float32)
    std = torch.tensor(std_list, dtype=torch.float32)
    net.calib_mu, net.calib_std, net.calib_use_l2 = mu, std, bool(use_l2)

    if verbose:
        print(f"[calib-fg] images≈{n_used_imgs} boxes≈{n_used_boxes}")
        print("[calib-fg] layers =", len(mu_list),
              "mu =", [round(x, 6) for x in mu_list],
              "std =", [round(x, 6) for x in std_list])
    return mu, std

# calib_dataloader から1バッチ拝借して座標レンジを確認（0-1正規化か、300ピクセル系か）
imgs, targs = next(iter(calib_dataloader))
print("target[0] shape:", targs[0].shape)
print("coords max/min:", targs[0][:,:4].max().item(), targs[0][:,:4].min().item())
# 1.5以下なら 0-1 正規化と判定されます（関数内実装通り）

def load_calib_stats(net, path):
    """保存済み μ/σ を読み込み、net に設定。"""
    blob = torch.load(path, map_location='cpu')
    net.calib_mu     = blob.get('mu',  None)
    net.calib_std    = blob.get('std', None)
    net.calib_use_l2 = bool(blob.get('use_l2', True))
    print(f"[calib] loaded -> {path}  use_l2={net.calib_use_l2}")

def warmup_ae_epochs_fg(net, dataloaders_dict, device, ssd_cfg,
                        epochs=3, lambda_rec_warmup=0.5, use_l2=False, inner_shrink=0.9, lr=5e-4):
    """検出を凍結し、AEのみ“前景マスク付き”で短期ウォームアップ。"""
    net.to(device); net.train()
    # 検出側を凍結
    for p in list(net.vgg.parameters()) + list(net.extras.parameters()) \
            + list(net.loc.parameters()) + list(net.conf.parameters()):
        p.requires_grad = False
    # AEパラメータ
    ae_params = []
    for name in ('ae_proj2','ae_proj3','ae_proj4','ae_dec2','ae_dec3','ae_dec4'):
        if hasattr(net, name):
            ae_params += list(getattr(net, name).parameters())
    if not ae_params:
        print("[warmup_fg] AE modules not found. Skip."); net.eval(); return
    optim_ae = torch.optim.SGD(ae_params, lr=lr, momentum=0.9, weight_decay=1e-4)
    for ep in range(epochs):
        tot = 0.0
        for images, targets in dataloaders_dict['train']:
            images = images.to(device, non_blocking=True)
            optim_ae.zero_grad()
            _ = net(images)
            rec = getattr(net, 'last_rec', None)
            if rec is None:
                raise RuntimeError("last_rec が None。")
            # 既知だけをAE対象に
            known_targets = filter_known_targets(targets, num_known=len(voc_classes))
            loss_fg = recon_loss_from_feats_fg(
                rec['feat_hat'], rec['feat_ref'],
                targets=known_targets, ssd_cfg=ssd_cfg,
                kind=("l2" if use_l2 else "l1"),
            )
            (lambda_rec_warmup * loss_fg).backward()
            optim_ae.step()
            tot += float(loss_fg.item())
        print(f"[warmup_fg] epoch {ep+1}/{epochs} AE_fg_loss={tot:.4f}")
    for p in net.parameters():
        p.requires_grad = True
    net.eval()

# 先頭か、このセルの上で一度だけ
import json, numpy as np
from tqdm import tqdm

# しきい値の目標FPR（既知前景での誤検出率）
TARGET_FPR_PX  = 0.1   # ピクセル閾: 上位1%
TARGET_FPR_REG = 0.05  # 領域平均閾: 上位0.5%
R_MIN_DEFAULT  = 0.05   # 最小面積率（ROIに対して5%）
SAMPLE_PER_BATCH = 2000 # 1バッチあたりのサンプル上限（重いときは減らす）


def calibrate_ae_stats(calib_loader, net, ssd_cfg, voc_classes, device,
                       use_l2=USE_L2, pm_mode="default"):
    """
    - 既知ROI内で AE 残差を集計し、層ごとの μ/σ を推定
    - “生残差”の分布から ピクセル閾 tau_px と 領域平均閾 tau_reg を分位で決定
    生成: ae_stats.json = {"layers":[{"mu":..,"sigma":..},...], "tau_px":.., "tau_reg":.., "r_min":..}
    """
    import torch, hashlib
    import math

    # 簡易な重みメタ（厳密なckpt md5でなく、モデル先頭パラメタのmd5を目安として付与）
    def _md5_of_first_param(m):
        try:
            t = next(m.parameters()).detach().float().cpu().numpy().tobytes()
            return hashlib.md5(t).hexdigest()
        except Exception:
            return None

    net.eval()
    num_known = len(voc_classes)

    sums, sums2, counts = None, None, None
    per_pixel_vals = []

    input_size = int(ssd_cfg.get('input_size', 300))

    with torch.no_grad():
        for imgs, targets in tqdm(calib_loader, desc="[calib] AE stats"):
            imgs = imgs.to(device, non_blocking=True)
            # 既知クラスのGTだけ残す
            known_t = filter_known_targets([t.to(device) for t in targets], num_known=num_known)

            # forward（net.last_rec を更新）
            _ = net(imgs)
            rec = getattr(net, 'last_rec', None)
            if rec is None:
                # AE分岐が forward で last_rec をセットする設計か確認
                continue

            feat_hat_list = rec['feat_hat']
            feat_ref_list = rec['feat_ref']

            # 初回だけ器を用意
            if sums is None:
                L = len(feat_hat_list)
                sums  = [0.0]*L
                sums2 = [0.0]*L
                counts= [0]*L

            for li, (fh, fr) in enumerate(zip(feat_hat_list, feat_ref_list)):
                # e: [N,1,H,W]（L1残差。L2にしたいなら .pow(2) に変更）
                diff = (fh - fr)
                if use_l2:
                    e = diff.pow(2).mean(dim=1, keepdim=True)
                else:
                    e = diff.abs().mean(dim=1, keepdim=True)
                N,_,H,W = e.shape

                # 既知GTの前景マスク（同解像度へ射影）
                fg = _roi_mask_from_targets(known_t, (H,W), input_size, device, inner_shrink=0.9) # [N,1,H,W]

                # running stats（前景画素のみの合計・二乗合計・枚数）
                # ※ e*fg で前景以外は0になるので、sumは“前景上の合計”になる
                s  = (e * fg).sum().item()
                s2 = ((e * fg) ** 2).sum().item()
                c  = int(fg.sum().item())  # 画素カウント

                sums[li]  += s
                sums2[li] += s2
                counts[li]+= c

                # ピクセル閾用のサンプル（必ず“前景画素だけ”から抽出する）
                if c > 0 and SAMPLE_PER_BATCH > 0:
                    e_flat  = e.view(N, -1)
                    fg_flat = (fg.view(N, -1) > 0.5)
                    vals_fg = e_flat[fg_flat]  # 前景画素の値だけを抽出
                    if vals_fg.numel() > 0:
                        take = min(SAMPLE_PER_BATCH, vals_fg.numel())
                        idx  = torch.randint(0, vals_fg.numel(), (take,), device=vals_fg.device)
                        per_pixel_vals.append(vals_fg[idx].detach().cpu().numpy())

    # 何も集まらなかった場合のガード
    if sums is None:
        raise RuntimeError("校正で last_rec が一度も得られていません。calib_loader / AE分岐の動作を確認してください。")

    # 層別 μ,σ
    layers = []
    for li in range(len(sums)):
        if counts[li] <= 0:
            mu, sigma = 0.0, 1.0   # デフォルト
        else:
            mu = sums[li] / counts[li]
            var = max(0.0, (sums2[li] / counts[li]) - mu*mu)
            sigma = float(np.sqrt(var + 1e-12))
        layers.append({'mu': float(mu), 'sigma': sigma})

    # ピクセル閾（生eの分布から分位で決定）
    if len(per_pixel_vals):
        arr = np.concatenate(per_pixel_vals, axis=0)
        tau_px  = float(np.quantile(arr, 1.0 - TARGET_FPR_PX))
        tau_reg = float(np.quantile(arr, 1.0 - TARGET_FPR_REG))
    else:
        tau_px, tau_reg = 1.0, 1.5  # 最低限の安全デフォルト

    # --- JSON保存（拡張スキーマ） ---------------------------------------
    # metric は校正で使った距離（L1/L2）に合わせる
    metric = "L2" if use_l2 else "L1"

    stats = {
        "metric": metric,                         # "L1" or "L2"
        "weight_md5": _md5_of_first_param(net),   # 目安用のmd5。Noneでも可
        "pm_mode": pm_mode,                       # "default" or "sensitive"
        "layers": layers,                         # [{"mu":..,"sigma":..}, ...]
        "tau_px": float(tau_px),
        "tau_reg": float(tau_reg),
        "r_min": float(R_MIN_DEFAULT),
    }

    with open("ae_stats.json", "w") as f:
        json.dump(stats, f, indent=2)
    print("[calib] saved ae_stats.json")
    print(json.dumps(stats, indent=2))
    return stats


# 実行
stats = calibrate_ae_stats(
    calib_dataloader, net, ssd_cfg, voc_classes, device,
    use_l2=False,                 # ← L2で校正したい場合。L1なら False
    pm_mode="default"            # ← "sensitive" を使う運用なら変更
)

# === μ/σ を収集→保存→読み込み（前景ベース・重み整合メタ付き） ===
import os, time, json, hashlib, torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

net.to(device); net.eval()

# （任意）前景限定ウォームアップを使う場合は fg 版を呼ぶ
# warmup_ae_epochs_fg(net, dataloaders_dict, device, ssd_cfg,
#                     epochs=3, lambda_rec_warmup=0.5, use_l2=False, lr=5e-4)

# --- ここで“この重み”のMD5を記録 → μ/σ もこの重みで取る ---
def md5_of_first_param(m):
    t = next(m.parameters()).detach().float().cpu().numpy().tobytes()
    return hashlib.md5(t).hexdigest()

weight_md5 = md5_of_first_param(net)
print("[CHK] calib_before weight md5:", weight_md5)

# --- μ/σ（前景ベース）を収集 ---
mu, std = compute_calib_stats_fg(
    dataloader=calib_dataloader,   # phase="val"（No-Aug）
    net=net,
    device=device,
    ssd_cfg=ssd_cfg,
    use_l2=USE_L2,
    max_batches=None,
    verbose=True
)

# --- 取得に使った“同じ重み”を保存（推論でそのまま使えるように） ---
BEST_CALIB_PATH = os.path.join(WEIGHTS_DIR, "best_for_calib.pth")
torch.save({'model': net.state_dict(), 'classes': voc_classes, 'cfg': ssd_cfg}, BEST_CALIB_PATH)
print("[calib] saved weights for inference ->", BEST_CALIB_PATH)

# --- μ/σをメタ付きで保存（MD5/層数/時刻も保存） ---
CALIB_PATH_FG = os.path.join(WEIGHTS_DIR, 'calib_stats_fg.pth')
meta = {
    'mu': mu.cpu(),
    'std': std.cpu(),
    'use_l2': USE_L2,
    'weight_md5': weight_md5,
    'num_layers': int(len(mu)),
    'saved_at': time.strftime("%Y-%m-%d %H:%M:%S"),
}
torch.save(meta, CALIB_PATH_FG)
print("[calib-fg] saved ->", CALIB_PATH_FG)

# --- 明示的に読み込み（この場のnetにも反映） ---
load_calib_stats(net, CALIB_PATH_FG)
if isinstance(net.calib_mu, torch.Tensor):
    net.calib_mu = net.calib_mu.to(device)
    net.calib_std = net.calib_std.to(device)
print("[calib] loaded into current net. layers:", len(net.calib_mu))

# === D. 動作チェック（1回だけ） ===
import os, json

# 1) ファイルの存在チェック
assert os.path.exists("ae_stats.json"), "ae_stats.json がありません。calibrate_ae_stats のセルを見直してください。"

# 2) 中身の基本チェック
with open("ae_stats.json","r") as f:
    st = json.load(f)

assert "layers" in st and isinstance(st["layers"], list) and len(st["layers"]) > 0, "layers が不正です。"
assert "tau_px" in st and "tau_reg" in st and "r_min" in st, "tau_px/tau_reg/r_min がありません。"

print("[ok] layers:", len(st["layers"]))
print("[ok] tau_px:", st["tau_px"], " tau_reg:", st["tau_reg"], " r_min:", st["r_min"])

# 3) 先頭2層の μ/σ を軽く表示（目視チェック）
for i, d in enumerate(st["layers"][:2]):
    print(f"  layer{i}: mu={d.get('mu')}, sigma={d.get('sigma')}")

ds = dataloaders_dict['train'].dataset
img1,_ = ds[0]; img2,_ = ds[0]

import torch
if not torch.is_tensor(img1):
    from torchvision import transforms as T
    to_t = T.ToTensor()
    img1, img2 = to_t(img1), to_t(img2)

print("mean abs diff:", (img1.float()-img2.float()).abs().mean().item())
# ≈0 → No-Aug、明らかに >0 → まだAugが生きてる

from importlib import reload
import utils.ssd_model as sm; reload(sm)
import utils.ssd_predict_show as sps
reload(sps)

# ★ リロード後に改めて import
from utils.ssd_model import SSD, DataTransform
from utils.ssd_predict_show import SSDPredictShow

# 推論ネットを構築・重み/キャリブ読み込み（既存セルのまま）
ssd_cfg = {
    # クラス数（＋背景1クラス分）：
    #   物体クラスが N 個なら、背景を加えて N+1 にするのが一般的
    'num_classes': len(voc_classes) + 1,

    # モデルが受け取る画像の一辺の長さ（SSD300なら300）：
    'input_size': INPUT_SIZE,

    # 各特徴段で何種類のアンカー形状を置くか（デフォルト値の例）
    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],

    # 各段の特徴マップのサイズ（画像300から畳み込みで縮んでいく）
    'feature_maps': [38, 19, 10, 5, 3, 1],

    # 各段のストライド（元画像に対して、何ピクセルおきにアンカーを置くかの目安）
    'steps': [8, 16, 32, 64, 100, 300],

    # アンカーの最小サイズ（デフォルトの例。のちに提案値で上書きする）
    'min_sizes': [21, 45, 99, 153, 207, 261],

    # アンカーの最大サイズ（同上。のちに提案値で上書きする）
    'max_sizes': [45, 99, 153, 207, 261, 315],

    # 各段の縦横比候補（細長さ）：
    #   例では [2] または [2,3] の簡素な構成にしている
    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],

    # ▼ 追加：推論時のふるまい（学習時と整合をとるために固定しておくと混乱が少ない）
    'conf_thresh': 0.4,   # これ未満は“自信が足りない”として捨てる
    'nms_thresh' : 0.55,  # 重なり整理のしきい値（近い箱をまとめて残す）
    'top_k'      : 400,   # 一時的に保持する上限（多すぎると遅く、少なすぎると取りこぼす）
}

# === （推論）学習と同じアンカーに統一 ===
# ckptにcfgが入っていればロード側が優先されるが、無い場合に備えて上書きしておく
ssd_cfg['min_sizes'] = [int(x) for x in list(min_sizes)]
ssd_cfg['max_sizes'] = [int(x) for x in list(max_sizes)]
ssd_cfg['aspect_ratios'] = aspect_ratios
print("[cfg/infer] anchors updated (pre-ckpt):", ssd_cfg['min_sizes'], ssd_cfg['max_sizes'])


import os, torch, hashlib, json
from importlib import reload
import utils.ssd_model as sm; reload(sm)
from utils.ssd_model import SSD
from utils.ssd_predict_show import SSDPredictShow

def md5_of_first_param(m):
    t = next(m.parameters()).detach().float().cpu().numpy().tobytes()
    return hashlib.md5(t).hexdigest()

# 1) まず ckpt を読み込む（“校正時の重み”を優先）
BEST_PATH = os.path.join(WEIGHTS_DIR, "best_for_calib.pth")
if not os.path.exists(BEST_PATH):
    # フォールバック：従来の best.pth
    BEST_PATH = os.path.join(WEIGHTS_DIR, "best.pth")
state = torch.load(BEST_PATH, map_location='cpu')

# 2) cfg と voc_classes の整合
if isinstance(state, dict) and 'cfg' in state:
    ssd_cfg = state['cfg']; print("[inference] cfg loaded from ckpt.")
net = SSD(phase="inference", cfg=ssd_cfg)

classes_in_ckpt = state['classes'] if isinstance(state, dict) and 'classes' in state else None
if classes_in_ckpt is not None and list(classes_in_ckpt) != list(voc_classes):
    print("[warning] voc_classes mismatch:",
          f"ckpt={list(classes_in_ckpt)}  vs  current={list(voc_classes)}")
    # ★学習時のクラス順を真とし、評価側を合わせる
    voc_classes = list(classes_in_ckpt)

missing, unexpected = net.load_state_dict(state['model'] if 'model' in state else state, strict=False)
print("[inference] missing keys:", missing)
print("[inference] unexpected keys:", unexpected)
# 3) dbox 復元＋整合チェック
try:
    # DBoxのクラスを取得
    try:
        from utils.ssd_model import DBox
    except ModuleNotFoundError:
        from ssd_model import DBox

    import numpy as np, hashlib, torch

    dbox_ckpt = state.get('dbox', None) if isinstance(state, dict) else None
    dbox_md5_saved = state.get('dbox_md5', None) if isinstance(state, dict) else None
    if dbox_ckpt is not None:
        net.dbox_list = dbox_ckpt.to(next(net.parameters()).device)
        def _md5_tensor(t: torch.Tensor) -> str:
            arr = t.detach().float().cpu().numpy().view(np.uint8)
            return hashlib.md5(arr.tobytes()).hexdigest()
        md5_now = _md5_tensor(net.dbox_list)
        try:
            dbox_regen = DBox(ssd_cfg).make_dbox_list()
            md5_regen = _md5_tensor(dbox_regen)
        except Exception:
            md5_regen = None
        if (dbox_md5_saved and md5_now != dbox_md5_saved) or (md5_regen and md5_now != md5_regen):
            print(f"[WARNING] dbox content mismatch! saved={dbox_md5_saved} now={md5_now} regen={md5_regen}")
    else:
        # フォールバック：cfg から生成
        net.dbox_list = DBox(ssd_cfg).make_dbox_list().to(next(net.parameters()).device)
except Exception as e:
    print("[warn] dbox restore/check skipped:", e)

# キャリブ読み込み時に L1 を強制
load_calib_stats(net, "path/to/ae_stats.pth", use_l2=False)

# 異常マップ生成を L1 指定で
rgb, boxes, labels, scores, anom, pm_flags, pm_scores, pm_area, pm_mean, hm_vis = \
    pred.ssd_predict_with_anomaly(image_path, use_l2=False)

# 3) μ/σ 読み込み + MD5 整合チェック
CALIB_PATH = os.path.join(WEIGHTS_DIR, 'calib_stats_fg.pth')
if os.path.exists(CALIB_PATH):
    meta = torch.load(CALIB_PATH, map_location='cpu')
    load_calib_stats(net, CALIB_PATH)
    # デバイス合わせ
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    if isinstance(net.calib_mu, torch.Tensor):
        net.calib_mu = net.calib_mu.to(device)
        net.calib_std = net.calib_std.to(device)
    # 整合（MD5）
    md5_now = md5_of_first_param(net)
    md5_saved = meta.get('weight_md5', None)
    if md5_saved is not None and md5_now != md5_saved:
        print(f"[WARNING] weight/μσ mismatch! now={md5_now} saved={md5_saved}")
    else:
        print("[inference] weight/μσ md5 match.")
else:
    net.calib_mu = None; net.calib_std = None; net.calib_use_l2 = False
    print("[inference] calib not found -> standardization is skipped")

import json
TEMP_PATH = os.path.join(WEIGHTS_DIR, 'temperature.json')
if os.path.exists(TEMP_PATH):
    with open(TEMP_PATH, 'r') as f:
        obj = json.load(f)
    try:
        T = float(obj.get("T", 1.0))
    except Exception:
        T = 1.0
    net.detect.temperature = max(0.25, min(5.0, T))  # T<1 を防ぐ
    print("[inference] loaded temperature =", net.detect.temperature)
else:
    net.detect.temperature = 1.0
    print("[inference] temperature file not found. use T=1.0")

# 重要ログ（常時）
try:
    md5_now = md5_of_first_param(net)
    L_mu = int(len(net.calib_mu)) if isinstance(net.calib_mu, torch.Tensor) else -1
    print(f"[status] weight_md5={md5_now}  classes={list(voc_classes)}  calib_layers={L_mu}")
except Exception as e:
    print("[status] logging failed:", e)


# --- 既存の md5 計算の直後に追記 ---
setattr(net, "weight_md5", md5_now)   # ← JSON照合のために持たせる

# --- 予測器の初期化（外部JSONがあれば μ/σ/τ を上書きする方針を明示） ---
pred = SSDPredictShow(
    eval_categories=voc_classes,
    net=net,
    stats_policy="external_overrides",   # 既定でOKだが方針を明記
    # tau_scale=(1.0, 1.0, 1.0),        # ← しきい値を動かしたい時だけ明示
)

# （必要に応じて別名）
predictor = pred

USE_L2 = (getattr(predictor, "_ae_metric", "L1") == "L2")

# --- JSON metric から推論時の use_l2 を決定 ---

print("[info] AE metric =", getattr(predictor, "_ae_metric", None), " -> use_l2:", USE_L2)

# === 簡易の健全性チェック（任意） ===
print("metric(JSON) =", getattr(predictor, "_ae_metric", None))
print("pm_mode(JSON) =", getattr(predictor, "_pm_mode", None))
print("pm_mode(effective default) =", getattr(sps, "DEFAULT_PM_MODE", "default"))
print("num_layers(mu) =", len(getattr(predictor.net, "calib_mu", []) or []))
print("num_layers(std) =", len(getattr(predictor.net, "calib_std", []) or []))

# 1枚流して last_rec & hm を確認（警告が出たらL1/L2/層数ズレを再確認）
test_path = val_img_list[0] if len(val_img_list) else train_img_list[0]
_ = predictor.ssd_predict_with_anomaly(
    image_file_path=test_path,
    data_confidence_level=0.4,
    fuse="wmean", use_l2=False,
    inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8
)
print("[sanity] one forward ok.")


net.eval()
pred = SSDPredictShow(eval_categories=voc_classes, net=net)

"""#　検証用関数"""

#検証用関数 アンカー／DBox／入力サイズの不整合チェック

import hashlib, numpy as np, torch
from pprint import pprint
# あなたのDBoxクラスがここにある想定（パスは環境に合わせて）
from utils.ssd_model import DBox


def _md5(a: np.ndarray) -> str:
    a = np.ascontiguousarray(a.view(np.uint8))
    return hashlib.md5(a.tobytes()).hexdigest()

def _forward_flexible(net, x):
    """net(x) の戻り値が (loc,conf,dbox) / (loc,conf) / dict / Tensor の
       どれでも、可能な限り loc,conf,dbox_list を返す便利関数。"""
    with torch.no_grad():
        out = net(x)
    loc = conf = dbox_list = None
    if isinstance(out, tuple):
        if len(out) == 3:
            loc, conf, dbox_list = out
        elif len(out) == 2:
            loc, conf = out
            dbox_list = getattr(net, "dbox_list", None)
    elif isinstance(out, dict):
        loc = out.get("loc"); conf = out.get("conf"); dbox_list = out.get("dbox_list")
    # Detect後の最終Tensorのみ返す実装は、このチェックではスキップ（priors一致が見れないため）
    return loc, conf, dbox_list

def check_priors_and_shapes(net, predictor=None):
    print("=== [A] input_size / cfg / feature maps ===")
    in_size = int(getattr(net, "input_size", -1))
    cfg = getattr(net, "cfg", None)
    print("net.input_size:", in_size)
    if cfg and isinstance(cfg, dict):
        print("cfg keys:", sorted(cfg.keys()))
        if "input_size" in cfg: print("cfg['input_size']:", cfg["input_size"])
        if "feature_maps" in cfg: print("cfg['feature_maps']:", cfg["feature_maps"])
        if "min_sizes" in cfg: print("cfg['min_sizes'] (per fmap):", cfg["min_sizes"])
        if "max_sizes" in cfg and cfg["max_sizes"] is not None:
            print("cfg['max_sizes'] (per fmap):", cfg["max_sizes"])
        if "aspect_ratios" in cfg:
            print("cfg['aspect_ratios'] (per fmap):", cfg["aspect_ratios"])
    if predictor is not None:
        ps = getattr(predictor, "_input_size", None)
        print("predictor._input_size:", ps)

    print("\n=== [B] DBox 一致確認（md5/本数） ===")
    # net が保持している dbox_list
    dbox_net = getattr(net, "dbox_list", None)
    if dbox_net is not None:
        dbox_net_np = dbox_net.detach().cpu().numpy()
        print("net.dbox_list shape:", dbox_net_np.shape, " md5:", _md5(dbox_net_np))
    else:
        print("net.dbox_list: None")

    # cfg から再生成した dbox_list
    if cfg:
        try:
            dbox_cfg = DBox(cfg).make_dbox_list()
            dbox_cfg_np = dbox_cfg.detach().cpu().numpy()
            print("cfg→DBox shape:", dbox_cfg_np.shape, " md5:", _md5(dbox_cfg_np))
        except Exception as e:
            print("[warn] DBox(cfg) 生成に失敗:", e)

    print("\n=== [C] loc/conf のチャネル本数と priors の突き合わせ ===")
    # ダミー入力で forward
    size = in_size if in_size > 0 else int(cfg["input_size"]) if cfg and "input_size" in cfg else 300
    x = torch.zeros(1, 3, size, size).to(next(net.parameters()).device)
    loc, conf, dbox_from_fw = _forward_flexible(net, x)
    if dbox_from_fw is not None:
        print("forward() が dbox_list を返した: shape =", tuple(dbox_from_fw.shape))
    # loc/conf の shape から priors を推定
    if loc is not None:
        loc_shape = tuple(loc.shape)
        # よくある形: [B, num_priors*4] or [B, num_priors*4, H, W] or [B, num_priors, 4]
        num_priors_from_loc = None
        if loc.dim() == 2:
            num_priors_from_loc = loc.shape[1] // 4
        elif loc.dim() == 3 and loc.shape[-1] == 4:
            num_priors_from_loc = loc.shape[1]
        elif loc.dim() == 4:
            # 例: [B, Σ(k_i*4), H?, W?] → 総数はチャンネル/4 * H * W など実装依存
            c = loc.shape[1]
            h, w = loc.shape[2], loc.shape[3]
            num_priors_from_loc = (c // 4) * h * w
        print("loc shape:", loc_shape, "→ 推定priors:", num_priors_from_loc)
    if conf is not None:
        conf_shape = tuple(conf.shape)
        print("conf shape:", conf_shape)

    # 比較（net.dbox_list があれば本数一致を見る）
    if dbox_net is not None and loc is not None:
        np_loc = num_priors_from_loc
        np_dbox = int(dbox_net.shape[0])
        ok = (np_loc == np_dbox)
        print(f"priors 本数一致? loc側={np_loc}, dbox側={np_dbox} →", "OK" if ok else "NG")

    print("\n=== [D] 実際の feature maps の空間形状 ===")
    # sources shapes を出すフックがあればベスト。無ければ forward中にprintする仕組みを使う。
    # ここでは net 側がログを出す実装前提のため省略。代わりに cfg の期待値を表示済み。
    print("※ 学習時ログと [B][C] の md5/本数が一致すれば DBox/anchors は整合していると判断可。")

# 実行
check_priors_and_shapes(net, predictor=pred)

#検証用関数２ 前処理の**色順（BGR↔RGB）**ミス検出
import os, cv2, numpy as np, tempfile

def _predict_one(predictor, path, conf=0.05):
    (rgb_img, bbox, label_idx, conf_scores,
     *_rest) = predictor.ssd_predict_with_anomaly(
        image_file_path=path, data_confidence_level=conf,
        fuse="wmean", use_l2=False, inner_shrink=1.0, ring=8,
        drop_first=1, gamma=0.8, layer_weights=None
    )
    bbox = np.asarray(bbox, dtype=np.float32)
    scores = np.asarray(conf_scores, dtype=np.float32)
    return bbox, scores

def _swap_and_save(src_path):
    """BGRで読み→RGBにチャンネル入替→その配列をBGRとしてimwrite（=実画像のチャンネルを意図的に入替）"""
    bgr = cv2.imread(src_path, cv2.IMREAD_COLOR)
    if bgr is None:
        raise FileNotFoundError(src_path)
    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".jpg")
    cv2.imwrite(tmp.name, rgb)  # imwrite は BGR前提なので、敢えてRGB配列を渡して"間違った順"で保存
    tmp.close()
    return tmp.name

def probe_color_order(predictor, img_paths, conf=0.05, topn=10):
    print("=== Color-order A/B probe ===")
    stats = []
    temp_files = []
    try:
        for p in img_paths[:topn]:
            pB = p
            pS = _swap_and_save(p)  # スワップ画像
            temp_files.append(pS)

            bbA, scA = _predict_one(predictor, pB, conf=conf)
            bbB, scB = _predict_one(predictor, pS, conf=conf)

            stats.append([
                len(scA), float(np.mean(scA)) if scA.size else 0.0,
                len(scB), float(np.mean(scB)) if scB.size else 0.0
            ])
        stats = np.array(stats, dtype=np.float32)
        A_cnt, A_mean, B_cnt, B_mean = stats[:,0], stats[:,1], stats[:,2], stats[:,3]
        print(f"#images tested: {len(stats)}")
        print("[A] normal   : mean#det={A:.2f}, mean(score)={B:.3f}".format(A=A_cnt.mean(), B=A_mean.mean()))
        print("[B] swapped  : mean#det={A:.2f}, mean(score)={B:.3f}".format(A=B_cnt.mean(), B=B_mean.mean()))
        gain_cnt  = A_cnt.mean() - B_cnt.mean()
        gain_score= A_mean.mean() - B_mean.mean()
        print(f"[A-B] detections gain = {gain_cnt:.2f}, score gain = {gain_score:.3f}")
        if gain_cnt > 0.5 and gain_score > 0.02:
            print("→ 現在の色順（A）が明確に正。")
        elif gain_cnt < -0.5 and gain_score < -0.02:
            print("→ 色順が逆の可能性大（Bの方が良い）。transform内のBGR/RGB処理を見直し。")
        else:
            print("→ 差が小さい。色順は致命的ではないか、他要因が支配的。")
    finally:
        for f in temp_files:
            try: os.remove(f)
            except: pass

# 使い方：検証サンプルを数枚入れて実行
sample_imgs = val_img_list[:10] if len(val_img_list) >= 10 else val_img_list
probe_color_order(pred, sample_imgs, conf=0.05)

#検証用関数３　予測スコアの分布　しきい値のせいで0件になっていないか

# === Score distribution probe ===
import numpy as np

def probe_score_distribution(predictor, img_list, conf=0.05):
    all_scores = []
    for p in img_list:
        (_rgb, _b, _l, scores, *_rest) = predictor.ssd_predict_with_anomaly(
            image_file_path=p, data_confidence_level=conf,
            fuse="wmean", use_l2=False, inner_shrink=1.0, ring=8,
            drop_first=1, gamma=0.8, layer_weights=None
        )
        s = np.asarray(scores, dtype=np.float32)
        if s.size: all_scores.extend(s.tolist())
    if not all_scores:
        print("[scores] no predictions at conf >=", conf); return
    all_scores = np.array(all_scores, dtype=np.float32)
    qs = np.percentile(all_scores, [50, 75, 90, 95, 99])
    print(f"[scores] N={len(all_scores)}  p50={qs[0]:.3f} p75={qs[1]:.3f} p90={qs[2]:.3f} p95={qs[3]:.3f} p99={qs[4]:.3f}")
    for thr in [0.3, 0.4, 0.5, 0.6]:
        print(f"  >= {thr:.1f} : {(all_scores >= thr).mean()*100:.1f}% ({int((all_scores >= thr).sum())})")

probe_score_distribution(pred, val_img_list, conf=0.05)

# 検証用関数4 GTごとのベストIoU分布（「当たってない」かを確認）
# === Best IoU per GT (with low conf) ===
import numpy as np

def best_iou_per_gt(predictor, img_list, anno_list, classes, conf=0.05):
    from math import isnan
    ious_all = []
    for img_path, xml_path in zip(img_list, anno_list):
        # GT（px座標）
        gt_boxes, gt_labels, (W,H) = parse_voc(xml_path, classes)
        gt_boxes = np.asarray(gt_boxes, dtype=np.float32)
        # Pred（正規化→px変換）
        (_rgb, pb_norm, _pl, _ps, *_rest) = predictor.ssd_predict_with_anomaly(
            image_file_path=img_path, data_confidence_level=conf,
            fuse="wmean", use_l2=False, inner_shrink=1.0, ring=8,
            drop_first=1, gamma=0.8, layer_weights=None
        )
        pb_norm = np.asarray(pb_norm, dtype=np.float32)
        if gt_boxes.size == 0 or pb_norm.size == 0:
            continue
        b = pb_norm.copy()
        b[:, [0,2]] *= float(W); b[:, [1,3]] *= float(H)
        I = iou_xyxy(gt_boxes, b)  # [G,P]
        if I.size: ious_all.extend(I.max(axis=1).tolist())
    if not ious_all:
        print("[IoU] no GT×Pred overlap to evaluate"); return
    ious_all = np.array(ious_all, dtype=np.float32)
    qs = np.percentile(ious_all, [25,50,75,90,95])
    print(f"[IoU] best IoU per GT  p25={qs[0]:.3f} p50={qs[1]:.3f} p75={qs[2]:.3f} p90={qs[3]:.3f} p95={qs[4]:.3f}")

best_iou_per_gt(pred, val_img_list, val_anno_list, voc_classes, conf=0.05)

## 検証用関数５ === μ/σの現在値を確認＆必要ならTensor化（新規セルA） ===
import torch

def show_calib_source(model):
    mu, std = getattr(model, 'calib_mu', None), getattr(model, 'calib_std', None)
    typ = (type(mu).__name__, type(std).__name__)
    L = (len(mu) if mu is not None else -1)
    head_mu = [float(mu[i]) for i in range(min(3, L))] if mu is not None else None
    head_std= [float(std[i]) for i in range(min(3, L))] if std is not None else None
    print(f"[calib] type(mu,std)={typ}  layers={L}  mu[:3]={head_mu}  std[:3]={head_std}")

print("before:")
show_calib_source(net)

# list なら Tensor 化（推論用に確実化）
dev = next(net.parameters()).device
if isinstance(net.calib_mu, list):
    net.calib_mu = torch.tensor(net.calib_mu, dtype=torch.float32, device=dev)
if isinstance(net.calib_std, list):
    net.calib_std = torch.tensor(net.calib_std, dtype=torch.float32, device=dev)

print("after:")
show_calib_source(net)

#検証用関数６　層の形状ズレチェック

# === AE layer alignment checker ===
import numpy as np, torch

def _len_like(x):
    if x is None: return 0
    if isinstance(x, (list, tuple)): return len(x)
    if torch.is_tensor(x) and x.ndim >= 1: return int(x.shape[0])
    return 0

@torch.no_grad()
def check_ae_alignment(predictor, img_path, use_l2=USE_L2, drop_first=0):
    """
    1枚流して net.last_rec を更新し、calib_mu/std と
    層数・形状・ブロードキャスト可否をチェックする。
    """
    # 1) 1枚実行（最後に呼ぶ関数は既存のままでOK）
    (_rgb, _bbox, _lbl, _score,
     *_rest) = predictor.ssd_predict_with_anomaly(
        image_file_path=img_path, data_confidence_level=0.4,
        fuse="wmean", use_l2=False, inner_shrink=1.0, ring=8,
        drop_first=drop_first, gamma=0.8, layer_weights=None
    )

    net = predictor.net
    dev = next(net.parameters()).device
    rec = getattr(net, "last_rec", {}) or {}
    feat_hat = rec.get("feat_hat", [])  # 推測：再構成特徴
    feat_in  = rec.get("feat", []) or rec.get("feat_in", [])  # 元特徴（存在すれば）

    mu = getattr(net, "calib_mu", None)
    sd = getattr(net, "calib_std", None)

    L_rec = _len_like(feat_hat)
    L_mu  = _len_like(mu)
    L_sd  = _len_like(sd)

    print("=== AE layer alignment ===")
    print(f"L_rec={L_rec}  L_mu={L_mu}  L_std={L_sd}  (drop_first={drop_first})")

    ok_counts = (L_rec > 0) and (L_rec == L_mu == L_sd)
    print("count match:", "OK" if ok_counts else "NG")

    if not ok_counts:
        print("[hint] calib の作り直し or JSONの μ/σ を学習構成に合わせて更新してください。")
        return False

    # 2) 各層で (feat_hat[i] - mu[i]) / (std[i]) がブロードキャスト可能か検査
    all_ok = True
    for i in range(L_rec):
        r = feat_hat[i]
        m = mu[i]; s = sd[i]
        if torch.is_tensor(m): m = m.to(dev)
        if torch.is_tensor(s): s = s.to(dev)

        # 形状を取得
        r_shape = tuple(r.shape)              # 想定: [B,C,H,W]
        m_shape = tuple(m.shape) if torch.is_tensor(m) else (np.array(m).shape if not np.isscalar(m) else ())
        s_shape = tuple(s.shape) if torch.is_tensor(s) else (np.array(s).shape if not np.isscalar(s) else ())

        # ブロードキャスト検証（実際に演算してみる）
        ok_shape = True
        err_msg = ""
        try:
            _ = (r - torch.as_tensor(m, device=dev, dtype=r.dtype)) / (torch.as_tensor(s, device=dev, dtype=r.dtype) + 1e-12)
        except Exception as e:
            ok_shape = False
            err_msg = str(e)

        # 参考：空間サイズ確認（cfgのfeature_mapsと照合）
        H, W = r_shape[-2], r_shape[-1]

        print(f"[{i:02d}] rec={r_shape}  mu={m_shape}  std={s_shape}  → broadcast:", "OK" if ok_shape else f"NG ({err_msg})")

        if not ok_shape:
            all_ok = False

    if all_ok:
        print("→ 層の順序・形状ともに整合しています。")
    else:
        print("→ 少なくとも1層で μ/σ と再構成特徴の形状が一致していません。μ/σの再作成（学習と同一前処理・input_sizeでのキャリブ）を推奨。")

    return all_ok

# 実行例（valが空ならtrainなど任意の1枚に置換）
_test_path = val_img_list[0] if len(val_img_list) else train_img_list[0]
_ = check_ae_alignment(pred, _test_path, use_l2=USE_L2, drop_first=0)

# === Val mAP(2クラス) と 混同行列（セル①の直後に新規セル） ===
import xml.etree.ElementTree as ET
import numpy as np

IOU_THRESH = 0.5
CONF_FOR_MAP = 0.05   # mAPは低しきいで広く拾ってスコア順評価
CONF_FOR_CM  = 0.35   # 混同行列用の確信度しきい値

def parse_voc(xml_path, classes):
    root = ET.parse(xml_path).getroot()
    W = int(root.find('size/width').text); H = int(root.find('size/height').text)
    boxes, labels = [], []
    for obj in root.iter('object'):
        name = obj.find('name').text.strip()
        if name not in classes:  # 未知ラベルは除外
            continue
        b = obj.find('bndbox')
        xmin = max(0, int(b.find('xmin').text)-1)
        ymin = max(0, int(b.find('ymin').text)-1)
        xmax = int(b.find('xmax').text)-1
        ymax = int(b.find('ymax').text)-1
        boxes.append([xmin, ymin, xmax, ymax])
        labels.append(classes.index(name))
    return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64), (W, H)

def iou_xyxy(a, b):
    if a.size == 0 or b.size == 0:
        return np.zeros((a.shape[0], b.shape[0]), dtype=np.float32)
    A = a[:, None, :]; B = b[None, :, :]
    inter_x1 = np.maximum(A[...,0], B[...,0]); inter_y1 = np.maximum(A[...,1], B[...,1])
    inter_x2 = np.minimum(A[...,2], B[...,2]); inter_y2 = np.minimum(A[...,3], B[...,3])
    inter_w = np.clip(inter_x2 - inter_x1, 0, None); inter_h = np.clip(inter_y2 - inter_y1, 0, None)
    inter = inter_w * inter_h
    area_a = (A[...,2]-A[...,0])*(A[...,3]-A[...,1])
    area_b = (B[...,2]-B[...,0])*(B[...,3]-B[...,1])
    union = area_a + area_b - inter + 1e-6
    return (inter / union).astype(np.float32)

def eval_map_and_confusion(val_img_list, val_anno_list, predictor, classes):
    """
    評価（VOC方式のmAP@IOU_THRESH と 混同行列）
    - 予測ラベルが SSD 慣習（背景=0, 前景=1..K）でも動くように、
      ここで 0..K-1 に正規化し、背景は除外する。
    - 予測ボックスは [0..1] 正規化なので、必ず (W,H) を掛けて px に変換してから IoU を計算する。
    """
    import numpy as np

    K = len(classes)  # 前景クラス数（背景は含まない）

    # 1) mAP 用に全予測を集約（キーは 0..K-1 固定）
    preds_by_class = {c: [] for c in range(K)}
    gts_by_image = []  # (gt_boxes_px, gt_labels_0based) を格納

    for i, (img_path, xml_path) in enumerate(zip(val_img_list, val_anno_list)):
        (rgb_img, bbox_norm, label_idx,
         conf_scores, anomaly_scores,
         pm_flags, pm_scores, pm_area, pm_mean,
         a_map_vis) = predictor.ssd_predict_with_anomaly(
            image_file_path=img_path, data_confidence_level=CONF_FOR_MAP,
            fuse="wmean", use_l2=False, inner_shrink=1.0, ring=8,
            drop_first=1, gamma=0.8, layer_weights=None
        )

        # numpy 化
        bbox_norm   = np.asarray(bbox_norm,   dtype=np.float32)  # (N,4) [0..1]
        label_idx   = np.asarray(label_idx,   dtype=np.int64)    # (N,)
        conf_scores = np.asarray(conf_scores, dtype=np.float32)  # (N,)

        # GT 読み込み（0..K-1 に正規化）＋ 画像サイズ取得
        gt_boxes_px, gt_labels, (W, H) = parse_voc(xml_path, classes)
        gt_boxes_px = np.asarray(gt_boxes_px, dtype=np.float32)
        gt_labels   = np.asarray(gt_labels,   dtype=np.int64)
        if gt_labels.size and gt_labels.min() >= 1 and gt_labels.max() <= K:
            gt_labels = gt_labels - 1  # 1..K → 0..K-1
        gts_by_image.append((gt_boxes_px, gt_labels))

        # 予測：背景(0)除外 ＆ 1..K → 0..K-1 へ正規化
        if label_idx.size:
            if label_idx.min() >= 1 and label_idx.max() <= K:
                label_idx = label_idx - 1
            valid = (label_idx >= 0) & (label_idx < K)
            bbox_norm_v = bbox_norm[valid]
            labels_v    = label_idx[valid]
            scores_v    = conf_scores[valid]

            # ★ 正規化座標 → px に変換（IoUは px 同士で計算する）
            if bbox_norm_v.size:
                b = bbox_norm_v.copy()
                b[:, [0, 2]] *= float(W)
                b[:, [1, 3]] *= float(H)
                bbox_px_v = b
            else:
                bbox_px_v = bbox_norm_v  # 空

            # クラスごとの予測を集約（画像ID, スコア, box(px)）
            for bb, lb, sc in zip(bbox_px_v, labels_v, scores_v):
                preds_by_class[int(lb)].append((i, float(sc), bb))

    # 2) mAP（VOC方式, IOU_THRESH & CONF_FOR_MAP を使用）
    APs = []
    for c in range(K):
        preds = sorted(preds_by_class[c], key=lambda x: -x[1])  # score降順
        tp = np.zeros(len(preds), dtype=np.float32)
        fp = np.zeros(len(preds), dtype=np.float32)
        matched = [np.zeros(len(gts_by_image[i][0]), dtype=bool) for i in range(len(gts_by_image))]

        for k, (img_idx, score, box_px) in enumerate(preds):
            gt_boxes_px, gt_labels = gts_by_image[img_idx]
            pos_idx = np.where(gt_labels == c)[0]
            if pos_idx.size == 0:
                fp[k] = 1.0
                continue

            ious = iou_xyxy(np.expand_dims(box_px, 0), gt_boxes_px[pos_idx])[0]  # (M,)
            if ious.size == 0:
                fp[k] = 1.0
                continue

            j = int(np.argmax(ious))
            best_iou = float(ious[j])
            orig_j = int(pos_idx[j])
            if (best_iou >= IOU_THRESH) and (not matched[img_idx][orig_j]):
                tp[k] = 1.0
                matched[img_idx][orig_j] = True
            else:
                fp[k] = 1.0

        tp_cum = np.cumsum(tp)
        fp_cum = np.cumsum(fp)
        npos = sum((gts_by_image[i][1] == c).sum() for i in range(len(gts_by_image)))
        rec = tp_cum / max(npos, 1)
        prec = tp_cum / np.maximum(tp_cum + fp_cum, 1e-6)

        mrec = np.concatenate(([0.0], rec, [1.0]))
        mpre = np.concatenate(([0.0], prec, [0.0]))
        for i_ in range(mpre.size - 1, 0, -1):
            mpre[i_ - 1] = max(mpre[i_ - 1], mpre[i_])
        idx = np.where(mrec[1:] != mrec[:-1])[0]
        ap = float(np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1]))
        APs.append(ap)

    mAP = float(np.mean(APs)) if APs else 0.0

    # 3) 混同行列（IoU>=0.5 & conf≥CONF_FOR_CM）
    cm = np.zeros((K, K), dtype=int)  # rows=true, cols=pred

    for (img_path, xml_path) in zip(val_img_list, val_anno_list):
        gt_boxes_px, gt_labels, (W, H) = parse_voc(xml_path, classes)
        gt_boxes_px = np.asarray(gt_boxes_px, dtype=np.float32)
        gt_labels   = np.asarray(gt_labels,   dtype=np.int64)
        if gt_labels.size and gt_labels.min() >= 1 and gt_labels.max() <= K:
            gt_labels = gt_labels - 1
        valid_gt = (gt_labels >= 0) & (gt_labels < K)
        gt_boxes_px = gt_boxes_px[valid_gt]
        gt_labels   = gt_labels[valid_gt]

        (rgb_img, pb_norm, pl,
         ps, anomaly_scores,
         pm_flags, pm_scores, pm_area, pm_mean,
         a_map_vis) = predictor.ssd_predict_with_anomaly(
            image_file_path=img_path, data_confidence_level=CONF_FOR_CM,
            fuse="wmean", use_l2=USE_L2, inner_shrink=0.97, ring=12,
            drop_first=0, gamma=0.9, layer_weights=None
        )

        pb_norm = np.asarray(pb_norm, dtype=np.float32)  # (P,4) [0..1]
        pl      = np.asarray(pl,      dtype=np.int64)
        ps      = np.asarray(ps,      dtype=np.float32)

        # 予測ラベルを 0..K-1 に正規化し、背景/異常値を除外
        if pl.size:
            if pl.min() >= 1 and pl.max() <= K:
                pl = pl - 1
            valid_pred = (pl >= 0) & (pl < K)
            pb_norm = pb_norm[valid_pred]
            pl      = pl[valid_pred]
            ps      = ps[valid_pred]

        if pb_norm.size == 0 or gt_boxes_px.size == 0:
            continue

        # ★ 正規化座標 → px に変換
        b = pb_norm.copy()
        b[:, [0, 2]] *= float(W)
        b[:, [1, 3]] *= float(H)
        pb_px = b  # (P,4) px

        I = iou_xyxy(gt_boxes_px, pb_px)  # [G,P]

        for g in range(len(gt_boxes_px)):
            p = int(np.argmax(I[g]))
            if I[g, p] >= IOU_THRESH and ps[p] >= CONF_FOR_CM:
                r = int(gt_labels[g])
                c = int(pl[p])
                if 0 <= r < K and 0 <= c < K:
                    cm[r, c] += 1

    return mAP, APs, cm


# 呼び出し
mAP, APs, cm = eval_map_and_confusion(val_img_list, val_anno_list, pred, voc_classes)
print(f"[eval] mAP@0.5 = {mAP:.4f}  (per-class: {[round(x,4) for x in APs]})")
print("[eval] confusion (rows=true, cols=pred):")
print(cm)

# === ROI異常ヘルパー（推論セットアップの直後に新規セル） ===
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from utils.ssd_model import DataTransform

@torch.no_grad()
def roi_zscore_from_lastrec(
    net,
    fuse="wmean",         # "mean" or "wmean"
    use_l2=USE_L2,
    inner_shrink=0.90,
    ring=8,
    drop_first=0,
    gamma=0.5
):
    """
    前提: 直前に net(x) を呼んでおり、net.last_rec = {'feat_hat','feat_ref'} がある。
    返り値: zmap（[1,1,Hmax,Wmax] の torch.Tensor）
    """
    rec = getattr(net, "last_rec", None)
    if rec is None:
        raise RuntimeError("net.last_rec がありません。先に net(x) を実行してください。")

    feat_hat, feat_ref = rec['feat_hat'], rec['feat_ref']  # list of [B,C,H,W]
    L = len(feat_hat)

    if getattr(net, 'calib_mu', None) is None or getattr(net, 'calib_std', None) is None:
        raise RuntimeError("calib_mu/std が未設定です。μ/σの校正をロードしてください。")

    device = feat_hat[0].device
    mu_obj  = getattr(net, "calib_mu", None)
    std_obj = getattr(net, "calib_std", None)
    if mu_obj is None or std_obj is None:
        raise RuntimeError("calib_mu/std が未設定です。μ/σの校正をロードしてください。")
    # list でも Tensor でも受け付ける
    if isinstance(mu_obj, list):
        mu = torch.tensor(mu_obj, dtype=torch.float32, device=device)
    else:
        mu = mu_obj.to(device).float()
    if isinstance(std_obj, list):
        std = torch.tensor(std_obj, dtype=torch.float32, device=device)
    else:
        std = std_obj.to(device).float()
    mu  = mu.view(-1, 1, 1, 1)
    std = std.view(-1, 1, 1, 1)

    # 残差→z
    z_maps = []
    for li in range(L):
        e = (feat_hat[li] - feat_ref[li])
        e = e.pow(2) if use_l2 else e.abs()
        e = e.mean(dim=1, keepdim=True)  # [B,1,H,W]
        z = (e - mu[li]) / (std[li] + 1e-12)
        z = torch.clamp(z, min=0.0)      # 負を切り捨て
        z_maps.append(z)

    # 解像度を最大層に合わせて upsample
    Hmax = max(z.shape[2] for z in z_maps)
    Wmax = max(z.shape[3] for z in z_maps)
    z_ups = [F.interpolate(z, size=(Hmax, Wmax), mode='bilinear', align_corners=False) for z in z_maps]

    # 浅層抑制（必要なら）
    if drop_first and drop_first > 0:
        z_ups = z_ups[drop_first:]

    # 重み（wmean の場合は深層ほど重く）
    if fuse == "wmean":
        ws = torch.linspace(1.0, 1.0 + gamma*(len(z_ups)-1), steps=len(z_ups), device=device).view(-1,1,1,1)
        z_stack = torch.stack(z_ups, dim=0)
        zmap = (ws * z_stack).sum(dim=0) / (ws.sum(dim=0) + 1e-12)  # [B,1,H,W]
    else:
        zmap = torch.stack(z_ups, dim=0).mean(dim=0)  # simple mean

    return zmap  # [B,1,Hmax,Wmax]

@torch.no_grad()
def roi_zscore_map(image_path, net, ssd_cfg,
                   fuse="wmean", use_l2=False,
                   drop_first=0, gamma=0.5):
    """
    画像1枚から AE 残差の z-map を作る。
    戻り値: 2D numpy (H,W), 0以上にクリップ済み。
    """
    device = next(net.parameters()).device
    input_size = ssd_cfg['input_size']
    color_mean = (104,117,123)

    bgr = cv2.imread(image_path)
    if bgr is None:
        raise FileNotFoundError(image_path)

    # 推論と同じ 'val' 前処理
    transformer = DataTransform(input_size, color_mean)
    img_np, _, _ = transformer(bgr, "val", None, None)  # HWC(BGR)

    # BGR→RGB, HWC→CHW
    device = next(net.parameters()).device
    x = torch.from_numpy(img_np[:, :, (2, 1, 0)]).permute(2, 0, 1).unsqueeze(0).to(device)

    # forward -> last_rec
    _ = net(x)
    rec = getattr(net, 'last_rec', None)
    if rec is None:
        raise RuntimeError("last_rec が None。SSD.forward 内で self.last_rec を設定してください。")

    feat_hat, feat_ref = rec['feat_hat'], rec['feat_ref']  # list of [1,C,H,W]
    L = len(feat_hat)
    if getattr(net, 'calib_mu', None) is None or getattr(net, 'calib_std', None) is None:
        raise RuntimeError("calib_mu/std が未設定。前景校正（μ/σ）を読み込んでください。")

    mu  = net.calib_mu.to(device).view(L, 1, 1, 1)
    std = net.calib_std.to(device).view(L, 1, 1, 1)

    # 層ごと z-map
    z_list = []
    for li in range(L):
        if drop_first and li < drop_first:
            continue
        e = (feat_hat[li] - feat_ref[li])
        e = e.pow(2) if use_l2 else e.abs()
        e = e.mean(dim=1, keepdim=True)          # [1,1,H,W]
        z = (e - mu[li]) / (std[li] + 1e-12)
        z = torch.clamp(z, min=0.0)              # 負側は0に
        z_list.append(z)

    # 最高解像度へ補間して融合
    Hmax = max(z.shape[2] for z in z_list)
    Wmax = max(z.shape[3] for z in z_list)
    z_ups = [F.interpolate(z, size=(Hmax, Wmax), mode=('bilinear'), align_corners=False) for z in z_list]
    Z = torch.stack(z_ups, dim=0)  # [L',1,H,W]

    if fuse == "mean":
        zmap = Z.mean(dim=0)[0]                  # [H,W]
    else:
        # wmean: 深層ほど重く（指数  gamma）
        Lp = Z.size(0)
        w = torch.arange(1, Lp+1, device=Z.device, dtype=Z.dtype) ** float(gamma)
        w = (w / w.sum()).view(Lp, 1, 1, 1)
        zmap = (Z * w).sum(dim=0)[0]             # [H,W]

    return zmap.detach().cpu().numpy()

def roi_features_from_boxes(zmap,
                            boxes,
                            img_shape=None,
                            inner_shrink=0.90,
                            ring=8,
                            topk_ratio=0.02):
    """
    zmap: 2D numpy or torch [H,W] / [1,1,H,W] / [1,H,W]
    boxes: [[x0,y0,x1,y1], ...] ピクセル座標（VOC）
    戻り: np.ndarray [G,3] = [inner_mean, bg_ring_mean, inner_topk_mean]
    """
    # zmap を 2D numpy に統一
    if isinstance(zmap, torch.Tensor):
        z = zmap.detach().float()
        if z.dim() == 4:
            z = z[0, 0]
        elif z.dim() == 3:
            z = z[0]
        z = z.cpu().numpy()
    else:
        z = np.asarray(zmap, dtype=np.float32)
        if z.ndim == 3:
            z = z[0]

    # ★ 修正：H, W の取り方
    H, W = z.shape[-2], z.shape[-1]

    feats = []
    for (x0, y0, x1, y1) in boxes:
        # クリップ＆整数化
        x0 = int(np.floor(max(0, min(W-1, x0))))
        y0 = int(np.floor(max(0, min(H-1, y0))))
        x1 = int(np.ceil (max(1, min(W,   x1))))
        y1 = int(np.ceil (max(1, min(H,   y1))))
        if x1 <= x0 or y1 <= y0:
            continue

        # inner（縮めた矩形）
        cx = 0.5 * (x0 + x1)
        cy = 0.5 * (y0 + y1)
        bw = (x1 - x0)
        bh = (y1 - y0)
        iw = max(1, int(round(bw * float(inner_shrink))))
        ih = max(1, int(round(bh * float(inner_shrink))))
        ix0 = int(round(cx - iw/2)); ix1 = int(round(cx + iw/2))
        iy0 = int(round(cy - ih/2)); iy1 = int(round(cy + ih/2))
        ix0 = max(0, min(W-1, ix0)); ix1 = max(1, min(W,   ix1))
        iy0 = max(0, min(H-1, iy0)); iy1 = max(1, min(H,   iy1))
        inner = z[iy0:iy1, ix0:ix1]
        if inner.size == 0:
            continue
        inner_mean = float(inner.mean())

        # 背景リング（元BBoxの外側 ring ピクセル幅）
        rx0 = max(0, x0 - ring); ry0 = max(0, y0 - ring)
        rx1 = min(W, x1 + ring); ry1 = min(H, y1 + ring)
        outer = z[ry0:ry1, rx0:rx1]
        if outer.size == 0:
            bg_mean = 0.0
        else:
            bg = outer.copy()
            ox0 = x0 - rx0; oy0 = y0 - ry0
            ox1 = ox0 + (x1 - x0); oy1 = oy0 + (y1 - y0)
            bg[oy0:oy1, ox0:ox1] = np.nan  # 内側（元BBox）は除外
            bg_mean = float(np.nanmean(bg)) if np.isfinite(bg).any() else 0.0

        # inner の Top-k%
        flat = inner.reshape(-1)
        k = max(1, int(round(len(flat) * float(topk_ratio))))
        topk = np.partition(flat, -k)[-k:]
        topk_mean = float(np.mean(topk))

        feats.append([inner_mean, bg_mean, topk_mean])

    if len(feats) == 0:
        return np.zeros((0, 3), dtype=np.float32)
    return np.asarray(feats, dtype=np.float32)

# === サニティ：μ/σの層数整合＆ワンステップ推論（ROIヘルパーの直後に配置） ===
import cv2, torch
from utils.ssd_model import DataTransform

# pred を predictor 名で使いたい場合
predictor = pred

# 1) μ/σの層数がロードされているか
try:
    L_mu = int(len(net.calib_mu)) if isinstance(net.calib_mu, torch.Tensor) else -1
    print(f"[sanity] calib layers: {L_mu}")
except Exception as e:
    print("[sanity] calib tensors missing:", e)

# 2) 1枚だけ forward（predictor 経由で net.last_rec が埋まるか）
test_path = val_img_list[0] if len(val_img_list) else train_img_list[0]
_ = predictor.ssd_predict_with_anomaly(
    image_file_path=test_path,
    data_confidence_level=0.4,  # 低めでOK（サニティ目的）
    fuse="wmean", use_l2=False,
    inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8, layer_weights=None
)
assert getattr(net, 'last_rec', None) is not None, "last_rec が取得できません。SSD.forward 内の self.last_rec を確認してください。"
print("[sanity] forward ok & last_rec present. test:", test_path)

# 3) 前処理のサイズサニティ（推論本番は predictor に任せる）
print("[sanity] color_mean(BGR):", (104,117,123))
bgr = cv2.imread(test_path); assert bgr is not None, f"imread fail: {test_path}"
tfm = DataTransform(ssd_cfg['input_size'], (104,117,123))
img_t, _, _ = tfm(bgr, "val", None, None)
h, w = img_t.shape[:2]
assert (h, w) == (ssd_cfg['input_size'], ssd_cfg['input_size']), \
       f"Resize不一致: got {(h, w)} expected {(ssd_cfg['input_size'], ssd_cfg['input_size'])}"

try:
    L_rec = len(net.last_rec['feat_hat'])
    if L_mu > 0:
        print(f"[sanity] calib vs last_rec layers: {L_mu} vs {L_rec}")
except Exception:
    pass

# === フォールバック①：検出ゼロ時に使う“画像レベル異常” ===
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from utils.ssd_model import DataTransform

@torch.no_grad()
def image_level_anomaly(image_file_path,
                        net,
                        input_size=INPUT_SIZE,
                        color_mean=(104,117,123),
                        use_l2=False,
                        topk_ratio=0.02,
                        clip_negative=True,
                        device=None):
    """
    検出ゼロ時のフォールバック：
    画像全体のAE残差を標準化 → 全層を同解像度へ補間 → 平均融合 →
    上位k%のzを平均してスカラー異常度を返す。
    """
    device = device or next(net.parameters()).device

    # 1) 推論と同じ 'val' 前処理
    bgr = cv2.imread(image_file_path)
    if bgr is None:
        raise FileNotFoundError(image_file_path)
    transformer = DataTransform(input_size, color_mean)
    img, _, _ = transformer(bgr, "val", None, None)

    device = next(net.parameters()).device
    x = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1).unsqueeze(0).to(device)

    # 2) forward → 誤差マップ
    _ = net(x)
    rec = getattr(net, 'last_rec', None)
    if rec is None:
        raise RuntimeError("last_rec が None。SSD.forward 内で self.last_rec を設定してください。")

    feat_hat, feat_ref = rec['feat_hat'], rec['feat_ref']  # list of [1,C,H,W]
    L = len(feat_hat)
    if getattr(net, 'calib_mu', None) is None or getattr(net, 'calib_std', None) is None:
        raise RuntimeError("calib_mu/std が未設定。前景校正（μ/σ）を読み込んでください。")

    mu  = net.calib_mu.to(device).view(L, 1, 1, 1)
    std = net.calib_std.to(device).view(L, 1, 1, 1)

    # 3) 残差→標準化z
    z_maps = []
    for li in range(L):
        e = (feat_hat[li] - feat_ref[li])
        e = e.pow(2) if use_l2 else e.abs()
        e = e.mean(dim=1, keepdim=True)          # [1,1,H,W]
        z = (e - mu[li]) / (std[li] + 1e-12)
        if clip_negative:
            z = torch.clamp(z, min=0.0)
        z_maps.append(z)

    # 4) 最高解像度に合わせて平均融合
    Hmax = max(z.shape[2] for z in z_maps)
    Wmax = max(z.shape[3] for z in z_maps)
    z_ups = [F.interpolate(z, size=(Hmax, Wmax), mode='bilinear', align_corners=False) for z in z_maps]
    z_all = torch.stack(z_ups, dim=0).mean(dim=0)  # [1,1,H,W]

    # 5) Top-k%平均でスカラー異常度
    flat = z_all.flatten()
    k = max(1, int(flat.numel() * float(topk_ratio)))
    topk = torch.topk(flat, k).values
    return float(topk.mean().item())

# === フォールバック②：検出ゼロなら閾値を下げ、それでもゼロなら画像レベル異常へ ===
@torch.no_grad()
def detect_with_fallback(image_path,
                         predictor,
                         net,
                         ssd_cfg,
                         base_conf=0.25,    # 初回しきい値
                         low_conf=0.05,     # 再試行しきい値
                         fuse="wmean",
                         use_l2=False,
                         inner_shrink=0.97,
                         ring=12,
                         drop_first=0,
                         gamma=0.9):

    # ① 通常推論
    (rgb_img, bbox, label_idx,
     conf_scores, anomaly_scores,
     pm_flags, pm_scores, pm_area, pm_mean,
     a_map_vis) = predictor.ssd_predict_with_anomaly(
        image_file_path=image_path,
        data_confidence_level=base_conf,   # ★ 修正：base_conf_internal → base_conf
        fuse=fuse, use_l2=False,
        inner_shrink=inner_shrink, ring=ring, drop_first=drop_first, gamma=gamma, layer_weights=None
    )

    if len(bbox) == 0:
        # ② 低閾値で再試行
        (rgb_img, bbox, label_idx,
         conf_scores, anomaly_scores,
         pm_flags, pm_scores, pm_area, pm_mean,
         a_map_vis) = predictor.ssd_predict_with_anomaly(
            image_file_path=image_path,
            data_confidence_level=low_conf,  # ★ 修正：再試行は low_conf
            fuse=fuse, use_l2=USE_L2,
            inner_shrink=inner_shrink, ring=ring, drop_first=drop_first, gamma=gamma, layer_weights=None
        )

        if len(bbox) == 0:
            # ③ 画像レベル異常のフォールバック
            img_lvl = image_level_anomaly(
                image_file_path=image_path,
                net=net,
                input_size=ssd_cfg['input_size'],
                color_mean=(104,117,123),
                use_l2=USE_L2,
                topk_ratio=0.02,
                clip_negative=True,
                device=next(net.parameters()).device
            )
            print(f"[fallback] image-level anomaly = {img_lvl:.3f}")
            predictor.vis_bbox(rgb_img, [], [], [], predictor.eval_categories, anomaly_scores=None)
            return rgb_img, bbox, label_idx, conf_scores, anomaly_scores, a_map_vis

    # 検出あり：通常の可視化
    _draw_with_abnormal(rgb_img, bbox, label_idx, conf_scores, anomaly_scores, pm_flags, predictor.eval_categories)
    return rgb_img, bbox, label_idx, conf_scores, anomaly_scores, a_map_vis

# === OOD拒否性能：画像レベル異常で AUROC / FPR を確認（セル④の直後に新規セル） ===
import glob, os, numpy as np

def image_scores(paths):
    scores = []
    for p in paths:
        try:
            s = image_level_anomaly(
                image_file_path=p, net=net,
                input_size=ssd_cfg['input_size'], color_mean=(104,117,123),
                use_l2=USE_L2, topk_ratio=0.02, clip_negative=True,
                device=next(net.parameters()).device
            )
            scores.append(float(s))
        except Exception:
            pass
    return np.array(scores, dtype=np.float32)

# Val画像（正常）と OOD 画像（フォルダ）からスコア収集
normal_paths = val_img_list
ood_paths    = sorted([p for p in glob.glob("./od_crabs_ood/*.*")
                       if p.lower().endswith((".jpg",".jpeg",".png"))])

normal_scores = image_scores(normal_paths)
ood_scores    = image_scores(ood_paths)

def auroc(pos, neg):  # pos: OOD（高いほど異常）, neg: 正常
    all_scores = np.concatenate([pos, neg])
    labels = np.concatenate([np.ones_like(pos), np.zeros_like(neg)])
    order = np.argsort(all_scores)                      # 昇順
    ranks = np.empty_like(order); ranks[order] = np.arange(len(all_scores))
    n1, n0 = float(len(pos)), float(len(neg))
    # 0-index → 1-index への補正は +n1
    sum_ranks_pos = float(ranks[labels==1].sum()) + n1
    return float((sum_ranks_pos - n1*(n1+1)/2.0) / (n1*n0 + 1e-9))

if normal_scores.size and ood_scores.size:
    # 正常の98%点を τ として、OOD をどれだけ超過で“拒否”できるか
    tau = float(np.quantile(normal_scores, 0.98))
    reject_rate_on_ood = float((ood_scores > tau).mean())
    auc = auroc(ood_scores, normal_scores)
    print(f"[ood] AUROC={auc:.4f}  tau@0.98={tau:.3f}  reject_rate_on_OOD={reject_rate_on_ood:.3f}")
else:
    print("[ood] not enough samples to compute AUROC/FPR.")

# === 異常しきい値 τ を Val から作る（適応ゲート & フォールバック付き） ===
import os, json, numpy as np

# 初期値（分布に合わせて現実的に）
THETA_CONF_INIT = 0.40   # あなたの分布では 0.35〜0.40 が妥当
THETA_MIN       = 0.30   # ここまで下げて良い（必要なら0.25まで許可可）
TAU_QUANT       = 0.98   # 正常Valの98%点を τ に
N_MIN           = 200    # τ 推定に使いたい最小検出数
TOPK_PER_IMG    = 3      # フォールバック：各画像の上位Kを採用
DATA_CONF_LV    = 0.03   # ssd_predict_with_anomaly の低しきい（広く拾う）

per_img = []        # [(conf_scores(np1d), anomaly_scores(np1d)), ...]
all_conf = []       # すべての conf をためる（適応ゲート用）

for img_path in val_img_list:
    out = predictor.ssd_predict_with_anomaly(
        image_file_path=img_path,
        data_confidence_level=DATA_CONF_LV,
        fuse="wmean", use_l2=False,
        inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8, layer_weights=None
    )
    (rgb_img, bbox, label_idx,
     conf_scores, anomaly_scores,
     pm_flags, pm_scores, pm_area, pm_mean,
     a_map_vis) = out

    cs = np.asarray(conf_scores, dtype=np.float32)
    an = np.asarray(anomaly_scores, dtype=np.float32)
    per_img.append((cs, an))
    if cs.size:
        all_conf.extend(cs.tolist())

# まず、conf分布が極端に低すぎないかをチェック
if len(all_conf) == 0:
    raise RuntimeError(
        "Val から検出が全く得られません。data_confidence_level をさらに下げる / "
        "NMS/温度スケーリングの適用を見直してください。"
    )
all_conf = np.asarray(all_conf, dtype=np.float32)

# 適応的に THETA を決める：初期値→件数が足りなければ段階的に下げる
theta = float(THETA_CONF_INIT)
tau_pool = []

def _gather(theta_val):
    pool = []
    for cs, an in per_img:
        if cs.size == 0 or an.size == 0:
            continue
        keep = (cs >= theta_val)
        if np.any(keep):
            pool.extend(an[keep].tolist())
    return pool

# 1) 固定初期値で試す
tau_pool = _gather(theta)

# 2) 件数不足なら 0.05 刻みで下げる（下限 THETA_MIN）
while (len(tau_pool) < N_MIN) and (theta > THETA_MIN):
    theta = max(THETA_MIN, theta - 0.05)
    tau_pool = _gather(theta)

# 3) まだ足りなければフォールバック：各画像ごとに conf 上位K の anomaly を採用
if len(tau_pool) == 0:
    for cs, an in per_img:
        if cs.size == 0 or an.size == 0:
            continue
        idx = np.argsort(-cs)[:TOPK_PER_IMG]
        tau_pool.extend(an[idx].tolist())

# 4) それでもゼロなら致命的（推論分布が壊れている）
assert len(tau_pool) > 0, (
    "Val から τ を作るための検出が集まりません。"
    "NMS/温度/前処理/学習cfgの整合を確認し、data_confidence_level を一時的にさらに下げて再実行してください。"
)

tau = float(np.quantile(np.array(tau_pool, dtype=np.float32), TAU_QUANT))

meta = {
    "tau": tau,
    "theta_conf_init": THETA_CONF_INIT,
    "theta_conf_effective": theta,
    "theta_min": THETA_MIN,
    "q": TAU_QUANT,
    "n": len(tau_pool),
    "n_images": len(per_img),
    "data_conf_level": DATA_CONF_LV,
    "fallback_topk": TOPK_PER_IMG if len(tau_pool) < N_MIN else 0
}
os.makedirs(WEIGHTS_DIR, exist_ok=True)
with open(os.path.join(WEIGHTS_DIR, 'anomaly_tau.json'), 'w') as f:
    json.dump(meta, f, indent=2)

print(f"[tau] tau={tau:.4f}  from {len(tau_pool)} detections "
      f"(theta_conf_eff={theta:.2f}, q={TAU_QUANT}, imgs={len(per_img)})")

# === μ/σ をロード or 生成して net に注入（z-mapセルの直前に実行） ===
import os, json, torch

device = next(net.parameters()).device

def _as_tensor(x):
    return x if torch.is_tensor(x) else torch.tensor(x, dtype=torch.float32)

loaded = False

# 1) 推奨：学習時に保存した pth を使う
CALIB_PATH = os.path.join(WEIGHTS_DIR, "calib_stats_fg.pth")
if os.path.exists(CALIB_PATH):
    meta = torch.load(CALIB_PATH, map_location='cpu')
    net.calib_mu  = _as_tensor(meta.get('mu',  None)).to(device)
    net.calib_std = _as_tensor(meta.get('std', None)).to(device)
    net.calib_use_l2 = bool(meta.get('use_l2', True))
    print("[fix] loaded mu/sigma from", CALIB_PATH)
    loaded = True

# 2) フォールバック：ae_stats.json（layers[].mu/sigma）
if not loaded and os.path.exists("ae_stats.json"):
    with open("ae_stats.json","r") as f:
        st = json.load(f)
    layers = st.get("layers", [])
    if layers:
        mu  = [float(d.get("mu", 0.0))    for d in layers]
        std = [float(d.get("sigma", 1.0)) for d in layers]
        net.calib_mu  = torch.tensor(mu,  dtype=torch.float32, device=device)
        net.calib_std = torch.tensor(std, dtype=torch.float32, device=device)
        net.calib_use_l2 = False
        print("[fix] loaded mu/sigma from ae_stats.json")
        loaded = True

# 3) それも無ければ、手早く前景校正を数バッチだけ回して推定（最終手段）
if not loaded:
    print("[fix] no calib file found; estimating mu/sigma on-the-fly (few batches)…")
    mu, std = compute_calib_stats_fg(
        dataloader=calib_dataloader,
        net=net, device=device, ssd_cfg=ssd_cfg,
        use_l2=False, max_batches=10, verbose=True  # 手早く10バッチだけ
    )
    net.calib_mu, net.calib_std = mu.to(device), std.to(device)
    net.calib_use_l2 = False
    loaded = True

# 念のため list→Tensor 変換（別経路で list が入っている場合の保険）
if isinstance(net.calib_mu, list):
    net.calib_mu = torch.tensor(net.calib_mu, dtype=torch.float32, device=device)
if isinstance(net.calib_std, list):
    net.calib_std = torch.tensor(net.calib_std, dtype=torch.float32, device=device)

# サニティ：層数を軽くチェック（last_rec は直後の forward で埋まる前提）
print(f"[fix] calib ready. layers={int(len(net.calib_mu))}")

# === (③-2) ROI異常しきい値 τ_roi を Val から作成 ===
import os, json, numpy as np

ROI_THRESH_PATH = os.path.join(WEIGHTS_DIR, "roi_anom_thresh.json")
os.makedirs(WEIGHTS_DIR, exist_ok=True)

# ROIヘルパーが定義済みかチェック（③-1 を先に実行）
try:
    _ = roi_zscore_map
    _ = roi_features_from_boxes
except NameError:
    raise RuntimeError("roi_zscore_map / roi_features_from_boxes が未定義です。③-1（ROIヘルパー定義セル）を先に実行してください。")

# parse_voc が未定義なら最小版を用意（あなたの環境に既にあればこのブロックはスキップされます）
try:
    _ = parse_voc
except NameError:
    import xml.etree.ElementTree as ET
    def parse_voc(xml_path, classes):
        root = ET.parse(xml_path).getroot()
        boxes, labels = [], []
        for obj in root.iter('object'):
            name = obj.find('name').text.strip()
            if name not in classes:
                continue  # 既知クラスのみ
            b = obj.find('bndbox')
            x0 = int(b.find('xmin').text)-1; y0 = int(b.find('ymin').text)-1
            x1 = int(b.find('xmax').text)-1; y1 = int(b.find('ymax').text)-1
            boxes.append([x0,y0,x1,y1]); labels.append(classes.index(name))
        return np.array(boxes, np.float32), np.array(labels, np.int64)

vals = []

# Val 画像を走査
for img_path, xml_path in zip(val_img_list, val_anno_list):
    # ① 推論を1回実行して net.last_rec を更新（phase='val' 前処理で）
    _ = predictor.ssd_predict_with_anomaly(
        image_file_path=img_path,
        data_confidence_level=0.4,   # 広く拾う。結果そのものは使わない
        fuse="wmean", use_l2=False,
        inner_shrink=1.0, ring=8,
        drop_first=1, gamma=0.8,
        layer_weights=None
    )

    # ② last_rec から融合 z-map を生成
    zmap = roi_zscore_from_lastrec(
        net,
        fuse="wmean",      # or "mean"
        use_l2=USE_L2,
        inner_shrink=0.90,
        ring=8,
        drop_first=0,
        gamma=0.5,
    )  # [1,1,H,W] tensor

    # ③ 既知クラスの GT を取得（3戻り値に対応）
    gt_ret = parse_voc(xml_path, voc_classes)
    if isinstance(gt_ret, tuple) and len(gt_ret) == 3:
        gt_boxes, gt_labels, _ = gt_ret
    else:
        gt_boxes, gt_labels = gt_ret

    if gt_boxes is None or len(gt_boxes) == 0:
        continue

    # ④ ROI特徴を抽出 → スカラ異常度に変換（例：topk_mean - bg_mean を使用）
    feats = roi_features_from_boxes(
        zmap,
        gt_boxes,           # ピクセル座標
        None,               # 実装が (H,W) 必須なら (H,W) を渡す（今回は不要実装）
        inner_shrink=0.90,
        ring=8
    )  # [G,3] = [inner_mean, bg_ring_mean, topk_mean]

    if feats.shape[0]:
        # スカラ化：topk_mean - bg_mean（負は0に丸め）
        scores = np.maximum(0.0, feats[:, 2] - feats[:, 1])
        vals.extend(scores.tolist())
    # ⑤ τ_roi（例：正常 Val の98%点）を保存
    assert len(vals) > 0, "Val既知GTからROI統計が得られません。val_img_list/val_anno_listや GT を確認してください。"
    tau_roi = float(np.quantile(np.array(vals, dtype=np.float32), 0.95))

    with open(ROI_THRESH_PATH, "w") as f:
        json.dump({"tau_roi": tau_roi, "q": 0.98, "n": len(vals)}, f)

    print(f"[roi] tau_roi@0.98 = {tau_roi:.4f}  (samples={len(vals)})")

# === 単発推論＋可視化セル（修正適用版） ===
# 前提: net / voc_classes / WEIGHTS_DIR / (pred or predictor) / roi_* ヘルパ が既に定義済み

import os, json, numpy as np
import matplotlib.pyplot as plt
from matplotlib import patches

# predictor ハンドル（pred を使っている環境向けフォールバック）
try:
    predictor
except NameError:
    predictor = pred  # pred = SSDPredictShow(eval_categories=voc_classes, net=net)

# 入力画像を指定
image_path = "/content/drive/MyDrive/SSD/test_image/crab1.jpg"

# τ（異常しきい）と θ（信頼度しきい）をロード（存在しなければフォールバック）
TAU_PATH = os.path.join(WEIGHTS_DIR, 'anomaly_tau.json')
if os.path.exists(TAU_PATH):
    with open(TAU_PATH, 'r') as f:
        meta_tau = json.load(f)
    TAU = float(meta_tau.get("tau", float("inf")))
    THETA_CONF = float(meta_tau.get("theta_conf", 0.60))
else:
    TAU = float("inf")   # τが無いときは“異常ゲートなし”に等価
    THETA_CONF = 0.60
    print("[warn] anomaly_tau.json not found.gating uses conf only.")

# 1) 低い内部閾値で候補を広く取得（この forward で net.last_rec も更新される）
(rgb_img, bbox, label_idx,
 conf_scores, anomaly_scores,
 pm_flags, pm_scores, pm_area, pm_mean,
 a_map_vis) = predictor.ssd_predict_with_anomaly(
     image_file_path=image_path,
     data_confidence_level=0.4,   # 低めで候補を広く
     fuse="wmean",
     use_l2=False,
     inner_shrink=1.0,
     ring=8,
     drop_first=1,
     gamma=0.8,
     layer_weights=None,
)

# 2) 二重ゲート（conf と 異常）—— ndarray 化 → ブールマスク
if len(conf_scores):
    bbox            = np.asarray(bbox, dtype=np.float32)
    label_idx       = np.asarray(label_idx, dtype=np.int64)
    conf_scores     = np.asarray(conf_scores, dtype=np.float32)
    anomaly_scores  = np.asarray(anomaly_scores, dtype=np.float32)
    pm_flags        = np.asarray(pm_flags, dtype=bool)

    keep = (conf_scores >= THETA_CONF)
    if np.isfinite(TAU):
        keep &= (anomaly_scores <= TAU)

    bbox, label_idx      = bbox[keep], label_idx[keep]
    conf_scores          = conf_scores[keep]
    anomaly_scores       = anomaly_scores[keep]
    pm_flags             = pm_flags[keep]
else:
    # 予防的に空の ndarray を用意
    bbox = np.asarray(bbox, dtype=np.float32)
    label_idx = np.asarray(label_idx, dtype=np.int64)
    conf_scores = np.asarray(conf_scores, dtype=np.float32)
    anomaly_scores = np.asarray(anomaly_scores, dtype=np.float32)
    pm_flags = np.asarray(pm_flags, dtype=bool)

# 3) （任意）スコア融合：異常が高いほどペナルティ（conf × exp(-β·anom)）
if len(conf_scores):
    BETA = 0.5
    fused_conf = conf_scores * np.exp(-BETA * anomaly_scores)
    order = np.argsort(-fused_conf)  # 高い順
    bbox, label_idx = bbox[order], label_idx[order]
    conf_scores, anomaly_scores, fused_conf = conf_scores[order], anomaly_scores[order], fused_conf[order]
    pm_flags = pm_flags[order]
else:
    fused_conf = conf_scores  # 空

# 3.5) ROIゲート（Valから作った τ_roi があれば適用）
ROI_THRESH_PATH = os.path.join(WEIGHTS_DIR, "roi_anom_thresh.json")
if os.path.exists(ROI_THRESH_PATH) and len(bbox):
    with open(ROI_THRESH_PATH, 'r') as f:
        tau_roi = float(json.load(f).get("tau_roi", float("inf")))

    # 画像全体 zmap → 各ROI の異常特徴（inner/top-k）を算出して足切り
    z = roi_zscore_map(image_path, net, ssd_cfg)         # [H,W]
    roi_feats = roi_features_from_boxes(z, bbox, z.shape)  # [N,3] = (inner_mean, bg_mean, topk_mean)
    keep_roi = roi_feats[:, 2] <= tau_roi                  # topk_mean が τ_roi 以下
    bbox, label_idx      = bbox[keep_roi], label_idx[keep_roi]
    conf_scores          = conf_scores[keep_roi]
    anomaly_scores       = anomaly_scores[keep_roi]
    pm_flags             = pm_flags[keep_roi]
    if len(fused_conf):
        fused_conf = fused_conf[keep_roi]
    print(f"[roi] filtered by tau_roi={tau_roi:.3f} → kept {len(bbox)}")

# 4) 可視化：欠損（pm_flags=True）には [ABN] バッジを付けて表示
def _draw_with_abnormal(rgb_img, boxes, labels, conf, anom, pm_flags, class_names):
    fig, ax = plt.subplots(1, figsize=(10,10))
    ax.imshow(rgb_img)
    for i,(bb,lb) in enumerate(zip(boxes, labels)):
        if len(bb) != 4:
            continue
        x1,y1,x2,y2 = [int(v) for v in bb]
        ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, fill=False, linewidth=2))
        base = class_names[int(lb)] if 0 <= int(lb) < len(class_names) else str(lb)
        tag  = " [ABN]" if (i < len(pm_flags) and bool(pm_flags[i])) else ""
        cval = conf[i] if i < len(conf) else float('nan')
        aval = anom[i] if i < len(anom) else float('nan')
        text = f"{base}{tag} conf:{cval:.2f} anom:{aval:.2f}"
        ax.text(x1, y1, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))
    plt.axis('off'); plt.show()

_draw_with_abnormal(rgb_img, bbox, label_idx, conf_scores, anomaly_scores, pm_flags, voc_classes)

# 5) ログ（任意）
for bb, lb, cf, an, pf in zip(bbox, label_idx, conf_scores, anomaly_scores, pm_flags):
    tag = "ABN" if pf else "OK"
    print(f"box={bb.astype(int).tolist()}  label={voc_classes[int(lb)]}  conf={cf:.3f}  anom={an:.3f}  {tag}")

import math

def _clip_int(v, lo, hi):
    """Python int に丸めてから [lo, hi] でクリップ（右端は含む想定で後続で min として使用）"""
    return int(max(lo, min(hi, v)))

def _shrink_box_int(x0, y0, x2, y2, shrink, W, H):
    """
    浮動小数で縮小→floor/ceil→int化→画像サイズでクリップ。
    戻りは (xi1, yi1, xi2, yi2) すべて Python int。
    """
    # 中心・幅高
    cx = 0.5 * (x0 + x2)
    cy = 0.5 * (y0 + y2)
    w  = max(0.0, (x2 - x0) * float(shrink))
    h  = max(0.0, (y2 - y0) * float(shrink))

    # 浮動小数のボックス
    fx1 = cx - 0.5 * w
    fy1 = cy - 0.5 * h
    fx2 = cx + 0.5 * w
    fy2 = cy + 0.5 * h

    # 画素境界に合わせる（内側は floor、外側は ceil で0面積を避ける）
    xi1 = _clip_int(math.floor(fx1), 0, W)
    yi1 = _clip_int(math.floor(fy1), 0, H)
    xi2 = _clip_int(math.ceil (fx2), 0, W)
    yi2 = _clip_int(math.ceil (fy2), 0, H)

    # 万一逆転したら修正（最小1ピクセル確保を試みるが、範囲が尽きる場合はそのまま）
    if xi2 <= xi1 and xi1 < W:
        xi2 = min(W, xi1 + 1)
    if yi2 <= yi1 and yi1 < H:
        yi2 = min(H, yi1 + 1)
    return xi1, yi1, xi2, yi2

def debug_anom_terms(predictor, img_path, conf=0.05,
                     inner_shrink=0.90, ring=8, drop_first=0,
                     fuse="mean", use_l2=False):
    """
    - 予測＆異常マップ取得後、各検出ボックス内外の指標をデバッグ表示
    - スライス添字は必ず int に丸める
    - hm_std が numpy.ndarray の場合を考慮して .copy() を使用
    """
    rgb, bbox, label_idx, conf_scores, anom, pm_flags, pm_scores, pm_area, pm_mean, hm_std = \
        predictor.ssd_predict_with_anomaly(
            image_file_path=img_path,
            data_confidence_level=conf,
            inner_shrink=inner_shrink,
            ring=ring,
            drop_first=drop_first,
            fuse=fuse,
            use_l2=False
        )

    # hm_std は numpy.ndarray を想定
    H, W = hm_std.shape[-2], hm_std.shape[-1]
    print("[n_det]", len(bbox))

    ring_int = int(round(float(ring)))

    for (x0, y0, x2, y2) in bbox:
        xi1, yi1, xi2, yi2 = _shrink_box_int(float(x0), float(y0), float(x2), float(y2),
                                              float(inner_shrink), W, H)

        xr1 = _clip_int(xi1 - ring_int, 0, W)
        yr1 = _clip_int(yi1 - ring_int, 0, H)
        xr2 = _clip_int(xi2 + ring_int, 0, W)
        yr2 = _clip_int(yi2 + ring_int, 0, H)

        # numpy 用に copy() を使用
        ring_patch = hm_std[yr1:yr2, xr1:xr2].copy()

        ih1, iw1 = yi1 - yr1, xi1 - xr1
        ih2, iw2 = ih1 + (yi2 - yi1), iw1 + (xi2 - xi1)

        if (yr2 - yr1) > 0 and (xr2 - xr1) > 0 and (ih2 > ih1) and (iw2 > iw1):
            ring_patch[ih1:ih2, iw1:iw2] = float('nan')

        # 例：前景・背景の平均を表示
        fg_mean = np.nanmean(hm_std[yi1:yi2, xi1:xi2])
        bg_mean = np.nanmean(ring_patch)
        print(f"fg_mean={fg_mean:.3f}, bg_mean={bg_mean:.3f}")

    return

# predictor は既に SSDPredictShow(...) などで作成済みを想定
image_path = "/content/drive/MyDrive/SSD/test_image/crab2.jpg"

# 修正版 debug_anom_terms を呼び出す
debug_anom_terms(
    predictor,
    img_path=image_path,
    conf=0.05,          # 確信度しきい値
    inner_shrink=0.90,  # ROIを少し縮める
    ring=8,             # 背景リング幅
    drop_first=0,       # 浅い層も使う
    fuse="mean",        # 融合方法
    use_l2=False         # L2誤差を使用
)

# === hm_vis を重ね描き（新規セルC） ===
import matplotlib.pyplot as plt

def show_overlay(rgb_img, hm_vis, alpha=0.5):
    plt.figure(figsize=(7,7))
    plt.imshow(rgb_img)
    plt.imshow(hm_vis, cmap='jet', alpha=alpha)
    plt.axis('off'); plt.show()

img_path = "/content/drive/MyDrive/SSD/test_image/crab2.jpg"
rgb, boxes, labels, confs, anoms, pm_f, pm_s, pm_a, pm_m, hmv = \
    pred.ssd_predict_with_anomaly(img_path, data_confidence_level=0.4,
                                  fuse="wmean", use_l2=False,
                                  inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8)
show_overlay(rgb, hmv, alpha=0.5)

# === 浅層ON・誤差をL2で（新規セルE） ===
rgb, boxes, labels, confs, anoms, *_ = \
    pred.ssd_predict_with_anomaly(img_path, data_confidence_level=0.4,
                                  fuse="wmean", use_l2=False,
                                  inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8)
print("anom(use_l2, drop_first=0):", anoms)

# === z正規化 OFF 比較（新規セルF） ===
mu_bak, std_bak = net.calib_mu, net.calib_std
net.calib_mu, net.calib_std = None, None

rgb, boxes, labels, confs, anoms_noz, *_ = \
    pred.ssd_predict_with_anomaly(img_path, data_confidence_level=0.4,
                                  fuse="wmean", use_l2=False,
                                  inner_shrink=1.0, ring=8, drop_first=1, gamma=0.8)
print("anom(no z-norm):", anoms_noz)

# 戻す
net.calib_mu, net.calib_std = mu_bak, std_bak

# 収縮/リング（欠損が枠ギリギリなら shrink を 0.98〜1.0）
rgb, boxes, labels, confs, anoms, pm_f, pm_s, pm_a, pm_m, hmv = \
    pred.ssd_predict_with_anomaly(img_path, data_confidence_level=0.05,
                                  fuse="wmean", use_l2=USE_L2,
                                  inner_shrink=0.98, ring=8, drop_first=0, gamma=0.2)
print("anom:", anoms)
print("pm_ok:", pm_f, " pm_area:", pm_a, " pm_mean:", pm_m)
show_overlay(rgb, hmv, alpha=0.5)

"""一定値以下の場合、異常と判別するようにしたい。 それと現在の状態だとanomalyの値が欠損したカニでもanom0.0となっており、ほぼ機能していない。これを改良するには具体的にどのような手順を踏めばいい？

現状は異常値は検出しないようになっているため、異常値用のラベルを設定し、基準値以下であればそれを適用する。
"""

# ===== Inference / Evaluation Consistency Audit =====
# 前提: pred = SSDPredictShow(eval_categories=voc_classes, net=net)
#       val_img_list (検証画像のパス配列), 既に存在
# 使い方例:
# run_eval_audit(pred, val_img_list, data_confidence_level=0.40, q=0.98, sample=24)

import math, random, json, numpy as np, torch
from collections import defaultdict

def _to_device(x, dev):
    if torch.is_tensor(x):
        return x.to(dev, non_blocking=True)
    return x

def _prepare_input_for_net(predictor, img_path):
    # ssd_predict_show.py と同じ前処理系を踏襲して BCHW tensor を得る
    import cv2
    img_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)
    if img_bgr is None:
        raise FileNotFoundError(img_path)
    H, W = img_bgr.shape[:2]
    img_transformed, _, _ = predictor.transform(img_bgr, 'val', boxes=np.zeros((0,4),np.float32), labels=np.zeros((0,),np.int64))
    # HWC(BGR)→RGB→CHW（ssd_predict_show と整合）
    img_t = torch.from_numpy(img_transformed[:, :, (2,1,0)]).permute(2,0,1).contiguous().float()
    x = img_t.unsqueeze(0).to(next(predictor.net.parameters()).device)
    return (img_bgr[..., ::-1], (H, W), x)  # RGB, size, tensor

def _forward_raw_loc_conf_dbox(net, x):
    net.eval()
    with torch.no_grad():
        out = net(x)
    # ssd_predict_show が持つ柔軟パーサ相当
    loc = conf = dbox_list = None
    if isinstance(out, tuple):
        if len(out) == 3:
            loc, conf, dbox_list = out
        elif len(out) == 2:
            loc, conf = out
            dbox_list = getattr(net, "dbox_list", None)
            if dbox_list is None:
                from ssd_model import DBox
                dbox_list = DBox(getattr(net, "cfg", {})).make_dbox_list()
        else:
            raise ValueError(f"Unexpected tuple length from net(x): {len(out)}")
    elif isinstance(out, dict):
        loc = out.get("loc"); conf = out.get("conf")
        dbox_list = out.get("dbox_list", getattr(net, "dbox_list", None))
        # 'final' を返す実装もあり得るが、ここでは生の loc/conf を比較したいので無視
    elif torch.is_tensor(out):
        # Detect後テンソルのみを返す特殊実装は比較不能
        raise TypeError("net(x) returned final tensor only; need (loc,conf[,dbox]) for audit.")
    else:
        raise TypeError(f"Unsupported return type from net(x): {type(out)}")
    return loc, conf, dbox_list

def _parse_final_preds_like_predict_show(net, final_preds, data_conf):
    # ssd_predict_show の形状吸収ロジックのサブセット
    if not isinstance(final_preds, torch.Tensor):
        raise TypeError(f"final prediction must be Tensor, got {type(final_preds)}")
    if final_preds.dim() == 2:
        final_preds = final_preds.unsqueeze(0)
    if final_preds.dim() == 3:
        # [1,N,C] で [x1,y1,x2,y2,score,label] 等の並び
        B,N,C = final_preds.shape
        assert B==1
        p = final_preds[0]
        # 並び推定（スコアが[0,1]っぽく、座標が0..1でx2>x1,y2>y1を満たす候補を選ぶ）
        cand = [(0,1,2,3,4,5),(1,2,3,4,0,5),(0,1,2,3,5,4),(2,3,4,5,1,0)]
        chosen = None
        for (xi1,yi1,xi2,yi2,si,li) in cand:
            if C <= max([xi1,yi1,xi2,yi2,si,li]): continue
            xy = p[:, [xi1,yi1,xi2,yi2]]
            if not torch.isfinite(xy).all(): continue
            x1,y1,x2,y2 = xy[:,0],xy[:,1],xy[:,2],xy[:,3]
            ok_range = ((x1>=-0.05)&(x1<=1.05)&(y1>=-0.05)&(y1<=1.05)&(x2>=-0.05)&(x2<=1.05)&(y2>=-0.05)&(y2<=1.05)).float().mean() > 0.7
            ok_order = ((x2-x1>0)&(y2-y1>0)).float().mean() > 0.7
            if ok_range and ok_order:
                chosen=(xi1,yi1,xi2,yi2,si,li); break
        if chosen is None:
            return torch.empty((0,4)), torch.empty((0,)), torch.empty((0,),dtype=torch.int64)
        (xi1,yi1,xi2,yi2,si,li)=chosen
        boxes  = p[:, [xi1,yi1,xi2,yi2]]
        scores = p[:, si]
        labels = p[:, li].to(torch.int64)
        keep = scores >= float(data_conf)
        return boxes[keep], scores[keep], labels[keep]

    elif final_preds.dim() == 4:
        # [1, C, K, 5] or [1, K, C, 5] 形式
        B,A,K,D = final_preds.shape
        assert B==1 and D==5
        num_classes = int(getattr(net, "num_classes", A))
        preds = final_preds
        if A != num_classes and final_preds.shape[2] == num_classes:
            preds = final_preds.permute(0, 2, 1, 3).contiguous()
            _,A,K,D = preds.shape
        boxes_list=[]; scores_list=[]; labels_list=[]
        for cls in range(1, num_classes):
            t = preds[0, cls]  # [K,5]
            # 末尾の5要素のどちらがscoreか推定
            xy1, sc1 = t[:, :4], t[:, 4]
            xy2, sc2 = t[:, 1:], t[:, 0]
            def ok(xy, sc):
                score_ok = float(((sc>=-0.01)&(sc<=1.01)).float().mean())
                x1,y1,x2,y2 = xy[:,0],xy[:,1],xy[:,2],xy[:,3]
                xy_ok = ((x1>=-0.05)&(x1<=1.05)&(y1>=-0.05)&(y1<=1.05)&(x2>=-0.05)&(x2<=1.05)&(y2>=-0.05)&(y2<=1.05)).float().mean().item()
                ord_ok= ((x2-x1>0)&(y2-y1>0)).float().mean().item()
                return score_ok*xy_ok*ord_ok
            xy, sc = (xy1, sc1) if ok(xy1,sc1) >= ok(xy2,sc2) else (xy2,sc2)
            keep = sc >= float(data_conf)
            if keep.any():
                boxes_list.append(xy[keep]); scores_list.append(sc[keep])
                labels_list.append(torch.full((int(keep.sum()),), cls, dtype=torch.int64, device=xy.device))
        if len(boxes_list)==0:
            return torch.empty((0,4)), torch.empty((0,)), torch.empty((0,),dtype=torch.int64)
        return torch.cat(boxes_list,0), torch.cat(scores_list,0), torch.cat(labels_list,0)

    else:
        return torch.empty((0,4)), torch.empty((0,)), torch.empty((0,),dtype=torch.int64)

def run_eval_audit(predictor, img_paths, data_confidence_level=0.40, q=0.98, sample=24, seed=0):
    torch.set_grad_enabled(False)
    rng = random.Random(seed)
    picks = [img_paths[i] for i in rng.sample(range(len(img_paths)), min(sample, len(img_paths)))]
    dev = next(predictor.net.parameters()).device

    # --- Detect 呼び出し回数と conf 入力の「確率っぽさ」を観測（モンキーパッチ） ---
    detect_calls = {"n": 0, "prob_like": None, "sum1_like": None}
    if hasattr(predictor.net, "detect"):
        _orig_detect_forward = predictor.net.detect.forward
        def _wrapped_detect_forward(loc, conf, dbox):
            # conf が [0,1]かつクラス和≈1 なら "確率っぽい" と判断
            if detect_calls["prob_like"] is None:
                with torch.no_grad():
                    vmin = float(conf.min().item())
                    vmax = float(conf.max().item())
                    prob_like = float(((conf>=-1e-5)&(conf<=1+1e-5)).float().mean().item())
                    sum1_like = float(((conf.sum(dim=-1)-1.0).abs()<1e-3).float().mean().item())
                detect_calls["prob_like"] = (vmin, vmax, prob_like)
                detect_calls["sum1_like"] = sum1_like
            detect_calls["n"] += 1
            return _orig_detect_forward(loc, conf, dbox)
        predictor.net.detect.forward = _wrapped_detect_forward

    # === 1) “三経路”出力の差を1カ所で比較 ==============================
    # A: 可視化/通常経路（pred.ssd_predict）※現状は Detect 前に softmax を掛ける実装
    # B: loc,logits を Detect にそのまま入力（Detect 内部softmaxのみ）
    # C: loc,probs(=外側softmax) を Detect に入力（Detect 内/外の二重softmax）
    all_scores_A, all_scores_B, all_scores_C = [], [], []
    cntA=cntB=cntC=0

    # 併せて ROI スコア分布と zマップの統計も収集
    roi_scores_all = []
    z_stats = {"mean": [], "std": [], "p95": [], "p99": []}

    for path in picks:
        # 前処理 & raw forward
        rgb, (H,W), x = _prepare_input_for_net(predictor, path)
        try:
            loc, conf_logits, dbox = _forward_raw_loc_conf_dbox(predictor.net, x)
        except Exception as e:
            print(f"[skip] {path}: {e}")
            continue

        # B: logits で Detect
        preds_B = predictor.net.detect(loc, conf_logits, dbox)
        boxes_B, scores_B, labels_B = _parse_final_preds_like_predict_show(predictor.net, preds_B, data_confidence_level)
        cntB += int(scores_B.numel()); all_scores_B += scores_B.detach().cpu().tolist()

        # C: probs(=外側softmax) で Detect
        conf_probs = torch.softmax(conf_logits, dim=-1)
        preds_C = predictor.net.detect(loc, conf_probs, dbox)
        boxes_C, scores_C, labels_C = _parse_final_preds_like_predict_show(predictor.net, preds_C, data_confidence_level)
        cntC += int(scores_C.numel()); all_scores_C += scores_C.detach().cpu().tolist()

        # A: 既存の ssd_predict（内部で forward→外側softmax→Detect の可能性）
        rgb_img, predict_bbox, pre_idx, scores_A = predictor.ssd_predict(path, data_confidence_level)
        cntA += int(len(scores_A)); all_scores_A += list(map(float, scores_A))

        # --- zマップ & ROI 統計（単位/スケーリングの妥当性チェック） ---
        with torch.no_grad():
            hm_std_b, hm_vis_b = predictor._make_anomaly_map_from_feats(
                predictor.net, size_hw=(H, W), fuse="wmean",
                use_l2=False if getattr(predictor, "_ae_metric", "L2")=="L2" else False,
                make_vis=True, layer_weights=None, drop_first=0, gamma=0.5
            )
        hm_std = hm_std_b[0]  # [H,W]
        flat = hm_std.flatten()
        z_stats["mean"].append(float(flat.mean().item()))
        z_stats["std"].append(float(flat.std(unbiased=False).item()))
        z_stats["p95"].append(float(torch.quantile(flat, 0.95).item()))
        z_stats["p99"].append(float(torch.quantile(flat, 0.99).item()))

        # ROI スコア（内側平均 - 周囲リング平均）
        # ssd_predict_with_anomaly と同じ関数を用い、可視化/評価経路と同一算出にする
        boxes_px = []
        if predict_bbox is not None and len(predict_bbox)>0:
            for i in range(predict_bbox.shape[0]):
                x1 = int(max(0, min(W - 1, predict_bbox[i, 0] * W)))
                y1 = int(max(0, min(H - 1, predict_bbox[i, 1] * H)))
                x2 = int(max(0, min(W,     predict_bbox[i, 2] * W)))
                y2 = int(max(0, min(H,     predict_bbox[i, 3] * H)))
                boxes_px.append((x1,y1,x2,y2))
            roi_scores = predictor._anomaly_score_for_boxes(hm_std_b[0], boxes_px, ring=10, inner_shrink=0.90)
            roi_scores_all += list(map(float, roi_scores))

    # Detect 呼出しの集計結果
    if hasattr(predictor.net, "detect"):
        print("=== [Detect 呼び出し監視] ===")
        print(f"detect.forward called: {detect_calls['n']} times in this audit")
        if detect_calls["prob_like"] is not None:
            vmin, vmax, prob_like = detect_calls["prob_like"]
            print(f"first-call conf range = [{vmin:.4f}, {vmax:.4f}]  prob-like(frac in [0,1])={prob_like:.3f}  sum≈1 frac={detect_calls['sum1_like']:.3f}")
        # 後始末
        predictor.net.detect.forward = _orig_detect_forward

    # スコア分布（A/B/C）と件数差（=二重softmax/閾値二重などの兆候）
    def _quantiles(v):
        if len(v)==0: return {}
        a = np.array(v, np.float32)
        return {"p50":float(np.quantile(a,0.50)), "p75":float(np.quantile(a,0.75)), "p90":float(np.quantile(a,0.90)), "p95":float(np.quantile(a,0.95)), "p99":float(np.quantile(a,0.99))}
    Qa, Qb, Qc = _quantiles(all_scores_A), _quantiles(all_scores_B), _quantiles(all_scores_C)

    print("\n=== [A/B/C: 予測拾い方の差分チェック] ===")
    print(f"A(vis path) count={cntA}  quantiles={Qa}")
    print(f"B(logits->Detect) count={cntB}  quantiles={Qb}")
    print(f"C(probs->Detect)  count={cntC}  quantiles={Qc}")
    print("→ B と C の乖離が大きければ『外側softmaxの追加で挙動が変化』= 二重softmaxの影響濃厚。")
    print("→ A と B の乖離が大きければ『可視化経路と素のDetectが不一致』= 経路差/二重Detect/閾値二重の疑い。")

    # zマップの“単位”ざっくり検証
    if len(z_stats["mean"])>0:
        m  = float(np.mean(z_stats["mean"]))
        sd = float(np.mean(z_stats["std"]))
        p95= float(np.mean(z_stats["p95"]))
        p99= float(np.mean(z_stats["p99"]))
        print("\n=== [z-map 基本統計（画像平均）] ===")
        print(f"mean≈{m:.3f}  std≈{sd:.3f}  p95≈{p95:.3f}  p99≈{p99:.3f}")
        print("→ 本来“標準化”なら標本内でオーダーは数単位〜十数単位程度。100超が常態ならスケーリング不整合の可能性。")

    # ROI スコア分布と分位 q の当たり確認
    if len(roi_scores_all)>0:
        a = np.array(roi_scores_all, np.float32)
        Q = {f"p{int(p*100)}": float(np.quantile(a,p)) for p in [0.50,0.75,0.90,0.95,0.98,0.99]}
        print("\n=== [ROI スコア分布 & qの当たり] ===")
        print(f"quantiles: {Q}")
        print(f"current q={q} → tau_roi@q ≈ {np.quantile(a, q):.4f}")
        print("→ q が高すぎる場合（検出0が頻発）: p95 や p98 付近へ下げると母集団の上位を拾える。")
    else:
        print("\n[warn] ROI スコアが空。検出が無い/閾値が高すぎる/経路不一致の可能性。")

    # 提案: “ちょい緩め”ラインのサジェスト（scores と ROI）
    if len(all_scores_B)>0:
        qb = _quantiles(all_scores_B)
        suggest_conf = qb.get("p75", None)
    else:
        suggest_conf = None
    if len(roi_scores_all)>0:
        suggest_q = 0.95 if np.isnan(np.quantile(np.array(roi_scores_all), 0.98)) else 0.98
    else:
        suggest_q = None

    print("\n=== [しきい値の当たり所サジェスト] ===")
    if suggest_conf is not None:
        print(f"data_confidence_level ≈ p75(B, logits基準) → {suggest_conf:.3f}（“ちょい緩め”）")
    else:
        print("data_confidence_level: 十分なサンプルが無く推定不能")
    if suggest_q is not None:
        # 実値も合わせて出す
        if len(roi_scores_all)>0:
            tau_suggest = float(np.quantile(np.array(roi_scores_all), min(0.98, max(0.90, q))))
            print(f"q: p98 で様子見（または p95）。例: tau_roi@p98 ≈ {tau_suggest:.4f}")
        else:
            print("q: ROI分布空のため推定不能")
    else:
        print("q: ROI分布空のため推定不能")

    print("\n=== [チェック観点の読み方] ===")
    print("* detect.forward first-call で conf が [0,1] & Σ=1 に近い → 既に“確率”を渡している兆候（外側softmax適用中）。")
    print("* A vs B の件数/分位が乖離 → 経路不一致（可視化と評価の拾い方が違う）/二重Detect/閾値二重の疑い。")
    print("* B vs C の件数/分位が乖離 → “外側softmax”の有無で挙動が変化＝二重softmaxの影響が強い。")
    print("* z-map p95/p99 が極端に大 → 標準化の単位ズレ（μ/σの不適用/層重み・drop_firstの齟齬）を疑う。")
    print("* ROI 分布で q=0.98 の tau が外れ過ぎ → p95〜p98 あたりへ下げて初期化。")

# ===== Exec: Evaluation Consistency Audit (module-path safe) =====
# 前提: run_eval_audit(...) が定義済み

import sys, os, types, importlib.util

# 1) ssd_predict_show をインポート可能にする（/mnt/data などを探索）
for d in ['/mnt/data', '.', '/content', '/workspace']:
    if os.path.isdir(d) and d not in sys.path:
        sys.path.append(d)

_make = None
_anomaly = None

# 2) まず通常インポートを試す
try:
    from utils.ssd_predict_show import _make_anomaly_map_from_feats as _make, _anomaly_score_for_boxes as _anomaly  # type: ignore
except Exception:
    # 次にファイル直指定ロードを試す
    for p in [
        '/mnt/data/ssd_predict_show.py',
        './ssd_predict_show.py',
        '/content/ssd_predict_show.py',
        '/workspace/ssd_predict_show.py',
    ]:
        if os.path.exists(p):
            spec = importlib.util.spec_from_file_location('ssd_predict_show_loaded', p)
            mod = importlib.util.module_from_spec(spec)
            assert spec and spec.loader
            spec.loader.exec_module(mod)  # type: ignore
            _make = getattr(mod, '_make_anomaly_map_from_feats', None)
            _anomaly = getattr(mod, '_anomaly_score_for_boxes', None)
            if _make is not None and _anomaly is not None:
                break
    if _make is None or _anomaly is None:
        raise ImportError('ssd_predict_show の関数をロードできませんでした。ファイルパスを確認してください。')

# 3) 推論器インスタンス（predictor / pred）を解決
try:
    _aud_pred = predictor  # noqa
except NameError:
    try:
        _aud_pred = pred  # noqa
    except NameError:
        raise NameError("predictor（または pred）が未定義です。")

# 4) 必要メソッドを predictor に生やす（未実装環境向け）
if not hasattr(_aud_pred, "_make_anomaly_map_from_feats"):
    _aud_pred._make_anomaly_map_from_feats = types.MethodType(
        lambda self, net, **kwargs: _make(net, **kwargs), _aud_pred
    )
if not hasattr(_aud_pred, "_anomaly_score_for_boxes"):
    _aud_pred._anomaly_score_for_boxes = types.MethodType(
        lambda self, hm, boxes, **kwargs: _anomaly(hm, boxes, **kwargs), _aud_pred
    )

# 5) 画像リストを解決
try:
    _aud_imgs = val_img_list  # noqa
except NameError:
    raise NameError("val_img_list が未定義です。")

# 6) パラメータ
DATA_CONFIDENCE_LEVEL = 0.40  # “ちょい緩め”起点
Q_ROI = 0.98                  # ROI 分位（厳しければ 0.95 で比較）
SAMPLE = 24
SEED = 0

# 7) 監査関数の存在確認
try:
    run_eval_audit  # noqa
except NameError:
    raise NameError("run_eval_audit が未定義です。先に監査関数の定義セルを実行してください。")

# 8) 実行
run_eval_audit(
    _aud_pred,
    _aud_imgs,
    data_confidence_level=DATA_CONFIDENCE_LEVEL,
    q=Q_ROI,
    sample=SAMPLE,
    seed=SEED,
)

